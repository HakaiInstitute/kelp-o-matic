{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa156fd4efae720",
   "metadata": {},
   "source": "## Export S2-only model"
  },
  {
   "cell_type": "code",
   "id": "edf5616a460702af",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-29T16:41:23.975837Z",
     "start_time": "2025-10-29T16:41:17.746329Z"
    }
   },
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "EPS = 1e-10\n",
    "\n",
    "\n",
    "class SKeMaModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=\"tu-maxvit_tiny_tf_512\",\n",
    "            in_channels=10,\n",
    "            encoder_weights=None,\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"per_channel_mean\",\n",
    "            torch.tensor([\n",
    "                1.93357159e02,\n",
    "                2.53693333e02,\n",
    "                1.41648022e02,\n",
    "                9.99292362e02,\n",
    "                3.21693919e02,\n",
    "                6.49704998e-02,\n",
    "                1.57273007e-01,\n",
    "                -1.57273007e-01,\n",
    "                1.82229161e07,\n",
    "                1.09806622e-01,\n",
    "            ]).view(1, -1, 1, 1),\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"per_channel_std\",\n",
    "            torch.tensor([\n",
    "                1.55697494e02,\n",
    "                2.12700364e02,\n",
    "                2.04018106e02,\n",
    "                1.27588129e03,\n",
    "                3.77324432e02,\n",
    "                6.75251176e-01,\n",
    "                7.32966188e-01,\n",
    "                7.32966188e-01,\n",
    "                2.16768826e10,\n",
    "                4.11232123e-01,\n",
    "            ]).view(1, -1, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Unpack spectral bands\n",
    "        blue = x.select(1, 0).unsqueeze(1)\n",
    "        green = x.select(1, 1).unsqueeze(1)\n",
    "        red = x.select(1, 2).unsqueeze(1)\n",
    "        nir = x.select(1, 3).unsqueeze(1)\n",
    "        re = x.select(1, 4).unsqueeze(1)\n",
    "\n",
    "        # Compute vegetation indices\n",
    "        ndvi = self.normalized_index(nir, red)\n",
    "        gndvi = self.normalized_index(nir, green)\n",
    "        ndvi_re = self.normalized_index(re, red)\n",
    "\n",
    "        # Compute other indices\n",
    "        ndwi = self.normalized_index(green, nir)\n",
    "        chl_green = (nir / (green + EPS)) - 1  # Chlorophyll Index Green\n",
    "\n",
    "        # Stack all bands and indices\n",
    "        x_aug = torch.cat([blue, green, red, nir, re, ndvi, ndwi, gndvi, chl_green, ndvi_re], dim=1)\n",
    "\n",
    "        x_aug_normalized = (x_aug - self.per_channel_mean) / self.per_channel_std\n",
    "\n",
    "        return self.model(x_aug_normalized)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalized_index(a, b):\n",
    "        return (a - b) / (a + b + EPS)\n",
    "\n",
    "\n",
    "model = SKeMaModel()\n",
    "\n",
    "sample_input = torch.rand((2, 5, 512, 512), device=torch.device(\"cpu\"), requires_grad=False)\n",
    "model(sample_input)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.7303,  1.1697,  0.8220,  ...,  0.7788,  1.0340,  0.3348],\n",
       "          [ 0.0057, -0.1139, -0.4681,  ..., -0.2208,  0.1807,  0.0909],\n",
       "          [ 0.1513,  0.2111,  0.5061,  ...,  0.2507, -0.2675, -0.5306],\n",
       "          ...,\n",
       "          [-0.0229,  0.2071,  1.2011,  ...,  0.4269,  0.0604, -0.2384],\n",
       "          [-0.0645,  0.4666,  0.1669,  ...,  1.2136,  0.4438,  0.3940],\n",
       "          [-0.5043, -0.8172, -1.1685,  ..., -0.0964, -0.0730, -0.3842]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5609,  1.3146,  1.7041,  ...,  0.9803,  1.2356,  0.5044],\n",
       "          [-0.4189,  0.2561,  0.1056,  ..., -0.3059, -0.5648, -0.4129],\n",
       "          [ 0.2414,  0.7687, -0.5919,  ...,  0.1580, -0.2497, -0.3917],\n",
       "          ...,\n",
       "          [ 0.5073,  0.2510,  0.2949,  ..., -0.8702, -0.5213, -0.0911],\n",
       "          [ 0.2164,  0.0224, -0.8488,  ...,  0.2523, -0.2204, -0.2463],\n",
       "          [-0.0543, -0.1873, -0.5646,  ..., -0.7739, -0.4300, -0.2990]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "1f4f4a75e714ffad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:41:24.434808Z",
     "start_time": "2025-10-29T16:41:23.996191Z"
    }
   },
   "source": [
    "ckpt = torch.load(\"./Unet_tu-maxvit_tiny_tf_512_20250818_164043.ckpt\", map_location=\"cpu\")\n",
    "state_dict = ckpt[\"state_dict\"]\n",
    "\n",
    "# Update keys\n",
    "del state_dict[\"mean\"]\n",
    "del state_dict[\"std\"]\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SKeMaModel(\n",
       "  (model): Unet(\n",
       "    (encoder): TimmUniversalEncoder(\n",
       "      (model): FeatureListNet(\n",
       "        (stem): Stem(\n",
       "          (conv1): Conv2dSame(10, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "          (norm1): BatchNormAct2d(\n",
       "            64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): GELUTanh()\n",
       "          )\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (stages_0): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Identity()\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (stages_1): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (stages_2): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(1024, 1024, kernel_size=(3, 3), stride=(2, 2), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (2): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (3): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (4): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (stages_3): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(2048, 2048, kernel_size=(3, 3), stride=(2, 2), groups=2048, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): UnetDecoder(\n",
       "      (center): Identity()\n",
       "      (blocks): ModuleList(\n",
       "        (0): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (2): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (4): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (segmentation_head): SegmentationHead(\n",
       "      (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Identity()\n",
       "      (2): Activation(\n",
       "        (activation): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "dad1d1c326a10b53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:41:30.574351Z",
     "start_time": "2025-10-29T16:41:24.445736Z"
    }
   },
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    sample_input,\n",
    "    \"./Unet_tu-maxvit_tiny_tf_512_20250818_164043.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    export_params=True,\n",
    "    external_data=False,  # Store model weights in the model file\n",
    "    opset_version=15,  # ONNX opset version\n",
    "    do_constant_folding=True,  # Optimize constants\n",
    "    verbose=False,\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    # dynamic_shapes={\"x\": (torch.export.Dim(\"batch\"), 5, 512, 512)},\n",
    "    dynamo=False,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_97418/1706587419.py:1: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "/Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/__init__.py:2185: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "64beeaf2121b21db",
   "metadata": {},
   "source": "## Export S2 + Bathymetry + Substrate model"
  },
  {
   "cell_type": "code",
   "id": "bb24066e98db5f18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:41:34.352114Z",
     "start_time": "2025-10-29T16:41:30.997861Z"
    }
   },
   "source": [
    "EPS = 1e-10\n",
    "\n",
    "\n",
    "class SKeMaBathyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=\"tu-maxvit_tiny_tf_512\",\n",
    "            in_channels=12,\n",
    "            encoder_weights=None,\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"per_channel_mean\",\n",
    "            torch.tensor([\n",
    "                1.93357159e02,\n",
    "                2.53693333e02,\n",
    "                1.41648022e02,\n",
    "                9.99292362e02,\n",
    "                3.21693919e02,\n",
    "                1.30867292e00,\n",
    "                2.63550136e00,\n",
    "                6.49704998e-02,\n",
    "                1.57273007e-01,\n",
    "                -1.57273007e-01,\n",
    "                1.82229161e07,\n",
    "                1.09806622e-01,\n",
    "            ]).view(1, -1, 1, 1),\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"per_channel_std\",\n",
    "            torch.tensor([\n",
    "                1.55697494e02,\n",
    "                2.12700364e02,\n",
    "                2.04018106e02,\n",
    "                1.27588129e03,\n",
    "                3.77324432e02,\n",
    "                1.33938435e00,\n",
    "                2.14640498e02,\n",
    "                6.75251176e-01,\n",
    "                7.32966188e-01,\n",
    "                7.32966188e-01,\n",
    "                2.16768826e10,\n",
    "                4.11232123e-01,\n",
    "            ]).view(1, -1, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Unpack spectral bands\n",
    "        blue = x.select(1, 0).unsqueeze(1)\n",
    "        green = x.select(1, 1).unsqueeze(1)\n",
    "        red = x.select(1, 2).unsqueeze(1)\n",
    "        nir = x.select(1, 3).unsqueeze(1)\n",
    "        re = x.select(1, 4).unsqueeze(1)\n",
    "        substrate = x.select(1, 5).unsqueeze(1)\n",
    "        bathymetry = x.select(1, 6).unsqueeze(1)\n",
    "\n",
    "        # Compute vegetation indices\n",
    "        ndvi = self.normalized_index(nir, red)\n",
    "        gndvi = self.normalized_index(nir, green)\n",
    "        ndvi_re = self.normalized_index(re, red)\n",
    "\n",
    "        # Compute other indices\n",
    "        ndwi = self.normalized_index(green, nir)\n",
    "        chl_green = (nir / (green + EPS)) - 1  # Chlorophyll Index Green\n",
    "\n",
    "        # Stack all bands and indices\n",
    "        x_aug = torch.cat(\n",
    "            [blue, green, red, nir, re, substrate, bathymetry, ndvi, ndwi, gndvi, chl_green, ndvi_re], dim=1\n",
    "        )\n",
    "\n",
    "        x_aug_normalized = (x_aug - self.per_channel_mean) / self.per_channel_std\n",
    "\n",
    "        return self.model(x_aug_normalized)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalized_index(a, b):\n",
    "        return (a - b) / (a + b + EPS)\n",
    "\n",
    "\n",
    "model = SKeMaBathyModel()\n",
    "\n",
    "sample_input = torch.rand((2, 7, 512, 512), device=torch.device(\"cpu\"), requires_grad=False)\n",
    "model(sample_input)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.4813,  0.0931,  0.0299,  ...,  1.1187,  1.0073,  0.2813],\n",
       "          [ 0.5153, -0.0539,  0.6094,  ...,  1.4073,  1.8217,  0.7350],\n",
       "          [ 0.7110, -0.1710,  0.7305,  ...,  3.1022,  2.5016,  0.0418],\n",
       "          ...,\n",
       "          [ 0.2322, -0.3572, -0.6058,  ...,  0.0209,  0.8186,  0.3770],\n",
       "          [ 0.7268,  0.2270, -0.0429,  ..., -0.0917,  1.0067,  0.5032],\n",
       "          [-0.2261,  0.0787,  0.0792,  ...,  0.1037,  0.0944,  0.1754]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3767,  0.7756,  0.5304,  ...,  0.7425,  1.2530,  0.4901],\n",
       "          [-0.0702, -0.5960,  0.2289,  ...,  0.7328,  1.0064,  1.0822],\n",
       "          [ 0.5095, -0.2555, -0.3810,  ..., -0.4529,  0.7583,  1.3517],\n",
       "          ...,\n",
       "          [ 1.3123,  0.5650,  0.9073,  ...,  0.8774,  0.4388,  0.0916],\n",
       "          [ 0.7773,  1.8775,  1.1773,  ...,  0.6576,  0.6688, -0.2499],\n",
       "          [ 0.2814, -0.4011, -0.5158,  ...,  0.3432,  0.0538, -0.2632]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "3dfc772707bf682f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:41:35.150844Z",
     "start_time": "2025-10-29T16:41:34.757011Z"
    }
   },
   "source": [
    "ckpt = torch.load(\"./Unet_tu-maxvit_tiny_tf_512_20250714_222203.ckpt\", map_location=\"cpu\")\n",
    "state_dict = ckpt[\"state_dict\"]\n",
    "\n",
    "# Update keys\n",
    "del state_dict[\"mean\"]\n",
    "del state_dict[\"std\"]\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SKeMaBathyModel(\n",
       "  (model): Unet(\n",
       "    (encoder): TimmUniversalEncoder(\n",
       "      (model): FeatureListNet(\n",
       "        (stem): Stem(\n",
       "          (conv1): Conv2dSame(12, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "          (norm1): BatchNormAct2d(\n",
       "            64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): GELUTanh()\n",
       "          )\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (stages_0): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Identity()\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (stages_1): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (stages_2): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(1024, 1024, kernel_size=(3, 3), stride=(2, 2), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (2): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (3): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (4): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (stages_3): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(2048, 2048, kernel_size=(3, 3), stride=(2, 2), groups=2048, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): UnetDecoder(\n",
       "      (center): Identity()\n",
       "      (blocks): ModuleList(\n",
       "        (0): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (2): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (4): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (segmentation_head): SegmentationHead(\n",
       "      (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Identity()\n",
       "      (2): Activation(\n",
       "        (activation): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "5add639940bd05ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:41:42.761969Z",
     "start_time": "2025-10-29T16:41:35.601055Z"
    }
   },
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    sample_input,\n",
    "    \"./Unet_tu-maxvit_tiny_tf_512_20250714_222203.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    export_params=True,\n",
    "    external_data=False,  # Store model weights in the model file\n",
    "    opset_version=15,  # ONNX opset version\n",
    "    do_constant_folding=True,  # Optimize constants\n",
    "    verbose=False,\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    # dynamic_shapes={\"x\": (torch.export.Dim(\"batch\"), 5, 512, 512)},\n",
    "    dynamo=False,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_97418/2895710221.py:1: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "2e45c0c0978ee65c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:41:43.157801Z",
     "start_time": "2025-10-29T16:41:43.155988Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "199b76841b49a44a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
