{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-28T20:56:13.057252Z",
     "start_time": "2025-10-28T20:56:08.271909Z"
    }
   },
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "EPS = 1e-10\n",
    "\n",
    "\n",
    "class SKeMaModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=\"tu-maxvit_tiny_tf_512\",\n",
    "            in_channels=10,\n",
    "            encoder_weights=None,\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"per_channel_mean\",\n",
    "            torch.tensor([\n",
    "                1.93357159e02,\n",
    "                2.53693333e02,\n",
    "                1.41648022e02,\n",
    "                9.99292362e02,\n",
    "                3.21693919e02,\n",
    "                6.49704998e-02,\n",
    "                1.57273007e-01,\n",
    "                -1.57273007e-01,\n",
    "                1.82229161e07,\n",
    "                1.09806622e-01,\n",
    "            ]).view(1, -1, 1, 1),\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"per_channel_std\",\n",
    "            torch.tensor([\n",
    "                1.55697494e02,\n",
    "                2.12700364e02,\n",
    "                2.04018106e02,\n",
    "                1.27588129e03,\n",
    "                3.77324432e02,\n",
    "                6.75251176e-01,\n",
    "                7.32966188e-01,\n",
    "                7.32966188e-01,\n",
    "                2.16768826e10,\n",
    "                4.11232123e-01,\n",
    "            ]).view(1, -1, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Unpack spectral bands\n",
    "        blue = x.select(1, 0).unsqueeze(1)\n",
    "        green = x.select(1, 1).unsqueeze(1)\n",
    "        red = x.select(1, 2).unsqueeze(1)\n",
    "        nir = x.select(1, 3).unsqueeze(1)\n",
    "        re = x.select(1, 4).unsqueeze(1)\n",
    "\n",
    "        # Compute vegetation indices\n",
    "        ndvi = self.normalized_index(nir, red)\n",
    "        gndvi = self.normalized_index(nir, green)\n",
    "        ndvi_re = self.normalized_index(re, red)\n",
    "\n",
    "        # Compute other indices\n",
    "        ndwi = self.normalized_index(green, nir)\n",
    "        chl_green = (nir / (green + EPS)) - 1  # Chlorophyll Index Green\n",
    "\n",
    "        # Stack all bands and indices\n",
    "        x_aug = torch.cat([blue, green, red, nir, re, ndvi, ndwi, gndvi, chl_green, ndvi_re], dim=1)\n",
    "\n",
    "        x_aug_normalized = (x_aug - self.per_channel_mean) / self.per_channel_std\n",
    "\n",
    "        return self.model(x_aug_normalized)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalized_index(a, b):\n",
    "        return (a - b) / (a + b + EPS)\n",
    "\n",
    "\n",
    "model = SKeMaModel()\n",
    "\n",
    "sample_input = torch.rand((2, 5, 512, 512), device=torch.device(\"cpu\"), requires_grad=False)\n",
    "model(sample_input)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5728,  0.9021, -0.5201,  ...,  0.7888,  1.0070,  0.4125],\n",
       "          [ 0.1453,  0.0023, -0.9450,  ..., -0.1469, -0.0198, -0.0842],\n",
       "          [-0.1167, -0.2070, -1.2092,  ..., -0.7366, -0.5809, -0.3558],\n",
       "          ...,\n",
       "          [-0.2290, -1.1171, -1.8872,  ..., -0.5359,  0.1297,  0.2123],\n",
       "          [-0.5878, -0.3411, -0.9095,  ..., -0.1793, -0.2207,  0.3550],\n",
       "          [-0.0072,  0.3908,  0.1005,  ..., -0.0839,  0.1583, -0.1113]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4041,  0.8689,  0.6696,  ...,  0.1934,  0.0745,  0.3263],\n",
       "          [ 0.5460,  0.2423,  0.4991,  ...,  0.1760, -0.1802,  0.0019],\n",
       "          [-0.0688, -0.1781, -0.0824,  ..., -0.0106, -0.1722, -0.1696],\n",
       "          ...,\n",
       "          [-0.6502, -0.3678, -0.9837,  ..., -0.8543,  0.7018, -0.1046],\n",
       "          [-0.3223,  0.3278, -0.7628,  ..., -0.6914,  0.4229,  0.1085],\n",
       "          [-0.1094, -0.0042,  0.0209,  ..., -1.1350, -0.1796, -0.1817]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "1f4f4a75e714ffad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:56:13.521034Z",
     "start_time": "2025-10-28T20:56:13.078462Z"
    }
   },
   "source": [
    "ckpt = torch.load(\"./Unet_tu-maxvit_tiny_tf_512_20250818_164043.ckpt\", map_location=\"cpu\")\n",
    "state_dict = ckpt[\"state_dict\"]\n",
    "\n",
    "# Update keys\n",
    "del state_dict[\"mean\"]\n",
    "del state_dict[\"std\"]\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SKeMaModel(\n",
       "  (model): Unet(\n",
       "    (encoder): TimmUniversalEncoder(\n",
       "      (model): FeatureListNet(\n",
       "        (stem): Stem(\n",
       "          (conv1): Conv2dSame(10, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "          (norm1): BatchNormAct2d(\n",
       "            64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): GELUTanh()\n",
       "          )\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (stages_0): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Identity()\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (stages_1): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (stages_2): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(1024, 1024, kernel_size=(3, 3), stride=(2, 2), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (2): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (3): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (4): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (stages_3): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(2048, 2048, kernel_size=(3, 3), stride=(2, 2), groups=2048, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): UnetDecoder(\n",
       "      (center): Identity()\n",
       "      (blocks): ModuleList(\n",
       "        (0): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (2): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (4): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (segmentation_head): SegmentationHead(\n",
       "      (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Identity()\n",
       "      (2): Activation(\n",
       "        (activation): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "dad1d1c326a10b53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T21:08:28.890937Z",
     "start_time": "2025-10-28T21:08:18.871961Z"
    }
   },
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    sample_input,\n",
    "    \"./Unet_tu-maxvit_tiny_tf_512_20250818_164043.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    export_params=True,\n",
    "    external_data=False,  # Store model weights in the model file\n",
    "    opset_version=15,  # ONNX opset version\n",
    "    do_constant_folding=True,  # Optimize constants\n",
    "    verbose=False,\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    # dynamic_shapes={\"x\": (torch.export.Dim(\"batch\"), 5, 512, 512)},\n",
    "    dynamo=False,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_27742/1706587419.py:1: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "/Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/__init__.py:2185: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bb24066e98db5f18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
