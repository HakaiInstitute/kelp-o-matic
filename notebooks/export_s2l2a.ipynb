{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T18:31:35.759848Z",
     "start_time": "2025-10-28T18:31:16.605218Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0673, -0.1038, -0.0036,  ..., -0.4200, -0.3432,  0.2372],\n",
       "          [-0.3212, -0.6060, -0.3758,  ..., -0.6710, -0.9601, -0.3606],\n",
       "          [-0.5148, -0.5346, -0.1938,  ..., -0.3407,  0.1450,  0.1428],\n",
       "          ...,\n",
       "          [-0.2488, -0.2109,  0.0623,  ...,  0.1308,  0.3180, -0.0263],\n",
       "          [ 0.0498, -0.6152, -0.0099,  ...,  0.2848,  0.3143, -0.2256],\n",
       "          [-0.3968, -0.5310, -0.3592,  ...,  0.4874, -0.0841, -0.1486]]],\n",
       "\n",
       "\n",
       "        [[[-0.0040, -0.3440, -0.1445,  ...,  0.1392, -0.4046,  0.0536],\n",
       "          [-0.6309,  0.0186,  0.3355,  ..., -0.3364, -0.2863, -0.4915],\n",
       "          [-0.3740, -0.1786,  0.0284,  ..., -0.4980,  0.0996,  0.1463],\n",
       "          ...,\n",
       "          [-0.1797,  0.4279,  0.5669,  ..., -0.3368, -0.1825,  0.1968],\n",
       "          [ 0.0198, -0.0563,  0.3539,  ...,  0.1226, -0.0461,  0.1824],\n",
       "          [-0.4663, -0.6480,  0.0725,  ..., -0.2525, -0.3959, -0.2123]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "EPS = 1e-10\n",
    "\n",
    "\n",
    "class SKeMaModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=\"tu-maxvit_tiny_tf_512\",\n",
    "            in_channels=10,\n",
    "            encoder_weights=None,\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"per_channel_mean\",\n",
    "            torch.tensor([\n",
    "                1.93357159e02,\n",
    "                2.53693333e02,\n",
    "                1.41648022e02,\n",
    "                9.99292362e02,\n",
    "                3.21693919e02,\n",
    "                6.49704998e-02,\n",
    "                1.57273007e-01,\n",
    "                -1.57273007e-01,\n",
    "                1.82229161e07,\n",
    "                1.09806622e-01,\n",
    "            ]).view(1, -1, 1, 1),\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"per_channel_std\",\n",
    "            torch.tensor([\n",
    "                1.55697494e02,\n",
    "                2.12700364e02,\n",
    "                2.04018106e02,\n",
    "                1.27588129e03,\n",
    "                3.77324432e02,\n",
    "                6.75251176e-01,\n",
    "                7.32966188e-01,\n",
    "                7.32966188e-01,\n",
    "                2.16768826e10,\n",
    "                4.11232123e-01,\n",
    "            ]).view(1, -1, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Unpack spectral bands\n",
    "        blue = x.select(1, 0).unsqueeze(1)\n",
    "        green = x.select(1, 1).unsqueeze(1)\n",
    "        red = x.select(1, 2).unsqueeze(1)\n",
    "        nir = x.select(1, 3).unsqueeze(1)\n",
    "        re = x.select(1, 4).unsqueeze(1)\n",
    "\n",
    "        # Compute vegetation indices\n",
    "        ndvi = self.normalized_index(nir, red)\n",
    "        gndvi = self.normalized_index(nir, green)\n",
    "        ndvi_re = self.normalized_index(re, red)\n",
    "\n",
    "        # Compute other indices\n",
    "        ndwi = self.normalized_index(green, nir)\n",
    "        chl_green = (nir / (green + EPS)) - 1  # Chlorophyll Index Green\n",
    "\n",
    "        # Stack all bands and indices\n",
    "        x_aug = torch.cat([blue, green, red, nir, re, ndvi, ndwi, gndvi, chl_green, ndvi_re], dim=1)\n",
    "\n",
    "        x_aug_normalized = (x_aug - self.per_channel_mean) / self.per_channel_std\n",
    "\n",
    "        return self.model(x_aug_normalized)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalized_index(a, b):\n",
    "        return (a - b) / (a + b + EPS)\n",
    "\n",
    "\n",
    "model = SKeMaModel()\n",
    "\n",
    "sample_input = torch.rand((2, 5, 512, 512), device=torch.device(\"cpu\"), requires_grad=False)\n",
    "model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f4f4a75e714ffad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T18:45:11.670783Z",
     "start_time": "2025-10-28T18:45:11.200257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SKeMaModel(\n",
       "  (model): Unet(\n",
       "    (encoder): TimmUniversalEncoder(\n",
       "      (model): FeatureListNet(\n",
       "        (stem): Stem(\n",
       "          (conv1): Conv2dSame(10, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "          (norm1): BatchNormAct2d(\n",
       "            64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): GELUTanh()\n",
       "          )\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (stages_0): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Identity()\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (stages_1): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (stages_2): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(1024, 1024, kernel_size=(3, 3), stride=(2, 2), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (2): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (3): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (4): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (stages_3): MaxxVitStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Downsample2d(\n",
       "                  (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "                  (expand): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2dSame(2048, 2048, kernel_size=(3, 3), stride=(2, 2), groups=2048, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): MaxxVitBlock(\n",
       "              (conv): MbConvBlock(\n",
       "                (shortcut): Identity()\n",
       "                (pre_norm): BatchNormAct2d(\n",
       "                  512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "                (down): Identity()\n",
       "                (conv1_1x1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm1): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (conv2_kxk): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "                (norm2): BatchNormAct2d(\n",
       "                  2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): GELUTanh()\n",
       "                )\n",
       "                (se): SEModule(\n",
       "                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (bn): Identity()\n",
       "                  (act): SiLU(inplace=True)\n",
       "                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (gate): Sigmoid()\n",
       "                )\n",
       "                (conv3_1x1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (drop_path): Identity()\n",
       "              )\n",
       "              (attn_block): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (attn_grid): PartitionAttentionCl(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): AttentionCl(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (rel_pos): RelPosBiasTf()\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls1): Identity()\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELUTanh()\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (ls2): Identity()\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): UnetDecoder(\n",
       "      (center): Identity()\n",
       "      (blocks): ModuleList(\n",
       "        (0): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (2): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (4): UnetDecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (segmentation_head): SegmentationHead(\n",
       "      (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Identity()\n",
       "      (2): Activation(\n",
       "        (activation): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"./Unet_tu-maxvit_tiny_tf_512_20250818_164043.ckpt\", map_location=\"cpu\")\n",
    "state_dict = ckpt[\"state_dict\"]\n",
    "\n",
    "# Update keys\n",
    "del state_dict[\"mean\"]\n",
    "del state_dict[\"std\"]\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dad1d1c326a10b53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T18:57:37.854393Z",
     "start_time": "2025-10-28T18:57:18.597240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 41 of general pattern rewrite rules.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 18},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.9.0',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"input\"<FLOAT,[batch,5,512,512]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"output\"<FLOAT,[batch,1,512,512]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"model.encoder.model.stem.conv1.weight\"<FLOAT,[64,10,3,3]>{Tensor(...)},\n",
       "                %\"model.encoder.model.stem.conv1.bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"model.encoder.model.stem.conv2.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stem.conv2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.pre_norm.weight\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.pre_norm.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.conv1_1x1.weight\"<FLOAT,[256,64,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.conv2_kxk.weight\"<FLOAT,[256,1,3,3]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.se.fc1.weight\"<FLOAT,[16,256,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.se.fc1.bias\"<FLOAT,[16]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.se.fc2.weight\"<FLOAT,[256,16,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.se.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.conv3_1x1.weight\"<FLOAT,[64,256,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.conv3_1x1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_block.norm1.weight\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_block.norm1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_block.attn.qkv.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_block.attn.proj.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_block.norm2.weight\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_block.norm2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_block.mlp.fc1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_block.mlp.fc2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_grid.norm1.weight\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_grid.norm1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_grid.attn.qkv.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_grid.attn.proj.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_grid.norm2.weight\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_grid.norm2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_grid.mlp.fc1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_grid.mlp.fc2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.pre_norm.weight\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.pre_norm.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.conv1_1x1.weight\"<FLOAT,[256,64,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.conv2_kxk.weight\"<FLOAT,[256,1,3,3]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.se.fc1.weight\"<FLOAT,[16,256,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.se.fc1.bias\"<FLOAT,[16]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.se.fc2.weight\"<FLOAT,[256,16,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.se.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.conv3_1x1.weight\"<FLOAT,[64,256,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.conv3_1x1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_block.norm1.weight\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_block.norm1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_block.attn.qkv.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_block.attn.proj.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_block.norm2.weight\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_block.norm2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_block.mlp.fc1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_block.mlp.fc2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_grid.norm1.weight\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_grid.norm1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_grid.attn.qkv.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_grid.attn.proj.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_grid.norm2.weight\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_grid.norm2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_grid.mlp.fc1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_grid.mlp.fc2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.shortcut.expand.weight\"<FLOAT,[128,64,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.shortcut.expand.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.pre_norm.weight\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.pre_norm.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.conv1_1x1.weight\"<FLOAT,[512,64,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.norm1.weight\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.norm1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.conv2_kxk.weight\"<FLOAT,[512,1,3,3]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.norm2.weight\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.norm2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.se.fc1.weight\"<FLOAT,[32,512,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.se.fc1.bias\"<FLOAT,[32]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.se.fc2.weight\"<FLOAT,[512,32,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.se.fc2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.conv3_1x1.weight\"<FLOAT,[128,512,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_block.norm1.weight\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_block.norm1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_block.attn.qkv.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_block.attn.proj.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_block.norm2.weight\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_block.norm2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_block.mlp.fc1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_block.mlp.fc2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_grid.norm1.weight\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_grid.norm1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_grid.attn.qkv.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_grid.attn.proj.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_grid.norm2.weight\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_grid.norm2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_grid.mlp.fc1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_grid.mlp.fc2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.pre_norm.weight\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.pre_norm.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.conv1_1x1.weight\"<FLOAT,[512,128,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.norm1.weight\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.norm1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.conv2_kxk.weight\"<FLOAT,[512,1,3,3]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.norm2.weight\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.norm2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.se.fc1.weight\"<FLOAT,[32,512,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.se.fc1.bias\"<FLOAT,[32]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.se.fc2.weight\"<FLOAT,[512,32,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.se.fc2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.conv3_1x1.weight\"<FLOAT,[128,512,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.conv3_1x1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_block.norm1.weight\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_block.norm1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_block.attn.qkv.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_block.attn.proj.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_block.norm2.weight\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_block.norm2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_block.mlp.fc1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_block.mlp.fc2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_grid.norm1.weight\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_grid.norm1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_grid.attn.qkv.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_grid.attn.proj.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_grid.norm2.weight\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_grid.norm2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_grid.mlp.fc1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_grid.mlp.fc2.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.shortcut.expand.weight\"<FLOAT,[256,128,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.shortcut.expand.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.pre_norm.weight\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.pre_norm.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.conv1_1x1.weight\"<FLOAT,[1024,128,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.norm1.weight\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.norm1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.conv2_kxk.weight\"<FLOAT,[1024,1,3,3]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.norm2.weight\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.norm2.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.se.fc1.weight\"<FLOAT,[64,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.se.fc1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.se.fc2.weight\"<FLOAT,[1024,64,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.se.fc2.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.conv3_1x1.weight\"<FLOAT,[256,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_block.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_block.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_block.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_block.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_block.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_block.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_block.mlp.fc1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_block.mlp.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_grid.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_grid.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_grid.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_grid.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_grid.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_grid.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_grid.mlp.fc1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_grid.mlp.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.pre_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.pre_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.conv1_1x1.weight\"<FLOAT,[1024,256,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.norm1.weight\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.norm1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.conv2_kxk.weight\"<FLOAT,[1024,1,3,3]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.norm2.weight\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.norm2.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.se.fc1.weight\"<FLOAT,[64,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.se.fc1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.se.fc2.weight\"<FLOAT,[1024,64,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.se.fc2.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.conv3_1x1.weight\"<FLOAT,[256,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.conv3_1x1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_block.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_block.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_block.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_block.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_block.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_block.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_block.mlp.fc1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_block.mlp.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_grid.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_grid.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_grid.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_grid.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_grid.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_grid.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_grid.mlp.fc1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_grid.mlp.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.pre_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.pre_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.conv1_1x1.weight\"<FLOAT,[1024,256,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.norm1.weight\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.norm1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.conv2_kxk.weight\"<FLOAT,[1024,1,3,3]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.norm2.weight\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.norm2.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.se.fc1.weight\"<FLOAT,[64,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.se.fc1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.se.fc2.weight\"<FLOAT,[1024,64,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.se.fc2.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.conv3_1x1.weight\"<FLOAT,[256,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.conv3_1x1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_block.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_block.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_block.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_block.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_block.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_block.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_block.mlp.fc1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_block.mlp.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_grid.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_grid.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_grid.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_grid.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_grid.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_grid.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_grid.mlp.fc1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_grid.mlp.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.pre_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.pre_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.conv1_1x1.weight\"<FLOAT,[1024,256,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.norm1.weight\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.norm1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.conv2_kxk.weight\"<FLOAT,[1024,1,3,3]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.norm2.weight\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.norm2.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.se.fc1.weight\"<FLOAT,[64,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.se.fc1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.se.fc2.weight\"<FLOAT,[1024,64,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.se.fc2.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.conv3_1x1.weight\"<FLOAT,[256,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.conv3_1x1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_block.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_block.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_block.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_block.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_block.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_block.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_block.mlp.fc1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_block.mlp.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_grid.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_grid.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_grid.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_grid.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_grid.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_grid.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_grid.mlp.fc1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_grid.mlp.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.pre_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.pre_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.conv1_1x1.weight\"<FLOAT,[1024,256,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.norm1.weight\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.norm1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.conv2_kxk.weight\"<FLOAT,[1024,1,3,3]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.norm2.weight\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.norm2.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.se.fc1.weight\"<FLOAT,[64,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.se.fc1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.se.fc2.weight\"<FLOAT,[1024,64,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.se.fc2.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.conv3_1x1.weight\"<FLOAT,[256,1024,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.conv3_1x1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_block.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_block.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_block.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_block.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_block.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_block.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_block.mlp.fc1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_block.mlp.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_grid.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_grid.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_grid.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_grid.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_grid.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_grid.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_grid.mlp.fc1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_grid.mlp.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.shortcut.expand.weight\"<FLOAT,[512,256,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.shortcut.expand.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.pre_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.pre_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.conv1_1x1.weight\"<FLOAT,[2048,256,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.norm1.weight\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.norm1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.conv2_kxk.weight\"<FLOAT,[2048,1,3,3]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.norm2.weight\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.norm2.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.se.fc1.weight\"<FLOAT,[128,2048,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.se.fc1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.se.fc2.weight\"<FLOAT,[2048,128,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.se.fc2.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.conv3_1x1.weight\"<FLOAT,[512,2048,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_block.norm1.weight\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_block.norm1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_block.attn.qkv.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_block.attn.rel_pos.relative_position_bias_table\"<FLOAT,[16,31,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_block.attn.proj.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_block.norm2.weight\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_block.norm2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_block.mlp.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_block.mlp.fc2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_grid.norm1.weight\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_grid.norm1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_grid.attn.qkv.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_grid.attn.rel_pos.relative_position_bias_table\"<FLOAT,[16,31,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_grid.attn.proj.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_grid.norm2.weight\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_grid.norm2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_grid.mlp.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_grid.mlp.fc2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.pre_norm.weight\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.pre_norm.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.conv1_1x1.weight\"<FLOAT,[2048,512,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.norm1.weight\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.norm1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.conv2_kxk.weight\"<FLOAT,[2048,1,3,3]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.norm2.weight\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.norm2.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.se.fc1.weight\"<FLOAT,[128,2048,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.se.fc1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.se.fc2.weight\"<FLOAT,[2048,128,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.se.fc2.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.conv3_1x1.weight\"<FLOAT,[512,2048,1,1]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.conv3_1x1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_block.norm1.weight\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_block.norm1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_block.attn.qkv.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_block.attn.rel_pos.relative_position_bias_table\"<FLOAT,[16,31,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_block.attn.proj.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_block.norm2.weight\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_block.norm2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_block.mlp.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_block.mlp.fc2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_grid.norm1.weight\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_grid.norm1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_grid.attn.qkv.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_grid.attn.rel_pos.relative_position_bias_table\"<FLOAT,[16,31,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_grid.attn.proj.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_grid.norm2.weight\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_grid.norm2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_grid.mlp.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_grid.mlp.fc2.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.0.conv1.0.weight\"<FLOAT,[256,768,3,3]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.0.conv1.1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.0.conv1.1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.0.conv2.0.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.0.conv2.1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.0.conv2.1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.1.conv1.0.weight\"<FLOAT,[128,384,3,3]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.1.conv1.1.weight\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.1.conv1.1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.1.conv2.0.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.1.conv2.1.weight\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.1.conv2.1.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.2.conv1.0.weight\"<FLOAT,[64,192,3,3]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.2.conv1.1.weight\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.2.conv1.1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.2.conv2.0.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.2.conv2.1.weight\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.2.conv2.1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.3.conv1.0.weight\"<FLOAT,[32,128,3,3]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.3.conv1.1.weight\"<FLOAT,[32]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.3.conv1.1.bias\"<FLOAT,[32]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.3.conv2.0.weight\"<FLOAT,[32,32,3,3]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.3.conv2.1.weight\"<FLOAT,[32]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.3.conv2.1.bias\"<FLOAT,[32]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.4.conv1.0.weight\"<FLOAT,[16,32,3,3]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.4.conv1.1.weight\"<FLOAT,[16]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.4.conv1.1.bias\"<FLOAT,[16]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.4.conv2.0.weight\"<FLOAT,[16,16,3,3]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.4.conv2.1.weight\"<FLOAT,[16]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.4.conv2.1.bias\"<FLOAT,[16]>{TorchTensor(...)},\n",
       "                %\"model.segmentation_head.0.weight\"<FLOAT,[1,16,3,3]>{TorchTensor(...)},\n",
       "                %\"model.segmentation_head.0.bias\"<FLOAT,[1]>{TorchTensor<FLOAT,[1]>(Parameter containing: tensor([-0.1306], requires_grad=True), name='model.segmentation_head.0.bias')},\n",
       "                %\"per_channel_mean\"<FLOAT,[1,10,1,1]>{TorchTensor<FLOAT,[1,10,1,1]>(tensor([[[[ 1.9336e+02]],  [[ 2.5369e+02]],  [[ 1.4165e+02]],  [[ 9.9929e+02]],  [[ 3.2169e+02]],  [[ 6.4971e-02]],  [[ 1.5727e-01]],  [[-1.5727e-01]],  [[ 1.8223e+07]],  [[ 1.0981e-01]]]]), name='per_channel_mean')},\n",
       "                %\"per_channel_std\"<FLOAT,[1,10,1,1]>{TorchTensor<FLOAT,[1,10,1,1]>(tensor([[[[1.5570e+02]],  [[2.1270e+02]],  [[2.0402e+02]],  [[1.2759e+03]],  [[3.7732e+02]],  [[6.7525e-01]],  [[7.3297e-01]],  [[7.3297e-01]],  [[2.1677e+10]],  [[4.1123e-01]]]]), name='per_channel_std')},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.pre_norm.running_mean\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.pre_norm.running_var\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.norm1.running_mean\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.norm1.running_var\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.norm2.running_mean\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.conv.norm2.running_var\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_block.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.0.attn_grid.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.pre_norm.running_mean\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.pre_norm.running_var\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.norm1.running_mean\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.norm1.running_var\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.norm2.running_mean\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.conv.norm2.running_var\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_block.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_0.blocks.1.attn_grid.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.pre_norm.running_mean\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.pre_norm.running_var\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.norm1.running_mean\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.norm1.running_var\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.norm2.running_mean\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.conv.norm2.running_var\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_block.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.0.attn_grid.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.pre_norm.running_mean\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.pre_norm.running_var\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.norm1.running_mean\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.norm1.running_var\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.norm2.running_mean\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.conv.norm2.running_var\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_block.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_1.blocks.1.attn_grid.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.pre_norm.running_mean\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.pre_norm.running_var\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.norm1.running_mean\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.norm1.running_var\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.norm2.running_mean\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.conv.norm2.running_var\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_block.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.0.attn_grid.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.pre_norm.running_mean\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.pre_norm.running_var\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.norm1.running_mean\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.norm1.running_var\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.norm2.running_mean\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.conv.norm2.running_var\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_block.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.1.attn_grid.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.pre_norm.running_mean\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.pre_norm.running_var\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.norm1.running_mean\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.norm1.running_var\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.norm2.running_mean\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.conv.norm2.running_var\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_block.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.2.attn_grid.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.pre_norm.running_mean\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.pre_norm.running_var\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.norm1.running_mean\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.norm1.running_var\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.norm2.running_mean\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.conv.norm2.running_var\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_block.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.3.attn_grid.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.pre_norm.running_mean\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.pre_norm.running_var\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.norm1.running_mean\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.norm1.running_var\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.norm2.running_mean\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.conv.norm2.running_var\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_block.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_2.blocks.4.attn_grid.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.pre_norm.running_mean\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.pre_norm.running_var\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.norm1.running_mean\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.norm1.running_var\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.norm2.running_mean\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.conv.norm2.running_var\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_block.attn.rel_pos.height_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_block.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_grid.attn.rel_pos.height_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.0.attn_grid.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.pre_norm.running_mean\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.pre_norm.running_var\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.norm1.running_mean\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.norm1.running_var\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.norm2.running_mean\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.conv.norm2.running_var\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_block.attn.rel_pos.height_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_block.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_grid.attn.rel_pos.height_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.encoder.model.stages_3.blocks.1.attn_grid.attn.rel_pos.width_lookup\"<FLOAT,[16,16,31]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.0.conv1.1.running_mean\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.0.conv1.1.running_var\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.0.conv2.1.running_mean\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.0.conv2.1.running_var\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.1.conv1.1.running_mean\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.1.conv1.1.running_var\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.1.conv2.1.running_mean\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.1.conv2.1.running_var\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.2.conv1.1.running_mean\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.2.conv1.1.running_var\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.2.conv2.1.running_mean\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.2.conv2.1.running_var\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.3.conv1.1.running_mean\"<FLOAT,[32]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.3.conv1.1.running_var\"<FLOAT,[32]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.3.conv2.1.running_mean\"<FLOAT,[32]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.3.conv2.1.running_var\"<FLOAT,[32]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.4.conv1.1.running_mean\"<FLOAT,[16]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.4.conv1.1.running_var\"<FLOAT,[16]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.4.conv2.1.running_mean\"<FLOAT,[16]>{TorchTensor(...)},\n",
       "                %\"model.decoder.blocks.4.conv2.1.running_var\"<FLOAT,[16]>{TorchTensor(...)},\n",
       "                %\"val_1\"<INT64,[]>{Tensor<INT64,[]>(array(0), name='val_1')},\n",
       "                %\"val_2\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_2')},\n",
       "                %\"val_3\"<INT64,[]>{Tensor<INT64,[]>(array(1), name='val_3')},\n",
       "                %\"val_4\"<INT64,[]>{Tensor<INT64,[]>(array(2), name='val_4')},\n",
       "                %\"val_5\"<INT64,[]>{Tensor<INT64,[]>(array(3), name='val_5')},\n",
       "                %\"val_6\"<INT64,[]>{Tensor<INT64,[]>(array(4), name='val_6')},\n",
       "                %\"scalar_tensor_default\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name='scalar_tensor_default')},\n",
       "                %\"val_16\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(3., dtype=float32), name='val_16')},\n",
       "                %\"val_18\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.044715, dtype=float32), name='val_18')},\n",
       "                %\"val_21\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.7978846, dtype=float32), name='val_21')},\n",
       "                %\"val_26\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name='val_26')},\n",
       "                %\"val_28\"<INT64,[8]>{Tensor<INT64,[8]>(array([0, 0, 0, 0, 0, 0, 0, 0]), name='val_28')},\n",
       "                %\"val_29\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0., dtype=float32), name='val_29')},\n",
       "                %\"val_39\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"val_81\"<INT64,[2]>{Tensor<INT64,[2]>(array([2, 3]), name='val_81')},\n",
       "                %\"val_87\"<INT64,[1]>{Tensor<INT64,[1]>(array([8]), name='val_87')},\n",
       "                %\"val_88\"<INT64,[1]>{Tensor<INT64,[1]>(array([16]), name='val_88')},\n",
       "                %\"val_91\"<INT64,[1]>{Tensor<INT64,[1]>(array([64]), name='val_91')},\n",
       "                %\"val_98\"<INT64,[4]>{Tensor<INT64,[4]>(array([-1, 16, 16, 64]), name='val_98')},\n",
       "                %\"val_99\"<FLOAT,[64,192]>{Tensor(...)},\n",
       "                %\"val_101\"<INT64,[]>{Tensor<INT64,[]>(array(64), name='val_101')},\n",
       "                %\"val_102\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1]), name='val_102')},\n",
       "                %\"val_105\"<INT64,[1]>{Tensor<INT64,[1]>(array([3]), name='val_105')},\n",
       "                %\"val_106\"<INT64,[1]>{Tensor<INT64,[1]>(array([2]), name='val_106')},\n",
       "                %\"val_107\"<INT64,[1]>{Tensor<INT64,[1]>(array([32]), name='val_107')},\n",
       "                %\"einsum\"<FLOAT,[2,16,16,31]>{Tensor(...)},\n",
       "                %\"val_117\"<INT64,[3]>{Tensor<INT64,[3]>(array([  2, 256, 256]), name='val_117')},\n",
       "                %\"val_128\"<INT64,[1]>{Tensor<INT64,[1]>(array([9223372036854775807]), name='val_128')},\n",
       "                %\"val_131\"<INT64,[1]>{Tensor<INT64,[1]>(array([-2]), name='val_131')},\n",
       "                %\"val_133\"<INT64,[1]>{Tensor<INT64,[1]>(array([-9223372036854775808]), name='val_133')},\n",
       "                %\"val_141\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.4204482], dtype=float32), name='val_141')},\n",
       "                %\"val_154\"<FLOAT,[64,64]>{Tensor(...)},\n",
       "                %\"val_163\"<INT64,[6]>{Tensor<INT64,[6]>(array([-1,  8,  8, 16, 16, 64]), name='val_163')},\n",
       "                %\"val_169\"<INT64,[4]>{Tensor<INT64,[4]>(array([ -1, 128, 128,  64]), name='val_169')},\n",
       "                %\"val_172\"<FLOAT,[64,256]>{Tensor(...)},\n",
       "                %\"val_186\"<FLOAT,[256,64]>{Tensor(...)},\n",
       "                %\"val_204\"<FLOAT,[64,192]>{Tensor(...)},\n",
       "                %\"einsum_2\"<FLOAT,[2,16,16,31]>{Tensor(...)},\n",
       "                %\"val_255\"<FLOAT,[64,64]>{Tensor(...)},\n",
       "                %\"val_273\"<FLOAT,[64,256]>{Tensor(...)},\n",
       "                %\"val_287\"<FLOAT,[256,64]>{Tensor(...)},\n",
       "                %\"val_354\"<FLOAT,[64,192]>{Tensor(...)},\n",
       "                %\"einsum_4\"<FLOAT,[2,16,16,31]>{Tensor(...)},\n",
       "                %\"val_405\"<FLOAT,[64,64]>{Tensor(...)},\n",
       "                %\"val_423\"<FLOAT,[64,256]>{Tensor(...)},\n",
       "                %\"val_437\"<FLOAT,[256,64]>{Tensor(...)},\n",
       "                %\"val_455\"<FLOAT,[64,192]>{Tensor(...)},\n",
       "                %\"einsum_6\"<FLOAT,[2,16,16,31]>{Tensor(...)},\n",
       "                %\"val_506\"<FLOAT,[64,64]>{Tensor(...)},\n",
       "                %\"val_524\"<FLOAT,[64,256]>{Tensor(...)},\n",
       "                %\"val_538\"<FLOAT,[256,64]>{Tensor(...)},\n",
       "                %\"val_550\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"val_597\"<INT64,[1]>{Tensor<INT64,[1]>(array([4]), name='val_597')},\n",
       "                %\"val_601\"<INT64,[1]>{Tensor<INT64,[1]>(array([128]), name='val_601')},\n",
       "                %\"val_608\"<INT64,[4]>{Tensor<INT64,[4]>(array([ -1,  16,  16, 128]), name='val_608')},\n",
       "                %\"val_609\"<FLOAT,[128,384]>{Tensor(...)},\n",
       "                %\"val_611\"<INT64,[]>{Tensor<INT64,[]>(array(16), name='val_611')},\n",
       "                %\"einsum_8\"<FLOAT,[4,16,16,31]>{Tensor(...)},\n",
       "                %\"val_626\"<INT64,[3]>{Tensor<INT64,[3]>(array([  4, 256, 256]), name='val_626')},\n",
       "                %\"val_661\"<FLOAT,[128,128]>{Tensor(...)},\n",
       "                %\"val_670\"<INT64,[6]>{Tensor<INT64,[6]>(array([ -1,   4,   4,  16,  16, 128]), name='val_670')},\n",
       "                %\"val_676\"<INT64,[4]>{Tensor<INT64,[4]>(array([ -1,  64,  64, 128]), name='val_676')},\n",
       "                %\"val_679\"<FLOAT,[128,512]>{Tensor(...)},\n",
       "                %\"val_693\"<FLOAT,[512,128]>{Tensor(...)},\n",
       "                %\"val_711\"<FLOAT,[128,384]>{Tensor(...)},\n",
       "                %\"einsum_10\"<FLOAT,[4,16,16,31]>{Tensor(...)},\n",
       "                %\"val_762\"<FLOAT,[128,128]>{Tensor(...)},\n",
       "                %\"val_780\"<FLOAT,[128,512]>{Tensor(...)},\n",
       "                %\"val_794\"<FLOAT,[512,128]>{Tensor(...)},\n",
       "                %\"val_861\"<FLOAT,[128,384]>{Tensor(...)},\n",
       "                %\"einsum_12\"<FLOAT,[4,16,16,31]>{Tensor(...)},\n",
       "                %\"val_912\"<FLOAT,[128,128]>{Tensor(...)},\n",
       "                %\"val_930\"<FLOAT,[128,512]>{Tensor(...)},\n",
       "                %\"val_944\"<FLOAT,[512,128]>{Tensor(...)},\n",
       "                %\"val_962\"<FLOAT,[128,384]>{Tensor(...)},\n",
       "                %\"einsum_14\"<FLOAT,[4,16,16,31]>{Tensor(...)},\n",
       "                %\"val_1013\"<FLOAT,[128,128]>{Tensor(...)},\n",
       "                %\"val_1031\"<FLOAT,[128,512]>{Tensor(...)},\n",
       "                %\"val_1045\"<FLOAT,[512,128]>{Tensor(...)},\n",
       "                %\"val_1057\"<FLOAT,[1024]>{Tensor(...)},\n",
       "                %\"val_1108\"<INT64,[1]>{Tensor<INT64,[1]>(array([256]), name='val_1108')},\n",
       "                %\"val_1115\"<INT64,[4]>{Tensor<INT64,[4]>(array([ -1,  16,  16, 256]), name='val_1115')},\n",
       "                %\"val_1116\"<FLOAT,[256,768]>{Tensor(...)},\n",
       "                %\"einsum_16\"<FLOAT,[8,16,16,31]>{Tensor(...)},\n",
       "                %\"val_1132\"<INT64,[3]>{Tensor<INT64,[3]>(array([  8, 256, 256]), name='val_1132')},\n",
       "                %\"val_1167\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_1176\"<INT64,[6]>{Tensor<INT64,[6]>(array([ -1,   2,   2,  16,  16, 256]), name='val_1176')},\n",
       "                %\"val_1182\"<INT64,[4]>{Tensor<INT64,[4]>(array([ -1,  32,  32, 256]), name='val_1182')},\n",
       "                %\"val_1185\"<FLOAT,[256,1024]>{Tensor(...)},\n",
       "                %\"val_1199\"<FLOAT,[1024,256]>{Tensor(...)},\n",
       "                %\"val_1217\"<FLOAT,[256,768]>{Tensor(...)},\n",
       "                %\"einsum_18\"<FLOAT,[8,16,16,31]>{Tensor(...)},\n",
       "                %\"val_1268\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_1286\"<FLOAT,[256,1024]>{Tensor(...)},\n",
       "                %\"val_1300\"<FLOAT,[1024,256]>{Tensor(...)},\n",
       "                %\"val_1367\"<FLOAT,[256,768]>{Tensor(...)},\n",
       "                %\"einsum_20\"<FLOAT,[8,16,16,31]>{Tensor(...)},\n",
       "                %\"val_1418\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_1436\"<FLOAT,[256,1024]>{Tensor(...)},\n",
       "                %\"val_1450\"<FLOAT,[1024,256]>{Tensor(...)},\n",
       "                %\"val_1468\"<FLOAT,[256,768]>{Tensor(...)},\n",
       "                %\"einsum_22\"<FLOAT,[8,16,16,31]>{Tensor(...)},\n",
       "                %\"val_1519\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_1537\"<FLOAT,[256,1024]>{Tensor(...)},\n",
       "                %\"val_1551\"<FLOAT,[1024,256]>{Tensor(...)},\n",
       "                %\"val_1618\"<FLOAT,[256,768]>{Tensor(...)},\n",
       "                %\"einsum_24\"<FLOAT,[8,16,16,31]>{Tensor(...)},\n",
       "                %\"val_1669\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_1687\"<FLOAT,[256,1024]>{Tensor(...)},\n",
       "                %\"val_1701\"<FLOAT,[1024,256]>{Tensor(...)},\n",
       "                %\"val_1719\"<FLOAT,[256,768]>{Tensor(...)},\n",
       "                %\"einsum_26\"<FLOAT,[8,16,16,31]>{Tensor(...)},\n",
       "                %\"val_1770\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_1788\"<FLOAT,[256,1024]>{Tensor(...)},\n",
       "                %\"val_1802\"<FLOAT,[1024,256]>{Tensor(...)},\n",
       "                %\"val_1869\"<FLOAT,[256,768]>{Tensor(...)},\n",
       "                %\"einsum_28\"<FLOAT,[8,16,16,31]>{Tensor(...)},\n",
       "                %\"val_1920\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_1938\"<FLOAT,[256,1024]>{Tensor(...)},\n",
       "                %\"val_1952\"<FLOAT,[1024,256]>{Tensor(...)},\n",
       "                %\"val_1970\"<FLOAT,[256,768]>{Tensor(...)},\n",
       "                %\"einsum_30\"<FLOAT,[8,16,16,31]>{Tensor(...)},\n",
       "                %\"val_2021\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_2039\"<FLOAT,[256,1024]>{Tensor(...)},\n",
       "                %\"val_2053\"<FLOAT,[1024,256]>{Tensor(...)},\n",
       "                %\"val_2120\"<FLOAT,[256,768]>{Tensor(...)},\n",
       "                %\"einsum_32\"<FLOAT,[8,16,16,31]>{Tensor(...)},\n",
       "                %\"val_2171\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_2189\"<FLOAT,[256,1024]>{Tensor(...)},\n",
       "                %\"val_2203\"<FLOAT,[1024,256]>{Tensor(...)},\n",
       "                %\"val_2221\"<FLOAT,[256,768]>{Tensor(...)},\n",
       "                %\"einsum_34\"<FLOAT,[8,16,16,31]>{Tensor(...)},\n",
       "                %\"val_2272\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_2290\"<FLOAT,[256,1024]>{Tensor(...)},\n",
       "                %\"val_2304\"<FLOAT,[1024,256]>{Tensor(...)},\n",
       "                %\"val_2316\"<FLOAT,[2048]>{Tensor(...)},\n",
       "                %\"val_2339\"<FLOAT,[2048]>{Tensor(...)},\n",
       "                %\"val_2367\"<INT64,[1]>{Tensor<INT64,[1]>(array([512]), name='val_2367')},\n",
       "                %\"val_2374\"<INT64,[4]>{Tensor<INT64,[4]>(array([ -1,  16,  16, 512]), name='val_2374')},\n",
       "                %\"val_2375\"<FLOAT,[512,1536]>{Tensor(...)},\n",
       "                %\"val_2391\"<INT64,[3]>{Tensor<INT64,[3]>(array([ 16, 256, 256]), name='val_2391')},\n",
       "                %\"val_2426\"<FLOAT,[512,512]>{Tensor(...)},\n",
       "                %\"val_2435\"<INT64,[6]>{Tensor<INT64,[6]>(array([ -1,   1,   1,  16,  16, 512]), name='val_2435')},\n",
       "                %\"val_2444\"<FLOAT,[512,2048]>{Tensor(...)},\n",
       "                %\"val_2458\"<FLOAT,[2048,512]>{Tensor(...)},\n",
       "                %\"val_2476\"<FLOAT,[512,1536]>{Tensor(...)},\n",
       "                %\"val_2527\"<FLOAT,[512,512]>{Tensor(...)},\n",
       "                %\"val_2545\"<FLOAT,[512,2048]>{Tensor(...)},\n",
       "                %\"val_2559\"<FLOAT,[2048,512]>{Tensor(...)},\n",
       "                %\"val_2569\"<FLOAT,[2048]>{Tensor(...)},\n",
       "                %\"val_2590\"<FLOAT,[2048]>{Tensor(...)},\n",
       "                %\"val_2626\"<FLOAT,[512,1536]>{Tensor(...)},\n",
       "                %\"val_2677\"<FLOAT,[512,512]>{Tensor(...)},\n",
       "                %\"val_2695\"<FLOAT,[512,2048]>{Tensor(...)},\n",
       "                %\"val_2709\"<FLOAT,[2048,512]>{Tensor(...)},\n",
       "                %\"val_2727\"<FLOAT,[512,1536]>{Tensor(...)},\n",
       "                %\"val_2778\"<FLOAT,[512,512]>{Tensor(...)},\n",
       "                %\"val_2796\"<FLOAT,[512,2048]>{Tensor(...)},\n",
       "                %\"val_2810\"<FLOAT,[2048,512]>{Tensor(...)},\n",
       "                %\"val_2814\"<INT64,[2]>{Tensor<INT64,[2]>(array([32, 32]), name='val_2814')},\n",
       "                %\"val_2837\"<INT64,[2]>{Tensor<INT64,[2]>(array([64, 64]), name='val_2837')},\n",
       "                %\"val_2843\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"val_2859\"<INT64,[2]>{Tensor<INT64,[2]>(array([128, 128]), name='val_2859')},\n",
       "                %\"val_2865\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"val_2881\"<INT64,[2]>{Tensor<INT64,[2]>(array([256, 256]), name='val_2881')},\n",
       "                %\"val_2887\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"val_2903\"<INT64,[2]>{Tensor<INT64,[2]>(array([512, 512]), name='val_2903')},\n",
       "                %\"val_2909\"<FLOAT,[16]>{Tensor(...)}\n",
       "            ),\n",
       "        ) {\n",
       "               0 |  # node_Shape_0\n",
       "                    %\"val_0\"<INT64,[1]>  ::Shape(%\"input\") {end=1, start=0}\n",
       "               1 |  # node_sym_size_int_37\n",
       "                    %\"sym_size_int_37\"<INT64,[]>  ::Squeeze(%\"val_0\")\n",
       "               2 |  # node_select\n",
       "                    %\"select\"<FLOAT,[batch,512,512]>  ::Gather(%\"input\", %\"val_1\"{0}) {axis=1}\n",
       "               3 |  # node_unsqueeze\n",
       "                    %\"unsqueeze\"<FLOAT,[batch,1,512,512]>  ::Unsqueeze(%\"select\", %\"val_2\"{[1]})\n",
       "               4 |  # node_select_1\n",
       "                    %\"select_1\"<FLOAT,[batch,512,512]>  ::Gather(%\"input\", %\"val_3\"{1}) {axis=1}\n",
       "               5 |  # node_unsqueeze_1\n",
       "                    %\"unsqueeze_1\"<FLOAT,[batch,1,512,512]>  ::Unsqueeze(%\"select_1\", %\"val_2\"{[1]})\n",
       "               6 |  # node_select_2\n",
       "                    %\"select_2\"<FLOAT,[batch,512,512]>  ::Gather(%\"input\", %\"val_4\"{2}) {axis=1}\n",
       "               7 |  # node_unsqueeze_2\n",
       "                    %\"unsqueeze_2\"<FLOAT,[batch,1,512,512]>  ::Unsqueeze(%\"select_2\", %\"val_2\"{[1]})\n",
       "               8 |  # node_select_3\n",
       "                    %\"select_3\"<FLOAT,[batch,512,512]>  ::Gather(%\"input\", %\"val_5\"{3}) {axis=1}\n",
       "               9 |  # node_unsqueeze_3\n",
       "                    %\"unsqueeze_3\"<FLOAT,[batch,1,512,512]>  ::Unsqueeze(%\"select_3\", %\"val_2\"{[1]})\n",
       "              10 |  # node_select_4\n",
       "                    %\"select_4\"<FLOAT,[batch,512,512]>  ::Gather(%\"input\", %\"val_6\"{4}) {axis=1}\n",
       "              11 |  # node_unsqueeze_4\n",
       "                    %\"unsqueeze_4\"<FLOAT,[batch,1,512,512]>  ::Unsqueeze(%\"select_4\", %\"val_2\"{[1]})\n",
       "              12 |  # node_sub_10\n",
       "                    %\"sub_10\"<FLOAT,[batch,1,512,512]>  ::Sub(%\"unsqueeze_3\", %\"unsqueeze_2\")\n",
       "              13 |  # node_add_50\n",
       "                    %\"add_50\"<FLOAT,[batch,1,512,512]>  ::Add(%\"unsqueeze_3\", %\"unsqueeze_2\")\n",
       "              14 |  # node_div\n",
       "                    %\"div\"<FLOAT,[batch,1,512,512]>  ::Div(%\"sub_10\", %\"add_50\")\n",
       "              15 |  # node_sub_15\n",
       "                    %\"sub_15\"<FLOAT,[batch,1,512,512]>  ::Sub(%\"unsqueeze_3\", %\"unsqueeze_1\")\n",
       "              16 |  # node_add_72\n",
       "                    %\"add_72\"<FLOAT,[batch,1,512,512]>  ::Add(%\"unsqueeze_3\", %\"unsqueeze_1\")\n",
       "              17 |  # node_div_1\n",
       "                    %\"div_1\"<FLOAT,[batch,1,512,512]>  ::Div(%\"sub_15\", %\"add_72\")\n",
       "              18 |  # node_sub_20\n",
       "                    %\"sub_20\"<FLOAT,[batch,1,512,512]>  ::Sub(%\"unsqueeze_4\", %\"unsqueeze_2\")\n",
       "              19 |  # node_add_94\n",
       "                    %\"add_94\"<FLOAT,[batch,1,512,512]>  ::Add(%\"unsqueeze_4\", %\"unsqueeze_2\")\n",
       "              20 |  # node_div_2\n",
       "                    %\"div_2\"<FLOAT,[batch,1,512,512]>  ::Div(%\"sub_20\", %\"add_94\")\n",
       "              21 |  # node_sub_25\n",
       "                    %\"sub_25\"<FLOAT,[batch,1,512,512]>  ::Sub(%\"unsqueeze_1\", %\"unsqueeze_3\")\n",
       "              22 |  # node_add_116\n",
       "                    %\"add_116\"<FLOAT,[batch,1,512,512]>  ::Add(%\"unsqueeze_1\", %\"unsqueeze_3\")\n",
       "              23 |  # node_div_3\n",
       "                    %\"div_3\"<FLOAT,[batch,1,512,512]>  ::Div(%\"sub_25\", %\"add_116\")\n",
       "              24 |  # node_div_4\n",
       "                    %\"div_4\"<FLOAT,[batch,1,512,512]>  ::Div(%\"unsqueeze_3\", %\"unsqueeze_1\")\n",
       "              25 |  # node_sub_32\n",
       "                    %\"sub_32\"<FLOAT,[batch,1,512,512]>  ::Sub(%\"div_4\", %\"scalar_tensor_default\"{1.0})\n",
       "              26 |  # node_cat\n",
       "                    %\"cat\"<FLOAT,[batch,10,512,512]>  ::Concat(%\"unsqueeze\", %\"unsqueeze_1\", %\"unsqueeze_2\", %\"unsqueeze_3\", %\"unsqueeze_4\", %\"div\", %\"div_3\", %\"div_1\", %\"sub_32\", %\"div_2\") {axis=1}\n",
       "              27 |  # node_sub_35\n",
       "                    %\"sub_35\"<FLOAT,[batch,10,512,512]>  ::Sub(%\"cat\", %\"per_channel_mean\"{[[[[193.35716247558594]], [[253.69332885742188]], [[141.6480255126953]], [[999.2923583984375]], [[321.69390869140625]], [[0.06497050076723099]], [[0.15727300941944122]], [[-0.15727300941944122]], [[18222916.0]], [[0.1098066195845604]]]]})\n",
       "              28 |  # node_div_5\n",
       "                    %\"div_5\"<FLOAT,[batch,10,512,512]>  ::Div(%\"sub_35\", %\"per_channel_std\"{[[[[155.69749450683594]], [[212.7003631591797]], [[204.0181121826172]], [[1275.88134765625]], [[377.3244323730469]], [[0.6752511858940125]], [[0.7329661846160889]], [[0.7329661846160889]], [[21676881920.0]], [[0.4112321138381958]]]]})\n",
       "              29 |  # node_Conv_3446\n",
       "                    %\"getitem\"<FLOAT,[batch,64,256,256]>  ::Conv(%\"div_5\", %\"model.encoder.model.stem.conv1.weight\"{...}, %\"model.encoder.model.stem.conv1.bias\"{...}) {group=1, pads=(0, 0, 1, 1), strides=(2, 2), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "              30 |  # node_Pow_17\n",
       "                    %\"val_17\"<FLOAT,[batch,64,256,256]>  ::Pow(%\"getitem\", %\"val_16\"{3.0})\n",
       "              31 |  # node_Mul_19\n",
       "                    %\"val_19\"<FLOAT,[batch,64,256,256]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_17\")\n",
       "              32 |  # node_Add_20\n",
       "                    %\"val_20\"<FLOAT,[batch,64,256,256]>  ::Add(%\"getitem\", %\"val_19\")\n",
       "              33 |  # node_Mul_22\n",
       "                    %\"val_22\"<FLOAT,[batch,64,256,256]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_20\")\n",
       "              34 |  # node_Tanh_23\n",
       "                    %\"val_23\"<FLOAT,[batch,64,256,256]>  ::Tanh(%\"val_22\")\n",
       "              35 |  # node_Add_25\n",
       "                    %\"val_25\"<FLOAT,[batch,64,256,256]>  ::Add(%\"val_23\", %\"scalar_tensor_default\"{1.0})\n",
       "              36 |  # node_Mul_27\n",
       "                    %\"val_27\"<FLOAT,[batch,64,256,256]>  ::Mul(%\"val_26\"{0.5}, %\"val_25\")\n",
       "              37 |  # node_gelu\n",
       "                    %\"gelu\"<FLOAT,[batch,64,256,256]>  ::Mul(%\"getitem\", %\"val_27\")\n",
       "              38 |  # node_conv2d_1\n",
       "                    %\"conv2d_1\"<FLOAT,[batch,64,256,256]>  ::Conv(%\"gelu\", %\"model.encoder.model.stem.conv2.weight\"{...}, %\"model.encoder.model.stem.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "              39 |  # node_pad_1\n",
       "                    %\"pad_1\"<FLOAT,[batch,64,256,256]>  ::Pad(%\"conv2d_1\", %\"val_28\"{[0, 0, 0, 0, 0, 0, 0, 0]}, %\"val_29\"{0.0}) {mode='constant'}\n",
       "              40 |  # node_avg_pool2d\n",
       "                    %\"avg_pool2d\"<FLOAT,[batch,64,128,128]>  ::AveragePool(%\"pad_1\") {count_include_pad=1, ceil_mode=0, pads=(0, 0, 0, 0), kernel_shape=(2, 2), strides=(2, 2), auto_pad='NOTSET'}\n",
       "              41 |  # node__native_batch_norm_legit_no_training_1__0\n",
       "                    %\"getitem_3\"<FLOAT,[batch,64,256,256]>  ::BatchNormalization(%\"conv2d_1\", %\"model.encoder.model.stages_0.blocks.0.conv.pre_norm.weight\"{...}, %\"model.encoder.model.stages_0.blocks.0.conv.pre_norm.bias\"{...}, %\"model.encoder.model.stages_0.blocks.0.conv.pre_norm.running_mean\"{...}, %\"model.encoder.model.stages_0.blocks.0.conv.pre_norm.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "              42 |  # node_conv2d_2\n",
       "                    %\"conv2d_2\"<FLOAT,[batch,256,256,256]>  ::Conv(%\"getitem_3\", %\"model.encoder.model.stages_0.blocks.0.conv.conv1_1x1.weight\"{...}, %\"val_39\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "              43 |  # node__native_batch_norm_legit_no_training_2__0\n",
       "                    %\"getitem_6\"<FLOAT,[batch,256,256,256]>  ::BatchNormalization(%\"conv2d_2\", %\"model.encoder.model.stages_0.blocks.0.conv.norm1.weight\"{...}, %\"model.encoder.model.stages_0.blocks.0.conv.norm1.bias\"{...}, %\"model.encoder.model.stages_0.blocks.0.conv.norm1.running_mean\"{...}, %\"model.encoder.model.stages_0.blocks.0.conv.norm1.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "              44 |  # node_Pow_45\n",
       "                    %\"val_45\"<FLOAT,[batch,256,256,256]>  ::Pow(%\"getitem_6\", %\"val_16\"{3.0})\n",
       "              45 |  # node_Mul_47\n",
       "                    %\"val_47\"<FLOAT,[batch,256,256,256]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_45\")\n",
       "              46 |  # node_Add_48\n",
       "                    %\"val_48\"<FLOAT,[batch,256,256,256]>  ::Add(%\"getitem_6\", %\"val_47\")\n",
       "              47 |  # node_Mul_50\n",
       "                    %\"val_50\"<FLOAT,[batch,256,256,256]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_48\")\n",
       "              48 |  # node_Tanh_51\n",
       "                    %\"val_51\"<FLOAT,[batch,256,256,256]>  ::Tanh(%\"val_50\")\n",
       "              49 |  # node_Add_53\n",
       "                    %\"val_53\"<FLOAT,[batch,256,256,256]>  ::Add(%\"val_51\", %\"scalar_tensor_default\"{1.0})\n",
       "              50 |  # node_Mul_55\n",
       "                    %\"val_55\"<FLOAT,[batch,256,256,256]>  ::Mul(%\"val_26\"{0.5}, %\"val_53\")\n",
       "              51 |  # node_gelu_1\n",
       "                    %\"gelu_1\"<FLOAT,[batch,256,256,256]>  ::Mul(%\"getitem_6\", %\"val_55\")\n",
       "              52 |  # node_conv2d_3\n",
       "                    %\"conv2d_3\"<FLOAT,[batch,256,128,128]>  ::Conv(%\"gelu_1\", %\"model.encoder.model.stages_0.blocks.0.conv.conv2_kxk.weight\"{...}, %\"val_39\"{...}) {group=256, pads=(0, 0, 1, 1), strides=(2, 2), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "              53 |  # node__native_batch_norm_legit_no_training_3__0\n",
       "                    %\"getitem_9\"<FLOAT,[batch,256,128,128]>  ::BatchNormalization(%\"conv2d_3\", %\"model.encoder.model.stages_0.blocks.0.conv.norm2.weight\"{...}, %\"model.encoder.model.stages_0.blocks.0.conv.norm2.bias\"{...}, %\"model.encoder.model.stages_0.blocks.0.conv.norm2.running_mean\"{...}, %\"model.encoder.model.stages_0.blocks.0.conv.norm2.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "              54 |  # node_Pow_68\n",
       "                    %\"val_68\"<FLOAT,[batch,256,128,128]>  ::Pow(%\"getitem_9\", %\"val_16\"{3.0})\n",
       "              55 |  # node_Mul_70\n",
       "                    %\"val_70\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_68\")\n",
       "              56 |  # node_Add_71\n",
       "                    %\"val_71\"<FLOAT,[batch,256,128,128]>  ::Add(%\"getitem_9\", %\"val_70\")\n",
       "              57 |  # node_Mul_73\n",
       "                    %\"val_73\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_71\")\n",
       "              58 |  # node_Tanh_74\n",
       "                    %\"val_74\"<FLOAT,[batch,256,128,128]>  ::Tanh(%\"val_73\")\n",
       "              59 |  # node_Add_76\n",
       "                    %\"val_76\"<FLOAT,[batch,256,128,128]>  ::Add(%\"val_74\", %\"scalar_tensor_default\"{1.0})\n",
       "              60 |  # node_Mul_78\n",
       "                    %\"val_78\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"val_26\"{0.5}, %\"val_76\")\n",
       "              61 |  # node_gelu_2\n",
       "                    %\"gelu_2\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"getitem_9\", %\"val_78\")\n",
       "              62 |  # node_mean\n",
       "                    %\"mean\"<FLOAT,[batch,256,1,1]>  ::ReduceMean(%\"gelu_2\", %\"val_81\"{[2, 3]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "              63 |  # node_conv2d_4\n",
       "                    %\"conv2d_4\"<FLOAT,[batch,16,1,1]>  ::Conv(%\"mean\", %\"model.encoder.model.stages_0.blocks.0.conv.se.fc1.weight\"{...}, %\"model.encoder.model.stages_0.blocks.0.conv.se.fc1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "              64 |  # node_Sigmoid_82\n",
       "                    %\"val_82\"<FLOAT,[batch,16,1,1]>  ::Sigmoid(%\"conv2d_4\")\n",
       "              65 |  # node_silu\n",
       "                    %\"silu\"<FLOAT,[batch,16,1,1]>  ::Mul(%\"conv2d_4\", %\"val_82\")\n",
       "              66 |  # node_conv2d_5\n",
       "                    %\"conv2d_5\"<FLOAT,[batch,256,1,1]>  ::Conv(%\"silu\", %\"model.encoder.model.stages_0.blocks.0.conv.se.fc2.weight\"{...}, %\"model.encoder.model.stages_0.blocks.0.conv.se.fc2.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "              67 |  # node_sigmoid\n",
       "                    %\"sigmoid\"<FLOAT,[batch,256,1,1]>  ::Sigmoid(%\"conv2d_5\")\n",
       "              68 |  # node_mul_126\n",
       "                    %\"mul_126\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"gelu_2\", %\"sigmoid\")\n",
       "              69 |  # node_conv2d_6\n",
       "                    %\"conv2d_6\"<FLOAT,[batch,64,128,128]>  ::Conv(%\"mul_126\", %\"model.encoder.model.stages_0.blocks.0.conv.conv3_1x1.weight\"{...}, %\"model.encoder.model.stages_0.blocks.0.conv.conv3_1x1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "              70 |  # node_add_280\n",
       "                    %\"add_280\"<FLOAT,[batch,64,128,128]>  ::Add(%\"conv2d_6\", %\"avg_pool2d\")\n",
       "              71 |  # node_permute\n",
       "                    %\"permute\"<FLOAT,[batch,128,128,64]>  ::Transpose(%\"add_280\") {perm=(0, 2, 3, 1)}\n",
       "              72 |  # node_layer_norm\n",
       "                    %\"layer_norm\"<FLOAT,[batch,128,128,64]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute\", %\"model.encoder.model.stages_0.blocks.0.attn_block.norm1.weight\"{...}, %\"model.encoder.model.stages_0.blocks.0.attn_block.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "              73 |  # node_Concat_90\n",
       "                    %\"val_92\"<INT64,[6]>  ::Concat(%\"val_0\", %\"val_87\"{[8]}, %\"val_88\"{[16]}, %\"val_87\"{[8]}, %\"val_88\"{[16]}, %\"val_91\"{[64]}) {axis=0}\n",
       "              74 |  # node_view\n",
       "                    %\"view\"<FLOAT,[batch,8,16,8,16,64]>  ::Reshape(%\"layer_norm\", %\"val_92\") {allowzero=1}\n",
       "              75 |  # node_permute_1\n",
       "                    %\"permute_1\"<FLOAT,[batch,8,8,16,16,64]>  ::Transpose(%\"view\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "              76 |  # node_view_1\n",
       "                    %\"view_1\"<FLOAT,[64*batch,16,16,64]>  ::Reshape(%\"permute_1\", %\"val_98\"{[-1, 16, 16, 64]}) {allowzero=1}\n",
       "              77 |  # node_MatMul_98\n",
       "                    %\"val_100\"<FLOAT,[64*batch,16,16,192]>  ::MatMul(%\"view_1\", %\"val_99\"{...})\n",
       "              78 |  # node_linear\n",
       "                    %\"linear\"<FLOAT,[64*batch,16,16,192]>  ::Add(%\"val_100\", %\"model.encoder.model.stages_0.blocks.0.attn_block.attn.qkv.bias\"{...})\n",
       "              79 |  # node_mul_153\n",
       "                    %\"mul_153\"<INT64,[]>  ::Mul(%\"val_101\"{64}, %\"sym_size_int_37\")\n",
       "              80 |  # node_Reshape_101\n",
       "                    %\"val_103\"<INT64,[1]>  ::Reshape(%\"mul_153\", %\"val_102\"{[-1]}) {allowzero=0}\n",
       "              81 |  # node_Concat_106\n",
       "                    %\"val_108\"<INT64,[5]>  ::Concat(%\"val_103\", %\"val_102\"{[-1]}, %\"val_105\"{[3]}, %\"val_106\"{[2]}, %\"val_107\"{[32]}) {axis=0}\n",
       "              82 |  # node_view_2\n",
       "                    %\"view_2\"<FLOAT,[64*batch,256,3,2,32]>  ::Reshape(%\"linear\", %\"val_108\") {allowzero=1}\n",
       "              83 |  # node_transpose\n",
       "                    %\"transpose\"<FLOAT,[64*batch,2,3,256,32]>  ::Transpose(%\"view_2\") {perm=(0, 3, 2, 1, 4)}\n",
       "              84 |  # node_Split_107\n",
       "                    %\"val_109\"<FLOAT,[64*batch,2,1,256,32]>, %\"val_110\"<FLOAT,[64*batch,2,1,256,32]>, %\"val_111\"<FLOAT,[64*batch,2,1,256,32]>  ::Split(%\"transpose\") {num_outputs=3, axis=2}\n",
       "              85 |  # node_unbind__0\n",
       "                    %\"getitem_12\"<FLOAT,[64*batch,2,256,32]>  ::Squeeze(%\"val_109\", %\"val_106\"{[2]})\n",
       "              86 |  # node_unbind__1\n",
       "                    %\"getitem_13\"<FLOAT,[64*batch,2,256,32]>  ::Squeeze(%\"val_110\", %\"val_106\"{[2]})\n",
       "              87 |  # node_unbind__2\n",
       "                    %\"getitem_14\"<FLOAT,[64*batch,2,256,32]>  ::Squeeze(%\"val_111\", %\"val_106\"{[2]})\n",
       "              88 |  # node_einsum_1\n",
       "                    %\"einsum_1\"<FLOAT,[2,16,16,16,16]>  ::Einsum(%\"einsum\"{...}, %\"model.encoder.model.stages_0.blocks.0.attn_block.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "              89 |  # node__unsafe_view\n",
       "                    %\"_unsafe_view\"<FLOAT,[2,256,256]>  ::Reshape(%\"einsum_1\", %\"val_117\"{[2, 256, 256]}) {allowzero=1}\n",
       "              90 |  # node_Shape_123\n",
       "                    %\"val_127\"<INT64,[4]>  ::Shape(%\"getitem_13\") {start=0}\n",
       "              91 |  # node_Slice_126\n",
       "                    %\"val_130\"<INT64,[1]>  ::Slice(%\"val_127\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "              92 |  # node_Slice_128\n",
       "                    %\"val_132\"<INT64,[1]>  ::Slice(%\"val_127\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "              93 |  # node_Slice_130\n",
       "                    %\"val_134\"<INT64,[2]>  ::Slice(%\"val_127\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "              94 |  # node_Concat_132\n",
       "                    %\"val_136\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_132\", %\"val_130\") {axis=0}\n",
       "              95 |  # node_Reshape_133\n",
       "                    %\"val_137\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_13\", %\"val_136\") {allowzero=0}\n",
       "              96 |  # node_Transpose_134\n",
       "                    %\"val_138\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_137\") {perm=(0, 2, 1)}\n",
       "              97 |  # node_Concat_135\n",
       "                    %\"val_139\"<INT64,[4]>  ::Concat(%\"val_134\", %\"val_130\", %\"val_132\") {axis=0}\n",
       "              98 |  # node_Reshape_136\n",
       "                    %\"val_140\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_138\", %\"val_139\") {allowzero=0}\n",
       "              99 |  # node_Mul_138\n",
       "                    %\"val_142\"<FLOAT,[64*batch,2,256,32]>  ::Mul(%\"getitem_12\", %\"val_141\"{[0.4204482138156891]})\n",
       "             100 |  # node_Mul_140\n",
       "                    %\"val_144\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_140\", %\"val_141\"{[0.4204482138156891]})\n",
       "             101 |  # node_MatMul_141\n",
       "                    %\"val_145\"<FLOAT,[None,2,256,None]>  ::MatMul(%\"val_142\", %\"val_144\")\n",
       "             102 |  # node_Add_142\n",
       "                    %\"val_146\"<FLOAT,[None,2,256,256]>  ::Add(%\"val_145\", %\"_unsafe_view\")\n",
       "             103 |  # node_Softmax_143\n",
       "                    %\"val_147\"<FLOAT,[None,2,256,256]>  ::Softmax(%\"val_146\") {axis=-1}\n",
       "             104 |  # node_scaled_dot_product_attention\n",
       "                    %\"scaled_dot_product_attention\"<FLOAT,[64*batch,2,256,32]>  ::MatMul(%\"val_147\", %\"getitem_14\")\n",
       "             105 |  # node_transpose_1\n",
       "                    %\"transpose_1\"<FLOAT,[64*batch,256,2,32]>  ::Transpose(%\"scaled_dot_product_attention\") {perm=(0, 2, 1, 3)}\n",
       "             106 |  # node_Concat_149\n",
       "                    %\"val_153\"<INT64,[4]>  ::Concat(%\"val_103\", %\"val_88\"{[16]}, %\"val_88\"{[16]}, %\"val_91\"{[64]}) {axis=0}\n",
       "             107 |  # node__unsafe_view_1\n",
       "                    %\"_unsafe_view_1\"<FLOAT,[64*batch,16,16,64]>  ::Reshape(%\"transpose_1\", %\"val_153\") {allowzero=1}\n",
       "             108 |  # node_MatMul_151\n",
       "                    %\"val_155\"<FLOAT,[64*batch,16,16,64]>  ::MatMul(%\"_unsafe_view_1\", %\"val_154\"{...})\n",
       "             109 |  # node_linear_1\n",
       "                    %\"linear_1\"<FLOAT,[64*batch,16,16,64]>  ::Add(%\"val_155\", %\"model.encoder.model.stages_0.blocks.0.attn_block.attn.proj.bias\"{...})\n",
       "             110 |  # node_view_3\n",
       "                    %\"view_3\"<FLOAT,[batch,8,8,16,16,64]>  ::Reshape(%\"linear_1\", %\"val_163\"{[-1, 8, 8, 16, 16, 64]}) {allowzero=1}\n",
       "             111 |  # node_permute_2\n",
       "                    %\"permute_2\"<FLOAT,[batch,8,16,8,16,64]>  ::Transpose(%\"view_3\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "             112 |  # node_view_4\n",
       "                    %\"view_4\"<FLOAT,[batch,128,128,64]>  ::Reshape(%\"permute_2\", %\"val_169\"{[-1, 128, 128, 64]}) {allowzero=1}\n",
       "             113 |  # node_add_410\n",
       "                    %\"add_410\"<FLOAT,[batch,128,128,64]>  ::Add(%\"permute\", %\"view_4\")\n",
       "             114 |  # node_layer_norm_1\n",
       "                    %\"layer_norm_1\"<FLOAT,[batch,128,128,64]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_410\", %\"model.encoder.model.stages_0.blocks.0.attn_block.norm2.weight\"{...}, %\"model.encoder.model.stages_0.blocks.0.attn_block.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             115 |  # node_MatMul_167\n",
       "                    %\"val_173\"<FLOAT,[batch,128,128,256]>  ::MatMul(%\"layer_norm_1\", %\"val_172\"{...})\n",
       "             116 |  # node_linear_2\n",
       "                    %\"linear_2\"<FLOAT,[batch,128,128,256]>  ::Add(%\"val_173\", %\"model.encoder.model.stages_0.blocks.0.attn_block.mlp.fc1.bias\"{...})\n",
       "             117 |  # node_Pow_169\n",
       "                    %\"val_175\"<FLOAT,[batch,128,128,256]>  ::Pow(%\"linear_2\", %\"val_16\"{3.0})\n",
       "             118 |  # node_Mul_171\n",
       "                    %\"val_177\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_175\")\n",
       "             119 |  # node_Add_172\n",
       "                    %\"val_178\"<FLOAT,[batch,128,128,256]>  ::Add(%\"linear_2\", %\"val_177\")\n",
       "             120 |  # node_Mul_174\n",
       "                    %\"val_180\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_178\")\n",
       "             121 |  # node_Tanh_175\n",
       "                    %\"val_181\"<FLOAT,[batch,128,128,256]>  ::Tanh(%\"val_180\")\n",
       "             122 |  # node_Add_177\n",
       "                    %\"val_183\"<FLOAT,[batch,128,128,256]>  ::Add(%\"val_181\", %\"scalar_tensor_default\"{1.0})\n",
       "             123 |  # node_Mul_179\n",
       "                    %\"val_185\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"val_26\"{0.5}, %\"val_183\")\n",
       "             124 |  # node_gelu_3\n",
       "                    %\"gelu_3\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"linear_2\", %\"val_185\")\n",
       "             125 |  # node_MatMul_181\n",
       "                    %\"val_187\"<FLOAT,[batch,128,128,64]>  ::MatMul(%\"gelu_3\", %\"val_186\"{...})\n",
       "             126 |  # node_linear_3\n",
       "                    %\"linear_3\"<FLOAT,[batch,128,128,64]>  ::Add(%\"val_187\", %\"model.encoder.model.stages_0.blocks.0.attn_block.mlp.fc2.bias\"{...})\n",
       "             127 |  # node_add_446\n",
       "                    %\"add_446\"<FLOAT,[batch,128,128,64]>  ::Add(%\"add_410\", %\"linear_3\")\n",
       "             128 |  # node_layer_norm_2\n",
       "                    %\"layer_norm_2\"<FLOAT,[batch,128,128,64]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_446\", %\"model.encoder.model.stages_0.blocks.0.attn_grid.norm1.weight\"{...}, %\"model.encoder.model.stages_0.blocks.0.attn_grid.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             129 |  # node_Concat_189\n",
       "                    %\"val_197\"<INT64,[6]>  ::Concat(%\"val_0\", %\"val_88\"{[16]}, %\"val_87\"{[8]}, %\"val_88\"{[16]}, %\"val_87\"{[8]}, %\"val_91\"{[64]}) {axis=0}\n",
       "             130 |  # node_view_5\n",
       "                    %\"view_5\"<FLOAT,[batch,16,8,16,8,64]>  ::Reshape(%\"layer_norm_2\", %\"val_197\") {allowzero=1}\n",
       "             131 |  # node_permute_3\n",
       "                    %\"permute_3\"<FLOAT,[batch,8,8,16,16,64]>  ::Transpose(%\"view_5\") {perm=(0, 2, 4, 1, 3, 5)}\n",
       "             132 |  # node_view_6\n",
       "                    %\"view_6\"<FLOAT,[64*batch,16,16,64]>  ::Reshape(%\"permute_3\", %\"val_98\"{[-1, 16, 16, 64]}) {allowzero=1}\n",
       "             133 |  # node_MatMul_197\n",
       "                    %\"val_205\"<FLOAT,[64*batch,16,16,192]>  ::MatMul(%\"view_6\", %\"val_204\"{...})\n",
       "             134 |  # node_linear_4\n",
       "                    %\"linear_4\"<FLOAT,[64*batch,16,16,192]>  ::Add(%\"val_205\", %\"model.encoder.model.stages_0.blocks.0.attn_grid.attn.qkv.bias\"{...})\n",
       "             135 |  # node_view_7\n",
       "                    %\"view_7\"<FLOAT,[64*batch,256,3,2,32]>  ::Reshape(%\"linear_4\", %\"val_108\") {allowzero=1}\n",
       "             136 |  # node_transpose_2\n",
       "                    %\"transpose_2\"<FLOAT,[64*batch,2,3,256,32]>  ::Transpose(%\"view_7\") {perm=(0, 3, 2, 1, 4)}\n",
       "             137 |  # node_Split_205\n",
       "                    %\"val_213\"<FLOAT,[64*batch,2,1,256,32]>, %\"val_214\"<FLOAT,[64*batch,2,1,256,32]>, %\"val_215\"<FLOAT,[64*batch,2,1,256,32]>  ::Split(%\"transpose_2\") {num_outputs=3, axis=2}\n",
       "             138 |  # node_unbind_1__0\n",
       "                    %\"getitem_15\"<FLOAT,[64*batch,2,256,32]>  ::Squeeze(%\"val_213\", %\"val_106\"{[2]})\n",
       "             139 |  # node_unbind_1__1\n",
       "                    %\"getitem_16\"<FLOAT,[64*batch,2,256,32]>  ::Squeeze(%\"val_214\", %\"val_106\"{[2]})\n",
       "             140 |  # node_unbind_1__2\n",
       "                    %\"getitem_17\"<FLOAT,[64*batch,2,256,32]>  ::Squeeze(%\"val_215\", %\"val_106\"{[2]})\n",
       "             141 |  # node_einsum_3\n",
       "                    %\"einsum_3\"<FLOAT,[2,16,16,16,16]>  ::Einsum(%\"einsum_2\"{...}, %\"model.encoder.model.stages_0.blocks.0.attn_grid.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "             142 |  # node__unsafe_view_2\n",
       "                    %\"_unsafe_view_2\"<FLOAT,[2,256,256]>  ::Reshape(%\"einsum_3\", %\"val_117\"{[2, 256, 256]}) {allowzero=1}\n",
       "             143 |  # node_Shape_220\n",
       "                    %\"val_230\"<INT64,[4]>  ::Shape(%\"getitem_16\") {start=0}\n",
       "             144 |  # node_Slice_222\n",
       "                    %\"val_232\"<INT64,[1]>  ::Slice(%\"val_230\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "             145 |  # node_Slice_223\n",
       "                    %\"val_233\"<INT64,[1]>  ::Slice(%\"val_230\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "             146 |  # node_Slice_225\n",
       "                    %\"val_235\"<INT64,[2]>  ::Slice(%\"val_230\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "             147 |  # node_Concat_227\n",
       "                    %\"val_237\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_233\", %\"val_232\") {axis=0}\n",
       "             148 |  # node_Reshape_228\n",
       "                    %\"val_238\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_16\", %\"val_237\") {allowzero=0}\n",
       "             149 |  # node_Transpose_229\n",
       "                    %\"val_239\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_238\") {perm=(0, 2, 1)}\n",
       "             150 |  # node_Concat_230\n",
       "                    %\"val_240\"<INT64,[4]>  ::Concat(%\"val_235\", %\"val_232\", %\"val_233\") {axis=0}\n",
       "             151 |  # node_Reshape_231\n",
       "                    %\"val_241\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_239\", %\"val_240\") {allowzero=0}\n",
       "             152 |  # node_Mul_233\n",
       "                    %\"val_243\"<FLOAT,[64*batch,2,256,32]>  ::Mul(%\"getitem_15\", %\"val_141\"{[0.4204482138156891]})\n",
       "             153 |  # node_Mul_235\n",
       "                    %\"val_245\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_241\", %\"val_141\"{[0.4204482138156891]})\n",
       "             154 |  # node_MatMul_236\n",
       "                    %\"val_246\"<FLOAT,[None,2,256,None]>  ::MatMul(%\"val_243\", %\"val_245\")\n",
       "             155 |  # node_Add_237\n",
       "                    %\"val_247\"<FLOAT,[None,2,256,256]>  ::Add(%\"val_246\", %\"_unsafe_view_2\")\n",
       "             156 |  # node_Softmax_238\n",
       "                    %\"val_248\"<FLOAT,[None,2,256,256]>  ::Softmax(%\"val_247\") {axis=-1}\n",
       "             157 |  # node_scaled_dot_product_attention_1\n",
       "                    %\"scaled_dot_product_attention_1\"<FLOAT,[64*batch,2,256,32]>  ::MatMul(%\"val_248\", %\"getitem_17\")\n",
       "             158 |  # node_transpose_3\n",
       "                    %\"transpose_3\"<FLOAT,[64*batch,256,2,32]>  ::Transpose(%\"scaled_dot_product_attention_1\") {perm=(0, 2, 1, 3)}\n",
       "             159 |  # node__unsafe_view_3\n",
       "                    %\"_unsafe_view_3\"<FLOAT,[64*batch,16,16,64]>  ::Reshape(%\"transpose_3\", %\"val_153\") {allowzero=1}\n",
       "             160 |  # node_MatMul_246\n",
       "                    %\"val_256\"<FLOAT,[64*batch,16,16,64]>  ::MatMul(%\"_unsafe_view_3\", %\"val_255\"{...})\n",
       "             161 |  # node_linear_5\n",
       "                    %\"linear_5\"<FLOAT,[64*batch,16,16,64]>  ::Add(%\"val_256\", %\"model.encoder.model.stages_0.blocks.0.attn_grid.attn.proj.bias\"{...})\n",
       "             162 |  # node_view_8\n",
       "                    %\"view_8\"<FLOAT,[batch,8,8,16,16,64]>  ::Reshape(%\"linear_5\", %\"val_163\"{[-1, 8, 8, 16, 16, 64]}) {allowzero=1}\n",
       "             163 |  # node_permute_4\n",
       "                    %\"permute_4\"<FLOAT,[batch,16,8,16,8,64]>  ::Transpose(%\"view_8\") {perm=(0, 3, 1, 4, 2, 5)}\n",
       "             164 |  # node_view_9\n",
       "                    %\"view_9\"<FLOAT,[batch,128,128,64]>  ::Reshape(%\"permute_4\", %\"val_169\"{[-1, 128, 128, 64]}) {allowzero=1}\n",
       "             165 |  # node_add_571\n",
       "                    %\"add_571\"<FLOAT,[batch,128,128,64]>  ::Add(%\"add_446\", %\"view_9\")\n",
       "             166 |  # node_layer_norm_3\n",
       "                    %\"layer_norm_3\"<FLOAT,[batch,128,128,64]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_571\", %\"model.encoder.model.stages_0.blocks.0.attn_grid.norm2.weight\"{...}, %\"model.encoder.model.stages_0.blocks.0.attn_grid.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             167 |  # node_MatMul_262\n",
       "                    %\"val_274\"<FLOAT,[batch,128,128,256]>  ::MatMul(%\"layer_norm_3\", %\"val_273\"{...})\n",
       "             168 |  # node_linear_6\n",
       "                    %\"linear_6\"<FLOAT,[batch,128,128,256]>  ::Add(%\"val_274\", %\"model.encoder.model.stages_0.blocks.0.attn_grid.mlp.fc1.bias\"{...})\n",
       "             169 |  # node_Pow_264\n",
       "                    %\"val_276\"<FLOAT,[batch,128,128,256]>  ::Pow(%\"linear_6\", %\"val_16\"{3.0})\n",
       "             170 |  # node_Mul_266\n",
       "                    %\"val_278\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_276\")\n",
       "             171 |  # node_Add_267\n",
       "                    %\"val_279\"<FLOAT,[batch,128,128,256]>  ::Add(%\"linear_6\", %\"val_278\")\n",
       "             172 |  # node_Mul_269\n",
       "                    %\"val_281\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_279\")\n",
       "             173 |  # node_Tanh_270\n",
       "                    %\"val_282\"<FLOAT,[batch,128,128,256]>  ::Tanh(%\"val_281\")\n",
       "             174 |  # node_Add_272\n",
       "                    %\"val_284\"<FLOAT,[batch,128,128,256]>  ::Add(%\"val_282\", %\"scalar_tensor_default\"{1.0})\n",
       "             175 |  # node_Mul_274\n",
       "                    %\"val_286\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"val_26\"{0.5}, %\"val_284\")\n",
       "             176 |  # node_gelu_4\n",
       "                    %\"gelu_4\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"linear_6\", %\"val_286\")\n",
       "             177 |  # node_MatMul_276\n",
       "                    %\"val_288\"<FLOAT,[batch,128,128,64]>  ::MatMul(%\"gelu_4\", %\"val_287\"{...})\n",
       "             178 |  # node_linear_7\n",
       "                    %\"linear_7\"<FLOAT,[batch,128,128,64]>  ::Add(%\"val_288\", %\"model.encoder.model.stages_0.blocks.0.attn_grid.mlp.fc2.bias\"{...})\n",
       "             179 |  # node_add_607\n",
       "                    %\"add_607\"<FLOAT,[batch,128,128,64]>  ::Add(%\"add_571\", %\"linear_7\")\n",
       "             180 |  # node_permute_5\n",
       "                    %\"permute_5\"<FLOAT,[batch,64,128,128]>  ::Transpose(%\"add_607\") {perm=(0, 3, 1, 2)}\n",
       "             181 |  # node__native_batch_norm_legit_no_training_4__0\n",
       "                    %\"getitem_18\"<FLOAT,[batch,64,128,128]>  ::BatchNormalization(%\"permute_5\", %\"model.encoder.model.stages_0.blocks.1.conv.pre_norm.weight\"{...}, %\"model.encoder.model.stages_0.blocks.1.conv.pre_norm.bias\"{...}, %\"model.encoder.model.stages_0.blocks.1.conv.pre_norm.running_mean\"{...}, %\"model.encoder.model.stages_0.blocks.1.conv.pre_norm.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             182 |  # node_conv2d_7\n",
       "                    %\"conv2d_7\"<FLOAT,[batch,256,128,128]>  ::Conv(%\"getitem_18\", %\"model.encoder.model.stages_0.blocks.1.conv.conv1_1x1.weight\"{...}, %\"val_39\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             183 |  # node__native_batch_norm_legit_no_training_5__0\n",
       "                    %\"getitem_21\"<FLOAT,[batch,256,128,128]>  ::BatchNormalization(%\"conv2d_7\", %\"model.encoder.model.stages_0.blocks.1.conv.norm1.weight\"{...}, %\"model.encoder.model.stages_0.blocks.1.conv.norm1.bias\"{...}, %\"model.encoder.model.stages_0.blocks.1.conv.norm1.running_mean\"{...}, %\"model.encoder.model.stages_0.blocks.1.conv.norm1.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             184 |  # node_Pow_291\n",
       "                    %\"val_303\"<FLOAT,[batch,256,128,128]>  ::Pow(%\"getitem_21\", %\"val_16\"{3.0})\n",
       "             185 |  # node_Mul_293\n",
       "                    %\"val_305\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_303\")\n",
       "             186 |  # node_Add_294\n",
       "                    %\"val_306\"<FLOAT,[batch,256,128,128]>  ::Add(%\"getitem_21\", %\"val_305\")\n",
       "             187 |  # node_Mul_296\n",
       "                    %\"val_308\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_306\")\n",
       "             188 |  # node_Tanh_297\n",
       "                    %\"val_309\"<FLOAT,[batch,256,128,128]>  ::Tanh(%\"val_308\")\n",
       "             189 |  # node_Add_299\n",
       "                    %\"val_311\"<FLOAT,[batch,256,128,128]>  ::Add(%\"val_309\", %\"scalar_tensor_default\"{1.0})\n",
       "             190 |  # node_Mul_301\n",
       "                    %\"val_313\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"val_26\"{0.5}, %\"val_311\")\n",
       "             191 |  # node_gelu_5\n",
       "                    %\"gelu_5\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"getitem_21\", %\"val_313\")\n",
       "             192 |  # node_conv2d_8\n",
       "                    %\"conv2d_8\"<FLOAT,[batch,256,128,128]>  ::Conv(%\"gelu_5\", %\"model.encoder.model.stages_0.blocks.1.conv.conv2_kxk.weight\"{...}, %\"val_39\"{...}) {group=256, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             193 |  # node__native_batch_norm_legit_no_training_6__0\n",
       "                    %\"getitem_24\"<FLOAT,[batch,256,128,128]>  ::BatchNormalization(%\"conv2d_8\", %\"model.encoder.model.stages_0.blocks.1.conv.norm2.weight\"{...}, %\"model.encoder.model.stages_0.blocks.1.conv.norm2.bias\"{...}, %\"model.encoder.model.stages_0.blocks.1.conv.norm2.running_mean\"{...}, %\"model.encoder.model.stages_0.blocks.1.conv.norm2.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             194 |  # node_Pow_312\n",
       "                    %\"val_324\"<FLOAT,[batch,256,128,128]>  ::Pow(%\"getitem_24\", %\"val_16\"{3.0})\n",
       "             195 |  # node_Mul_314\n",
       "                    %\"val_326\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_324\")\n",
       "             196 |  # node_Add_315\n",
       "                    %\"val_327\"<FLOAT,[batch,256,128,128]>  ::Add(%\"getitem_24\", %\"val_326\")\n",
       "             197 |  # node_Mul_317\n",
       "                    %\"val_329\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_327\")\n",
       "             198 |  # node_Tanh_318\n",
       "                    %\"val_330\"<FLOAT,[batch,256,128,128]>  ::Tanh(%\"val_329\")\n",
       "             199 |  # node_Add_320\n",
       "                    %\"val_332\"<FLOAT,[batch,256,128,128]>  ::Add(%\"val_330\", %\"scalar_tensor_default\"{1.0})\n",
       "             200 |  # node_Mul_322\n",
       "                    %\"val_334\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"val_26\"{0.5}, %\"val_332\")\n",
       "             201 |  # node_gelu_6\n",
       "                    %\"gelu_6\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"getitem_24\", %\"val_334\")\n",
       "             202 |  # node_mean_1\n",
       "                    %\"mean_1\"<FLOAT,[batch,256,1,1]>  ::ReduceMean(%\"gelu_6\", %\"val_81\"{[2, 3]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             203 |  # node_conv2d_9\n",
       "                    %\"conv2d_9\"<FLOAT,[batch,16,1,1]>  ::Conv(%\"mean_1\", %\"model.encoder.model.stages_0.blocks.1.conv.se.fc1.weight\"{...}, %\"model.encoder.model.stages_0.blocks.1.conv.se.fc1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             204 |  # node_Sigmoid_325\n",
       "                    %\"val_337\"<FLOAT,[batch,16,1,1]>  ::Sigmoid(%\"conv2d_9\")\n",
       "             205 |  # node_silu_1\n",
       "                    %\"silu_1\"<FLOAT,[batch,16,1,1]>  ::Mul(%\"conv2d_9\", %\"val_337\")\n",
       "             206 |  # node_conv2d_10\n",
       "                    %\"conv2d_10\"<FLOAT,[batch,256,1,1]>  ::Conv(%\"silu_1\", %\"model.encoder.model.stages_0.blocks.1.conv.se.fc2.weight\"{...}, %\"model.encoder.model.stages_0.blocks.1.conv.se.fc2.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             207 |  # node_sigmoid_1\n",
       "                    %\"sigmoid_1\"<FLOAT,[batch,256,1,1]>  ::Sigmoid(%\"conv2d_10\")\n",
       "             208 |  # node_mul_371\n",
       "                    %\"mul_371\"<FLOAT,[batch,256,128,128]>  ::Mul(%\"gelu_6\", %\"sigmoid_1\")\n",
       "             209 |  # node_conv2d_11\n",
       "                    %\"conv2d_11\"<FLOAT,[batch,64,128,128]>  ::Conv(%\"mul_371\", %\"model.encoder.model.stages_0.blocks.1.conv.conv3_1x1.weight\"{...}, %\"model.encoder.model.stages_0.blocks.1.conv.conv3_1x1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             210 |  # node_add_694\n",
       "                    %\"add_694\"<FLOAT,[batch,64,128,128]>  ::Add(%\"conv2d_11\", %\"permute_5\")\n",
       "             211 |  # node_permute_6\n",
       "                    %\"permute_6\"<FLOAT,[batch,128,128,64]>  ::Transpose(%\"add_694\") {perm=(0, 2, 3, 1)}\n",
       "             212 |  # node_layer_norm_4\n",
       "                    %\"layer_norm_4\"<FLOAT,[batch,128,128,64]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_6\", %\"model.encoder.model.stages_0.blocks.1.attn_block.norm1.weight\"{...}, %\"model.encoder.model.stages_0.blocks.1.attn_block.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             213 |  # node_view_10\n",
       "                    %\"view_10\"<FLOAT,[batch,8,16,8,16,64]>  ::Reshape(%\"layer_norm_4\", %\"val_92\") {allowzero=1}\n",
       "             214 |  # node_permute_7\n",
       "                    %\"permute_7\"<FLOAT,[batch,8,8,16,16,64]>  ::Transpose(%\"view_10\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "             215 |  # node_view_11\n",
       "                    %\"view_11\"<FLOAT,[64*batch,16,16,64]>  ::Reshape(%\"permute_7\", %\"val_98\"{[-1, 16, 16, 64]}) {allowzero=1}\n",
       "             216 |  # node_MatMul_341\n",
       "                    %\"val_355\"<FLOAT,[64*batch,16,16,192]>  ::MatMul(%\"view_11\", %\"val_354\"{...})\n",
       "             217 |  # node_linear_8\n",
       "                    %\"linear_8\"<FLOAT,[64*batch,16,16,192]>  ::Add(%\"val_355\", %\"model.encoder.model.stages_0.blocks.1.attn_block.attn.qkv.bias\"{...})\n",
       "             218 |  # node_view_12\n",
       "                    %\"view_12\"<FLOAT,[64*batch,256,3,2,32]>  ::Reshape(%\"linear_8\", %\"val_108\") {allowzero=1}\n",
       "             219 |  # node_transpose_4\n",
       "                    %\"transpose_4\"<FLOAT,[64*batch,2,3,256,32]>  ::Transpose(%\"view_12\") {perm=(0, 3, 2, 1, 4)}\n",
       "             220 |  # node_Split_349\n",
       "                    %\"val_363\"<FLOAT,[64*batch,2,1,256,32]>, %\"val_364\"<FLOAT,[64*batch,2,1,256,32]>, %\"val_365\"<FLOAT,[64*batch,2,1,256,32]>  ::Split(%\"transpose_4\") {num_outputs=3, axis=2}\n",
       "             221 |  # node_unbind_2__0\n",
       "                    %\"getitem_27\"<FLOAT,[64*batch,2,256,32]>  ::Squeeze(%\"val_363\", %\"val_106\"{[2]})\n",
       "             222 |  # node_unbind_2__1\n",
       "                    %\"getitem_28\"<FLOAT,[64*batch,2,256,32]>  ::Squeeze(%\"val_364\", %\"val_106\"{[2]})\n",
       "             223 |  # node_unbind_2__2\n",
       "                    %\"getitem_29\"<FLOAT,[64*batch,2,256,32]>  ::Squeeze(%\"val_365\", %\"val_106\"{[2]})\n",
       "             224 |  # node_einsum_5\n",
       "                    %\"einsum_5\"<FLOAT,[2,16,16,16,16]>  ::Einsum(%\"einsum_4\"{...}, %\"model.encoder.model.stages_0.blocks.1.attn_block.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "             225 |  # node__unsafe_view_4\n",
       "                    %\"_unsafe_view_4\"<FLOAT,[2,256,256]>  ::Reshape(%\"einsum_5\", %\"val_117\"{[2, 256, 256]}) {allowzero=1}\n",
       "             226 |  # node_Shape_364\n",
       "                    %\"val_380\"<INT64,[4]>  ::Shape(%\"getitem_28\") {start=0}\n",
       "             227 |  # node_Slice_366\n",
       "                    %\"val_382\"<INT64,[1]>  ::Slice(%\"val_380\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "             228 |  # node_Slice_367\n",
       "                    %\"val_383\"<INT64,[1]>  ::Slice(%\"val_380\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "             229 |  # node_Slice_369\n",
       "                    %\"val_385\"<INT64,[2]>  ::Slice(%\"val_380\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "             230 |  # node_Concat_371\n",
       "                    %\"val_387\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_383\", %\"val_382\") {axis=0}\n",
       "             231 |  # node_Reshape_372\n",
       "                    %\"val_388\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_28\", %\"val_387\") {allowzero=0}\n",
       "             232 |  # node_Transpose_373\n",
       "                    %\"val_389\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_388\") {perm=(0, 2, 1)}\n",
       "             233 |  # node_Concat_374\n",
       "                    %\"val_390\"<INT64,[4]>  ::Concat(%\"val_385\", %\"val_382\", %\"val_383\") {axis=0}\n",
       "             234 |  # node_Reshape_375\n",
       "                    %\"val_391\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_389\", %\"val_390\") {allowzero=0}\n",
       "             235 |  # node_Mul_377\n",
       "                    %\"val_393\"<FLOAT,[64*batch,2,256,32]>  ::Mul(%\"getitem_27\", %\"val_141\"{[0.4204482138156891]})\n",
       "             236 |  # node_Mul_379\n",
       "                    %\"val_395\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_391\", %\"val_141\"{[0.4204482138156891]})\n",
       "             237 |  # node_MatMul_380\n",
       "                    %\"val_396\"<FLOAT,[None,2,256,None]>  ::MatMul(%\"val_393\", %\"val_395\")\n",
       "             238 |  # node_Add_381\n",
       "                    %\"val_397\"<FLOAT,[None,2,256,256]>  ::Add(%\"val_396\", %\"_unsafe_view_4\")\n",
       "             239 |  # node_Softmax_382\n",
       "                    %\"val_398\"<FLOAT,[None,2,256,256]>  ::Softmax(%\"val_397\") {axis=-1}\n",
       "             240 |  # node_scaled_dot_product_attention_2\n",
       "                    %\"scaled_dot_product_attention_2\"<FLOAT,[64*batch,2,256,32]>  ::MatMul(%\"val_398\", %\"getitem_29\")\n",
       "             241 |  # node_transpose_5\n",
       "                    %\"transpose_5\"<FLOAT,[64*batch,256,2,32]>  ::Transpose(%\"scaled_dot_product_attention_2\") {perm=(0, 2, 1, 3)}\n",
       "             242 |  # node__unsafe_view_5\n",
       "                    %\"_unsafe_view_5\"<FLOAT,[64*batch,16,16,64]>  ::Reshape(%\"transpose_5\", %\"val_153\") {allowzero=1}\n",
       "             243 |  # node_MatMul_390\n",
       "                    %\"val_406\"<FLOAT,[64*batch,16,16,64]>  ::MatMul(%\"_unsafe_view_5\", %\"val_405\"{...})\n",
       "             244 |  # node_linear_9\n",
       "                    %\"linear_9\"<FLOAT,[64*batch,16,16,64]>  ::Add(%\"val_406\", %\"model.encoder.model.stages_0.blocks.1.attn_block.attn.proj.bias\"{...})\n",
       "             245 |  # node_view_13\n",
       "                    %\"view_13\"<FLOAT,[batch,8,8,16,16,64]>  ::Reshape(%\"linear_9\", %\"val_163\"{[-1, 8, 8, 16, 16, 64]}) {allowzero=1}\n",
       "             246 |  # node_permute_8\n",
       "                    %\"permute_8\"<FLOAT,[batch,8,16,8,16,64]>  ::Transpose(%\"view_13\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "             247 |  # node_view_14\n",
       "                    %\"view_14\"<FLOAT,[batch,128,128,64]>  ::Reshape(%\"permute_8\", %\"val_169\"{[-1, 128, 128, 64]}) {allowzero=1}\n",
       "             248 |  # node_add_824\n",
       "                    %\"add_824\"<FLOAT,[batch,128,128,64]>  ::Add(%\"permute_6\", %\"view_14\")\n",
       "             249 |  # node_layer_norm_5\n",
       "                    %\"layer_norm_5\"<FLOAT,[batch,128,128,64]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_824\", %\"model.encoder.model.stages_0.blocks.1.attn_block.norm2.weight\"{...}, %\"model.encoder.model.stages_0.blocks.1.attn_block.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             250 |  # node_MatMul_406\n",
       "                    %\"val_424\"<FLOAT,[batch,128,128,256]>  ::MatMul(%\"layer_norm_5\", %\"val_423\"{...})\n",
       "             251 |  # node_linear_10\n",
       "                    %\"linear_10\"<FLOAT,[batch,128,128,256]>  ::Add(%\"val_424\", %\"model.encoder.model.stages_0.blocks.1.attn_block.mlp.fc1.bias\"{...})\n",
       "             252 |  # node_Pow_408\n",
       "                    %\"val_426\"<FLOAT,[batch,128,128,256]>  ::Pow(%\"linear_10\", %\"val_16\"{3.0})\n",
       "             253 |  # node_Mul_410\n",
       "                    %\"val_428\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_426\")\n",
       "             254 |  # node_Add_411\n",
       "                    %\"val_429\"<FLOAT,[batch,128,128,256]>  ::Add(%\"linear_10\", %\"val_428\")\n",
       "             255 |  # node_Mul_413\n",
       "                    %\"val_431\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_429\")\n",
       "             256 |  # node_Tanh_414\n",
       "                    %\"val_432\"<FLOAT,[batch,128,128,256]>  ::Tanh(%\"val_431\")\n",
       "             257 |  # node_Add_416\n",
       "                    %\"val_434\"<FLOAT,[batch,128,128,256]>  ::Add(%\"val_432\", %\"scalar_tensor_default\"{1.0})\n",
       "             258 |  # node_Mul_418\n",
       "                    %\"val_436\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"val_26\"{0.5}, %\"val_434\")\n",
       "             259 |  # node_gelu_7\n",
       "                    %\"gelu_7\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"linear_10\", %\"val_436\")\n",
       "             260 |  # node_MatMul_420\n",
       "                    %\"val_438\"<FLOAT,[batch,128,128,64]>  ::MatMul(%\"gelu_7\", %\"val_437\"{...})\n",
       "             261 |  # node_linear_11\n",
       "                    %\"linear_11\"<FLOAT,[batch,128,128,64]>  ::Add(%\"val_438\", %\"model.encoder.model.stages_0.blocks.1.attn_block.mlp.fc2.bias\"{...})\n",
       "             262 |  # node_add_860\n",
       "                    %\"add_860\"<FLOAT,[batch,128,128,64]>  ::Add(%\"add_824\", %\"linear_11\")\n",
       "             263 |  # node_layer_norm_6\n",
       "                    %\"layer_norm_6\"<FLOAT,[batch,128,128,64]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_860\", %\"model.encoder.model.stages_0.blocks.1.attn_grid.norm1.weight\"{...}, %\"model.encoder.model.stages_0.blocks.1.attn_grid.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             264 |  # node_view_15\n",
       "                    %\"view_15\"<FLOAT,[batch,16,8,16,8,64]>  ::Reshape(%\"layer_norm_6\", %\"val_197\") {allowzero=1}\n",
       "             265 |  # node_permute_9\n",
       "                    %\"permute_9\"<FLOAT,[batch,8,8,16,16,64]>  ::Transpose(%\"view_15\") {perm=(0, 2, 4, 1, 3, 5)}\n",
       "             266 |  # node_view_16\n",
       "                    %\"view_16\"<FLOAT,[64*batch,16,16,64]>  ::Reshape(%\"permute_9\", %\"val_98\"{[-1, 16, 16, 64]}) {allowzero=1}\n",
       "             267 |  # node_MatMul_436\n",
       "                    %\"val_456\"<FLOAT,[64*batch,16,16,192]>  ::MatMul(%\"view_16\", %\"val_455\"{...})\n",
       "             268 |  # node_linear_12\n",
       "                    %\"linear_12\"<FLOAT,[64*batch,16,16,192]>  ::Add(%\"val_456\", %\"model.encoder.model.stages_0.blocks.1.attn_grid.attn.qkv.bias\"{...})\n",
       "             269 |  # node_view_17\n",
       "                    %\"view_17\"<FLOAT,[64*batch,256,3,2,32]>  ::Reshape(%\"linear_12\", %\"val_108\") {allowzero=1}\n",
       "             270 |  # node_transpose_6\n",
       "                    %\"transpose_6\"<FLOAT,[64*batch,2,3,256,32]>  ::Transpose(%\"view_17\") {perm=(0, 3, 2, 1, 4)}\n",
       "             271 |  # node_Split_444\n",
       "                    %\"val_464\"<FLOAT,[64*batch,2,1,256,32]>, %\"val_465\"<FLOAT,[64*batch,2,1,256,32]>, %\"val_466\"<FLOAT,[64*batch,2,1,256,32]>  ::Split(%\"transpose_6\") {num_outputs=3, axis=2}\n",
       "             272 |  # node_unbind_3__0\n",
       "                    %\"getitem_30\"<FLOAT,[64*batch,2,256,32]>  ::Squeeze(%\"val_464\", %\"val_106\"{[2]})\n",
       "             273 |  # node_unbind_3__1\n",
       "                    %\"getitem_31\"<FLOAT,[64*batch,2,256,32]>  ::Squeeze(%\"val_465\", %\"val_106\"{[2]})\n",
       "             274 |  # node_unbind_3__2\n",
       "                    %\"getitem_32\"<FLOAT,[64*batch,2,256,32]>  ::Squeeze(%\"val_466\", %\"val_106\"{[2]})\n",
       "             275 |  # node_einsum_7\n",
       "                    %\"einsum_7\"<FLOAT,[2,16,16,16,16]>  ::Einsum(%\"einsum_6\"{...}, %\"model.encoder.model.stages_0.blocks.1.attn_grid.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "             276 |  # node__unsafe_view_6\n",
       "                    %\"_unsafe_view_6\"<FLOAT,[2,256,256]>  ::Reshape(%\"einsum_7\", %\"val_117\"{[2, 256, 256]}) {allowzero=1}\n",
       "             277 |  # node_Shape_459\n",
       "                    %\"val_481\"<INT64,[4]>  ::Shape(%\"getitem_31\") {start=0}\n",
       "             278 |  # node_Slice_461\n",
       "                    %\"val_483\"<INT64,[1]>  ::Slice(%\"val_481\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "             279 |  # node_Slice_462\n",
       "                    %\"val_484\"<INT64,[1]>  ::Slice(%\"val_481\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "             280 |  # node_Slice_464\n",
       "                    %\"val_486\"<INT64,[2]>  ::Slice(%\"val_481\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "             281 |  # node_Concat_466\n",
       "                    %\"val_488\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_484\", %\"val_483\") {axis=0}\n",
       "             282 |  # node_Reshape_467\n",
       "                    %\"val_489\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_31\", %\"val_488\") {allowzero=0}\n",
       "             283 |  # node_Transpose_468\n",
       "                    %\"val_490\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_489\") {perm=(0, 2, 1)}\n",
       "             284 |  # node_Concat_469\n",
       "                    %\"val_491\"<INT64,[4]>  ::Concat(%\"val_486\", %\"val_483\", %\"val_484\") {axis=0}\n",
       "             285 |  # node_Reshape_470\n",
       "                    %\"val_492\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_490\", %\"val_491\") {allowzero=0}\n",
       "             286 |  # node_Mul_472\n",
       "                    %\"val_494\"<FLOAT,[64*batch,2,256,32]>  ::Mul(%\"getitem_30\", %\"val_141\"{[0.4204482138156891]})\n",
       "             287 |  # node_Mul_474\n",
       "                    %\"val_496\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_492\", %\"val_141\"{[0.4204482138156891]})\n",
       "             288 |  # node_MatMul_475\n",
       "                    %\"val_497\"<FLOAT,[None,2,256,None]>  ::MatMul(%\"val_494\", %\"val_496\")\n",
       "             289 |  # node_Add_476\n",
       "                    %\"val_498\"<FLOAT,[None,2,256,256]>  ::Add(%\"val_497\", %\"_unsafe_view_6\")\n",
       "             290 |  # node_Softmax_477\n",
       "                    %\"val_499\"<FLOAT,[None,2,256,256]>  ::Softmax(%\"val_498\") {axis=-1}\n",
       "             291 |  # node_scaled_dot_product_attention_3\n",
       "                    %\"scaled_dot_product_attention_3\"<FLOAT,[64*batch,2,256,32]>  ::MatMul(%\"val_499\", %\"getitem_32\")\n",
       "             292 |  # node_transpose_7\n",
       "                    %\"transpose_7\"<FLOAT,[64*batch,256,2,32]>  ::Transpose(%\"scaled_dot_product_attention_3\") {perm=(0, 2, 1, 3)}\n",
       "             293 |  # node__unsafe_view_7\n",
       "                    %\"_unsafe_view_7\"<FLOAT,[64*batch,16,16,64]>  ::Reshape(%\"transpose_7\", %\"val_153\") {allowzero=1}\n",
       "             294 |  # node_MatMul_485\n",
       "                    %\"val_507\"<FLOAT,[64*batch,16,16,64]>  ::MatMul(%\"_unsafe_view_7\", %\"val_506\"{...})\n",
       "             295 |  # node_linear_13\n",
       "                    %\"linear_13\"<FLOAT,[64*batch,16,16,64]>  ::Add(%\"val_507\", %\"model.encoder.model.stages_0.blocks.1.attn_grid.attn.proj.bias\"{...})\n",
       "             296 |  # node_view_18\n",
       "                    %\"view_18\"<FLOAT,[batch,8,8,16,16,64]>  ::Reshape(%\"linear_13\", %\"val_163\"{[-1, 8, 8, 16, 16, 64]}) {allowzero=1}\n",
       "             297 |  # node_permute_10\n",
       "                    %\"permute_10\"<FLOAT,[batch,16,8,16,8,64]>  ::Transpose(%\"view_18\") {perm=(0, 3, 1, 4, 2, 5)}\n",
       "             298 |  # node_view_19\n",
       "                    %\"view_19\"<FLOAT,[batch,128,128,64]>  ::Reshape(%\"permute_10\", %\"val_169\"{[-1, 128, 128, 64]}) {allowzero=1}\n",
       "             299 |  # node_add_985\n",
       "                    %\"add_985\"<FLOAT,[batch,128,128,64]>  ::Add(%\"add_860\", %\"view_19\")\n",
       "             300 |  # node_layer_norm_7\n",
       "                    %\"layer_norm_7\"<FLOAT,[batch,128,128,64]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_985\", %\"model.encoder.model.stages_0.blocks.1.attn_grid.norm2.weight\"{...}, %\"model.encoder.model.stages_0.blocks.1.attn_grid.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             301 |  # node_MatMul_501\n",
       "                    %\"val_525\"<FLOAT,[batch,128,128,256]>  ::MatMul(%\"layer_norm_7\", %\"val_524\"{...})\n",
       "             302 |  # node_linear_14\n",
       "                    %\"linear_14\"<FLOAT,[batch,128,128,256]>  ::Add(%\"val_525\", %\"model.encoder.model.stages_0.blocks.1.attn_grid.mlp.fc1.bias\"{...})\n",
       "             303 |  # node_Pow_503\n",
       "                    %\"val_527\"<FLOAT,[batch,128,128,256]>  ::Pow(%\"linear_14\", %\"val_16\"{3.0})\n",
       "             304 |  # node_Mul_505\n",
       "                    %\"val_529\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_527\")\n",
       "             305 |  # node_Add_506\n",
       "                    %\"val_530\"<FLOAT,[batch,128,128,256]>  ::Add(%\"linear_14\", %\"val_529\")\n",
       "             306 |  # node_Mul_508\n",
       "                    %\"val_532\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_530\")\n",
       "             307 |  # node_Tanh_509\n",
       "                    %\"val_533\"<FLOAT,[batch,128,128,256]>  ::Tanh(%\"val_532\")\n",
       "             308 |  # node_Add_511\n",
       "                    %\"val_535\"<FLOAT,[batch,128,128,256]>  ::Add(%\"val_533\", %\"scalar_tensor_default\"{1.0})\n",
       "             309 |  # node_Mul_513\n",
       "                    %\"val_537\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"val_26\"{0.5}, %\"val_535\")\n",
       "             310 |  # node_gelu_8\n",
       "                    %\"gelu_8\"<FLOAT,[batch,128,128,256]>  ::Mul(%\"linear_14\", %\"val_537\")\n",
       "             311 |  # node_MatMul_515\n",
       "                    %\"val_539\"<FLOAT,[batch,128,128,64]>  ::MatMul(%\"gelu_8\", %\"val_538\"{...})\n",
       "             312 |  # node_linear_15\n",
       "                    %\"linear_15\"<FLOAT,[batch,128,128,64]>  ::Add(%\"val_539\", %\"model.encoder.model.stages_0.blocks.1.attn_grid.mlp.fc2.bias\"{...})\n",
       "             313 |  # node_add_1021\n",
       "                    %\"add_1021\"<FLOAT,[batch,128,128,64]>  ::Add(%\"add_985\", %\"linear_15\")\n",
       "             314 |  # node_permute_11\n",
       "                    %\"permute_11\"<FLOAT,[batch,64,128,128]>  ::Transpose(%\"add_1021\") {perm=(0, 3, 1, 2)}\n",
       "             315 |  # node_pad_3\n",
       "                    %\"pad_3\"<FLOAT,[batch,64,128,128]>  ::Pad(%\"permute_11\", %\"val_28\"{[0, 0, 0, 0, 0, 0, 0, 0]}, %\"val_29\"{0.0}) {mode='constant'}\n",
       "             316 |  # node_avg_pool2d_1\n",
       "                    %\"avg_pool2d_1\"<FLOAT,[batch,64,64,64]>  ::AveragePool(%\"pad_3\") {count_include_pad=1, ceil_mode=0, pads=(0, 0, 0, 0), kernel_shape=(2, 2), strides=(2, 2), auto_pad='NOTSET'}\n",
       "             317 |  # node_conv2d_12\n",
       "                    %\"conv2d_12\"<FLOAT,[batch,128,64,64]>  ::Conv(%\"avg_pool2d_1\", %\"model.encoder.model.stages_1.blocks.0.conv.shortcut.expand.weight\"{...}, %\"model.encoder.model.stages_1.blocks.0.conv.shortcut.expand.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             318 |  # node__native_batch_norm_legit_no_training_7__0\n",
       "                    %\"getitem_33\"<FLOAT,[batch,64,128,128]>  ::BatchNormalization(%\"permute_11\", %\"model.encoder.model.stages_1.blocks.0.conv.pre_norm.weight\"{...}, %\"model.encoder.model.stages_1.blocks.0.conv.pre_norm.bias\"{...}, %\"model.encoder.model.stages_1.blocks.0.conv.pre_norm.running_mean\"{...}, %\"model.encoder.model.stages_1.blocks.0.conv.pre_norm.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             319 |  # node_conv2d_13\n",
       "                    %\"conv2d_13\"<FLOAT,[batch,512,128,128]>  ::Conv(%\"getitem_33\", %\"model.encoder.model.stages_1.blocks.0.conv.conv1_1x1.weight\"{...}, %\"val_550\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             320 |  # node__native_batch_norm_legit_no_training_8__0\n",
       "                    %\"getitem_36\"<FLOAT,[batch,512,128,128]>  ::BatchNormalization(%\"conv2d_13\", %\"model.encoder.model.stages_1.blocks.0.conv.norm1.weight\"{...}, %\"model.encoder.model.stages_1.blocks.0.conv.norm1.bias\"{...}, %\"model.encoder.model.stages_1.blocks.0.conv.norm1.running_mean\"{...}, %\"model.encoder.model.stages_1.blocks.0.conv.norm1.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             321 |  # node_Pow_532\n",
       "                    %\"val_556\"<FLOAT,[batch,512,128,128]>  ::Pow(%\"getitem_36\", %\"val_16\"{3.0})\n",
       "             322 |  # node_Mul_534\n",
       "                    %\"val_558\"<FLOAT,[batch,512,128,128]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_556\")\n",
       "             323 |  # node_Add_535\n",
       "                    %\"val_559\"<FLOAT,[batch,512,128,128]>  ::Add(%\"getitem_36\", %\"val_558\")\n",
       "             324 |  # node_Mul_537\n",
       "                    %\"val_561\"<FLOAT,[batch,512,128,128]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_559\")\n",
       "             325 |  # node_Tanh_538\n",
       "                    %\"val_562\"<FLOAT,[batch,512,128,128]>  ::Tanh(%\"val_561\")\n",
       "             326 |  # node_Add_540\n",
       "                    %\"val_564\"<FLOAT,[batch,512,128,128]>  ::Add(%\"val_562\", %\"scalar_tensor_default\"{1.0})\n",
       "             327 |  # node_Mul_542\n",
       "                    %\"val_566\"<FLOAT,[batch,512,128,128]>  ::Mul(%\"val_26\"{0.5}, %\"val_564\")\n",
       "             328 |  # node_gelu_9\n",
       "                    %\"gelu_9\"<FLOAT,[batch,512,128,128]>  ::Mul(%\"getitem_36\", %\"val_566\")\n",
       "             329 |  # node_conv2d_14\n",
       "                    %\"conv2d_14\"<FLOAT,[batch,512,64,64]>  ::Conv(%\"gelu_9\", %\"model.encoder.model.stages_1.blocks.0.conv.conv2_kxk.weight\"{...}, %\"val_550\"{...}) {group=512, pads=(0, 0, 1, 1), strides=(2, 2), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             330 |  # node__native_batch_norm_legit_no_training_9__0\n",
       "                    %\"getitem_39\"<FLOAT,[batch,512,64,64]>  ::BatchNormalization(%\"conv2d_14\", %\"model.encoder.model.stages_1.blocks.0.conv.norm2.weight\"{...}, %\"model.encoder.model.stages_1.blocks.0.conv.norm2.bias\"{...}, %\"model.encoder.model.stages_1.blocks.0.conv.norm2.running_mean\"{...}, %\"model.encoder.model.stages_1.blocks.0.conv.norm2.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             331 |  # node_Pow_555\n",
       "                    %\"val_579\"<FLOAT,[batch,512,64,64]>  ::Pow(%\"getitem_39\", %\"val_16\"{3.0})\n",
       "             332 |  # node_Mul_557\n",
       "                    %\"val_581\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_579\")\n",
       "             333 |  # node_Add_558\n",
       "                    %\"val_582\"<FLOAT,[batch,512,64,64]>  ::Add(%\"getitem_39\", %\"val_581\")\n",
       "             334 |  # node_Mul_560\n",
       "                    %\"val_584\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_582\")\n",
       "             335 |  # node_Tanh_561\n",
       "                    %\"val_585\"<FLOAT,[batch,512,64,64]>  ::Tanh(%\"val_584\")\n",
       "             336 |  # node_Add_563\n",
       "                    %\"val_587\"<FLOAT,[batch,512,64,64]>  ::Add(%\"val_585\", %\"scalar_tensor_default\"{1.0})\n",
       "             337 |  # node_Mul_565\n",
       "                    %\"val_589\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"val_26\"{0.5}, %\"val_587\")\n",
       "             338 |  # node_gelu_10\n",
       "                    %\"gelu_10\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"getitem_39\", %\"val_589\")\n",
       "             339 |  # node_mean_2\n",
       "                    %\"mean_2\"<FLOAT,[batch,512,1,1]>  ::ReduceMean(%\"gelu_10\", %\"val_81\"{[2, 3]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             340 |  # node_conv2d_15\n",
       "                    %\"conv2d_15\"<FLOAT,[batch,32,1,1]>  ::Conv(%\"mean_2\", %\"model.encoder.model.stages_1.blocks.0.conv.se.fc1.weight\"{...}, %\"model.encoder.model.stages_1.blocks.0.conv.se.fc1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             341 |  # node_Sigmoid_568\n",
       "                    %\"val_592\"<FLOAT,[batch,32,1,1]>  ::Sigmoid(%\"conv2d_15\")\n",
       "             342 |  # node_silu_2\n",
       "                    %\"silu_2\"<FLOAT,[batch,32,1,1]>  ::Mul(%\"conv2d_15\", %\"val_592\")\n",
       "             343 |  # node_conv2d_16\n",
       "                    %\"conv2d_16\"<FLOAT,[batch,512,1,1]>  ::Conv(%\"silu_2\", %\"model.encoder.model.stages_1.blocks.0.conv.se.fc2.weight\"{...}, %\"model.encoder.model.stages_1.blocks.0.conv.se.fc2.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             344 |  # node_sigmoid_2\n",
       "                    %\"sigmoid_2\"<FLOAT,[batch,512,1,1]>  ::Sigmoid(%\"conv2d_16\")\n",
       "             345 |  # node_mul_623\n",
       "                    %\"mul_623\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"gelu_10\", %\"sigmoid_2\")\n",
       "             346 |  # node_conv2d_17\n",
       "                    %\"conv2d_17\"<FLOAT,[batch,128,64,64]>  ::Conv(%\"mul_623\", %\"model.encoder.model.stages_1.blocks.0.conv.conv3_1x1.weight\"{...}, %\"model.encoder.model.stages_1.blocks.0.conv.shortcut.expand.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             347 |  # node_add_1128\n",
       "                    %\"add_1128\"<FLOAT,[batch,128,64,64]>  ::Add(%\"conv2d_17\", %\"conv2d_12\")\n",
       "             348 |  # node_permute_12\n",
       "                    %\"permute_12\"<FLOAT,[batch,64,64,128]>  ::Transpose(%\"add_1128\") {perm=(0, 2, 3, 1)}\n",
       "             349 |  # node_layer_norm_8\n",
       "                    %\"layer_norm_8\"<FLOAT,[batch,64,64,128]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_12\", %\"model.encoder.model.stages_1.blocks.0.attn_block.norm1.weight\"{...}, %\"model.encoder.model.stages_1.blocks.0.attn_block.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             350 |  # node_Concat_576\n",
       "                    %\"val_602\"<INT64,[6]>  ::Concat(%\"val_0\", %\"val_597\"{[4]}, %\"val_88\"{[16]}, %\"val_597\"{[4]}, %\"val_88\"{[16]}, %\"val_601\"{[128]}) {axis=0}\n",
       "             351 |  # node_view_20\n",
       "                    %\"view_20\"<FLOAT,[batch,4,16,4,16,128]>  ::Reshape(%\"layer_norm_8\", %\"val_602\") {allowzero=1}\n",
       "             352 |  # node_permute_13\n",
       "                    %\"permute_13\"<FLOAT,[batch,4,4,16,16,128]>  ::Transpose(%\"view_20\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "             353 |  # node_view_21\n",
       "                    %\"view_21\"<FLOAT,[16*batch,16,16,128]>  ::Reshape(%\"permute_13\", %\"val_608\"{[-1, 16, 16, 128]}) {allowzero=1}\n",
       "             354 |  # node_MatMul_584\n",
       "                    %\"val_610\"<FLOAT,[16*batch,16,16,384]>  ::MatMul(%\"view_21\", %\"val_609\"{...})\n",
       "             355 |  # node_linear_16\n",
       "                    %\"linear_16\"<FLOAT,[16*batch,16,16,384]>  ::Add(%\"val_610\", %\"model.encoder.model.stages_1.blocks.0.attn_block.attn.qkv.bias\"{...})\n",
       "             356 |  # node_mul_650\n",
       "                    %\"mul_650\"<INT64,[]>  ::Mul(%\"val_611\"{16}, %\"sym_size_int_37\")\n",
       "             357 |  # node_Reshape_587\n",
       "                    %\"val_613\"<INT64,[1]>  ::Reshape(%\"mul_650\", %\"val_102\"{[-1]}) {allowzero=0}\n",
       "             358 |  # node_Concat_592\n",
       "                    %\"val_618\"<INT64,[5]>  ::Concat(%\"val_613\", %\"val_102\"{[-1]}, %\"val_105\"{[3]}, %\"val_597\"{[4]}, %\"val_107\"{[32]}) {axis=0}\n",
       "             359 |  # node_view_22\n",
       "                    %\"view_22\"<FLOAT,[16*batch,256,3,4,32]>  ::Reshape(%\"linear_16\", %\"val_618\") {allowzero=1}\n",
       "             360 |  # node_transpose_8\n",
       "                    %\"transpose_8\"<FLOAT,[16*batch,4,3,256,32]>  ::Transpose(%\"view_22\") {perm=(0, 3, 2, 1, 4)}\n",
       "             361 |  # node_Split_593\n",
       "                    %\"val_619\"<FLOAT,[16*batch,4,1,256,32]>, %\"val_620\"<FLOAT,[16*batch,4,1,256,32]>, %\"val_621\"<FLOAT,[16*batch,4,1,256,32]>  ::Split(%\"transpose_8\") {num_outputs=3, axis=2}\n",
       "             362 |  # node_unbind_4__0\n",
       "                    %\"getitem_42\"<FLOAT,[16*batch,4,256,32]>  ::Squeeze(%\"val_619\", %\"val_106\"{[2]})\n",
       "             363 |  # node_unbind_4__1\n",
       "                    %\"getitem_43\"<FLOAT,[16*batch,4,256,32]>  ::Squeeze(%\"val_620\", %\"val_106\"{[2]})\n",
       "             364 |  # node_unbind_4__2\n",
       "                    %\"getitem_44\"<FLOAT,[16*batch,4,256,32]>  ::Squeeze(%\"val_621\", %\"val_106\"{[2]})\n",
       "             365 |  # node_einsum_9\n",
       "                    %\"einsum_9\"<FLOAT,[4,16,16,16,16]>  ::Einsum(%\"einsum_8\"{...}, %\"model.encoder.model.stages_1.blocks.0.attn_block.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "             366 |  # node__unsafe_view_8\n",
       "                    %\"_unsafe_view_8\"<FLOAT,[4,256,256]>  ::Reshape(%\"einsum_9\", %\"val_626\"{[4, 256, 256]}) {allowzero=1}\n",
       "             367 |  # node_Shape_608\n",
       "                    %\"val_636\"<INT64,[4]>  ::Shape(%\"getitem_43\") {start=0}\n",
       "             368 |  # node_Slice_610\n",
       "                    %\"val_638\"<INT64,[1]>  ::Slice(%\"val_636\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "             369 |  # node_Slice_611\n",
       "                    %\"val_639\"<INT64,[1]>  ::Slice(%\"val_636\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "             370 |  # node_Slice_613\n",
       "                    %\"val_641\"<INT64,[2]>  ::Slice(%\"val_636\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "             371 |  # node_Concat_615\n",
       "                    %\"val_643\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_639\", %\"val_638\") {axis=0}\n",
       "             372 |  # node_Reshape_616\n",
       "                    %\"val_644\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_43\", %\"val_643\") {allowzero=0}\n",
       "             373 |  # node_Transpose_617\n",
       "                    %\"val_645\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_644\") {perm=(0, 2, 1)}\n",
       "             374 |  # node_Concat_618\n",
       "                    %\"val_646\"<INT64,[4]>  ::Concat(%\"val_641\", %\"val_638\", %\"val_639\") {axis=0}\n",
       "             375 |  # node_Reshape_619\n",
       "                    %\"val_647\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_645\", %\"val_646\") {allowzero=0}\n",
       "             376 |  # node_Mul_621\n",
       "                    %\"val_649\"<FLOAT,[16*batch,4,256,32]>  ::Mul(%\"getitem_42\", %\"val_141\"{[0.4204482138156891]})\n",
       "             377 |  # node_Mul_623\n",
       "                    %\"val_651\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_647\", %\"val_141\"{[0.4204482138156891]})\n",
       "             378 |  # node_MatMul_624\n",
       "                    %\"val_652\"<FLOAT,[None,4,256,None]>  ::MatMul(%\"val_649\", %\"val_651\")\n",
       "             379 |  # node_Add_625\n",
       "                    %\"val_653\"<FLOAT,[None,4,256,256]>  ::Add(%\"val_652\", %\"_unsafe_view_8\")\n",
       "             380 |  # node_Softmax_626\n",
       "                    %\"val_654\"<FLOAT,[None,4,256,256]>  ::Softmax(%\"val_653\") {axis=-1}\n",
       "             381 |  # node_scaled_dot_product_attention_4\n",
       "                    %\"scaled_dot_product_attention_4\"<FLOAT,[16*batch,4,256,32]>  ::MatMul(%\"val_654\", %\"getitem_44\")\n",
       "             382 |  # node_transpose_9\n",
       "                    %\"transpose_9\"<FLOAT,[16*batch,256,4,32]>  ::Transpose(%\"scaled_dot_product_attention_4\") {perm=(0, 2, 1, 3)}\n",
       "             383 |  # node_Concat_632\n",
       "                    %\"val_660\"<INT64,[4]>  ::Concat(%\"val_613\", %\"val_88\"{[16]}, %\"val_88\"{[16]}, %\"val_601\"{[128]}) {axis=0}\n",
       "             384 |  # node__unsafe_view_9\n",
       "                    %\"_unsafe_view_9\"<FLOAT,[16*batch,16,16,128]>  ::Reshape(%\"transpose_9\", %\"val_660\") {allowzero=1}\n",
       "             385 |  # node_MatMul_634\n",
       "                    %\"val_662\"<FLOAT,[16*batch,16,16,128]>  ::MatMul(%\"_unsafe_view_9\", %\"val_661\"{...})\n",
       "             386 |  # node_linear_17\n",
       "                    %\"linear_17\"<FLOAT,[16*batch,16,16,128]>  ::Add(%\"val_662\", %\"model.encoder.model.stages_1.blocks.0.attn_block.attn.proj.bias\"{...})\n",
       "             387 |  # node_view_23\n",
       "                    %\"view_23\"<FLOAT,[batch,4,4,16,16,128]>  ::Reshape(%\"linear_17\", %\"val_670\"{[-1, 4, 4, 16, 16, 128]}) {allowzero=1}\n",
       "             388 |  # node_permute_14\n",
       "                    %\"permute_14\"<FLOAT,[batch,4,16,4,16,128]>  ::Transpose(%\"view_23\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "             389 |  # node_view_24\n",
       "                    %\"view_24\"<FLOAT,[batch,64,64,128]>  ::Reshape(%\"permute_14\", %\"val_676\"{[-1, 64, 64, 128]}) {allowzero=1}\n",
       "             390 |  # node_add_1258\n",
       "                    %\"add_1258\"<FLOAT,[batch,64,64,128]>  ::Add(%\"permute_12\", %\"view_24\")\n",
       "             391 |  # node_layer_norm_9\n",
       "                    %\"layer_norm_9\"<FLOAT,[batch,64,64,128]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_1258\", %\"model.encoder.model.stages_1.blocks.0.attn_block.norm2.weight\"{...}, %\"model.encoder.model.stages_1.blocks.0.attn_block.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             392 |  # node_MatMul_650\n",
       "                    %\"val_680\"<FLOAT,[batch,64,64,512]>  ::MatMul(%\"layer_norm_9\", %\"val_679\"{...})\n",
       "             393 |  # node_linear_18\n",
       "                    %\"linear_18\"<FLOAT,[batch,64,64,512]>  ::Add(%\"val_680\", %\"model.encoder.model.stages_1.blocks.0.attn_block.mlp.fc1.bias\"{...})\n",
       "             394 |  # node_Pow_652\n",
       "                    %\"val_682\"<FLOAT,[batch,64,64,512]>  ::Pow(%\"linear_18\", %\"val_16\"{3.0})\n",
       "             395 |  # node_Mul_654\n",
       "                    %\"val_684\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_682\")\n",
       "             396 |  # node_Add_655\n",
       "                    %\"val_685\"<FLOAT,[batch,64,64,512]>  ::Add(%\"linear_18\", %\"val_684\")\n",
       "             397 |  # node_Mul_657\n",
       "                    %\"val_687\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_685\")\n",
       "             398 |  # node_Tanh_658\n",
       "                    %\"val_688\"<FLOAT,[batch,64,64,512]>  ::Tanh(%\"val_687\")\n",
       "             399 |  # node_Add_660\n",
       "                    %\"val_690\"<FLOAT,[batch,64,64,512]>  ::Add(%\"val_688\", %\"scalar_tensor_default\"{1.0})\n",
       "             400 |  # node_Mul_662\n",
       "                    %\"val_692\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"val_26\"{0.5}, %\"val_690\")\n",
       "             401 |  # node_gelu_11\n",
       "                    %\"gelu_11\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"linear_18\", %\"val_692\")\n",
       "             402 |  # node_MatMul_664\n",
       "                    %\"val_694\"<FLOAT,[batch,64,64,128]>  ::MatMul(%\"gelu_11\", %\"val_693\"{...})\n",
       "             403 |  # node_linear_19\n",
       "                    %\"linear_19\"<FLOAT,[batch,64,64,128]>  ::Add(%\"val_694\", %\"model.encoder.model.stages_1.blocks.0.attn_block.mlp.fc2.bias\"{...})\n",
       "             404 |  # node_add_1294\n",
       "                    %\"add_1294\"<FLOAT,[batch,64,64,128]>  ::Add(%\"add_1258\", %\"linear_19\")\n",
       "             405 |  # node_layer_norm_10\n",
       "                    %\"layer_norm_10\"<FLOAT,[batch,64,64,128]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_1294\", %\"model.encoder.model.stages_1.blocks.0.attn_grid.norm1.weight\"{...}, %\"model.encoder.model.stages_1.blocks.0.attn_grid.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             406 |  # node_Concat_672\n",
       "                    %\"val_704\"<INT64,[6]>  ::Concat(%\"val_0\", %\"val_88\"{[16]}, %\"val_597\"{[4]}, %\"val_88\"{[16]}, %\"val_597\"{[4]}, %\"val_601\"{[128]}) {axis=0}\n",
       "             407 |  # node_view_25\n",
       "                    %\"view_25\"<FLOAT,[batch,16,4,16,4,128]>  ::Reshape(%\"layer_norm_10\", %\"val_704\") {allowzero=1}\n",
       "             408 |  # node_permute_15\n",
       "                    %\"permute_15\"<FLOAT,[batch,4,4,16,16,128]>  ::Transpose(%\"view_25\") {perm=(0, 2, 4, 1, 3, 5)}\n",
       "             409 |  # node_view_26\n",
       "                    %\"view_26\"<FLOAT,[16*batch,16,16,128]>  ::Reshape(%\"permute_15\", %\"val_608\"{[-1, 16, 16, 128]}) {allowzero=1}\n",
       "             410 |  # node_MatMul_680\n",
       "                    %\"val_712\"<FLOAT,[16*batch,16,16,384]>  ::MatMul(%\"view_26\", %\"val_711\"{...})\n",
       "             411 |  # node_linear_20\n",
       "                    %\"linear_20\"<FLOAT,[16*batch,16,16,384]>  ::Add(%\"val_712\", %\"model.encoder.model.stages_1.blocks.0.attn_grid.attn.qkv.bias\"{...})\n",
       "             412 |  # node_view_27\n",
       "                    %\"view_27\"<FLOAT,[16*batch,256,3,4,32]>  ::Reshape(%\"linear_20\", %\"val_618\") {allowzero=1}\n",
       "             413 |  # node_transpose_10\n",
       "                    %\"transpose_10\"<FLOAT,[16*batch,4,3,256,32]>  ::Transpose(%\"view_27\") {perm=(0, 3, 2, 1, 4)}\n",
       "             414 |  # node_Split_688\n",
       "                    %\"val_720\"<FLOAT,[16*batch,4,1,256,32]>, %\"val_721\"<FLOAT,[16*batch,4,1,256,32]>, %\"val_722\"<FLOAT,[16*batch,4,1,256,32]>  ::Split(%\"transpose_10\") {num_outputs=3, axis=2}\n",
       "             415 |  # node_unbind_5__0\n",
       "                    %\"getitem_45\"<FLOAT,[16*batch,4,256,32]>  ::Squeeze(%\"val_720\", %\"val_106\"{[2]})\n",
       "             416 |  # node_unbind_5__1\n",
       "                    %\"getitem_46\"<FLOAT,[16*batch,4,256,32]>  ::Squeeze(%\"val_721\", %\"val_106\"{[2]})\n",
       "             417 |  # node_unbind_5__2\n",
       "                    %\"getitem_47\"<FLOAT,[16*batch,4,256,32]>  ::Squeeze(%\"val_722\", %\"val_106\"{[2]})\n",
       "             418 |  # node_einsum_11\n",
       "                    %\"einsum_11\"<FLOAT,[4,16,16,16,16]>  ::Einsum(%\"einsum_10\"{...}, %\"model.encoder.model.stages_1.blocks.0.attn_grid.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "             419 |  # node__unsafe_view_10\n",
       "                    %\"_unsafe_view_10\"<FLOAT,[4,256,256]>  ::Reshape(%\"einsum_11\", %\"val_626\"{[4, 256, 256]}) {allowzero=1}\n",
       "             420 |  # node_Shape_703\n",
       "                    %\"val_737\"<INT64,[4]>  ::Shape(%\"getitem_46\") {start=0}\n",
       "             421 |  # node_Slice_705\n",
       "                    %\"val_739\"<INT64,[1]>  ::Slice(%\"val_737\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "             422 |  # node_Slice_706\n",
       "                    %\"val_740\"<INT64,[1]>  ::Slice(%\"val_737\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "             423 |  # node_Slice_708\n",
       "                    %\"val_742\"<INT64,[2]>  ::Slice(%\"val_737\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "             424 |  # node_Concat_710\n",
       "                    %\"val_744\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_740\", %\"val_739\") {axis=0}\n",
       "             425 |  # node_Reshape_711\n",
       "                    %\"val_745\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_46\", %\"val_744\") {allowzero=0}\n",
       "             426 |  # node_Transpose_712\n",
       "                    %\"val_746\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_745\") {perm=(0, 2, 1)}\n",
       "             427 |  # node_Concat_713\n",
       "                    %\"val_747\"<INT64,[4]>  ::Concat(%\"val_742\", %\"val_739\", %\"val_740\") {axis=0}\n",
       "             428 |  # node_Reshape_714\n",
       "                    %\"val_748\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_746\", %\"val_747\") {allowzero=0}\n",
       "             429 |  # node_Mul_716\n",
       "                    %\"val_750\"<FLOAT,[16*batch,4,256,32]>  ::Mul(%\"getitem_45\", %\"val_141\"{[0.4204482138156891]})\n",
       "             430 |  # node_Mul_718\n",
       "                    %\"val_752\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_748\", %\"val_141\"{[0.4204482138156891]})\n",
       "             431 |  # node_MatMul_719\n",
       "                    %\"val_753\"<FLOAT,[None,4,256,None]>  ::MatMul(%\"val_750\", %\"val_752\")\n",
       "             432 |  # node_Add_720\n",
       "                    %\"val_754\"<FLOAT,[None,4,256,256]>  ::Add(%\"val_753\", %\"_unsafe_view_10\")\n",
       "             433 |  # node_Softmax_721\n",
       "                    %\"val_755\"<FLOAT,[None,4,256,256]>  ::Softmax(%\"val_754\") {axis=-1}\n",
       "             434 |  # node_scaled_dot_product_attention_5\n",
       "                    %\"scaled_dot_product_attention_5\"<FLOAT,[16*batch,4,256,32]>  ::MatMul(%\"val_755\", %\"getitem_47\")\n",
       "             435 |  # node_transpose_11\n",
       "                    %\"transpose_11\"<FLOAT,[16*batch,256,4,32]>  ::Transpose(%\"scaled_dot_product_attention_5\") {perm=(0, 2, 1, 3)}\n",
       "             436 |  # node__unsafe_view_11\n",
       "                    %\"_unsafe_view_11\"<FLOAT,[16*batch,16,16,128]>  ::Reshape(%\"transpose_11\", %\"val_660\") {allowzero=1}\n",
       "             437 |  # node_MatMul_729\n",
       "                    %\"val_763\"<FLOAT,[16*batch,16,16,128]>  ::MatMul(%\"_unsafe_view_11\", %\"val_762\"{...})\n",
       "             438 |  # node_linear_21\n",
       "                    %\"linear_21\"<FLOAT,[16*batch,16,16,128]>  ::Add(%\"val_763\", %\"model.encoder.model.stages_1.blocks.0.attn_grid.attn.proj.bias\"{...})\n",
       "             439 |  # node_view_28\n",
       "                    %\"view_28\"<FLOAT,[batch,4,4,16,16,128]>  ::Reshape(%\"linear_21\", %\"val_670\"{[-1, 4, 4, 16, 16, 128]}) {allowzero=1}\n",
       "             440 |  # node_permute_16\n",
       "                    %\"permute_16\"<FLOAT,[batch,16,4,16,4,128]>  ::Transpose(%\"view_28\") {perm=(0, 3, 1, 4, 2, 5)}\n",
       "             441 |  # node_view_29\n",
       "                    %\"view_29\"<FLOAT,[batch,64,64,128]>  ::Reshape(%\"permute_16\", %\"val_676\"{[-1, 64, 64, 128]}) {allowzero=1}\n",
       "             442 |  # node_add_1419\n",
       "                    %\"add_1419\"<FLOAT,[batch,64,64,128]>  ::Add(%\"add_1294\", %\"view_29\")\n",
       "             443 |  # node_layer_norm_11\n",
       "                    %\"layer_norm_11\"<FLOAT,[batch,64,64,128]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_1419\", %\"model.encoder.model.stages_1.blocks.0.attn_grid.norm2.weight\"{...}, %\"model.encoder.model.stages_1.blocks.0.attn_grid.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             444 |  # node_MatMul_745\n",
       "                    %\"val_781\"<FLOAT,[batch,64,64,512]>  ::MatMul(%\"layer_norm_11\", %\"val_780\"{...})\n",
       "             445 |  # node_linear_22\n",
       "                    %\"linear_22\"<FLOAT,[batch,64,64,512]>  ::Add(%\"val_781\", %\"model.encoder.model.stages_1.blocks.0.attn_grid.mlp.fc1.bias\"{...})\n",
       "             446 |  # node_Pow_747\n",
       "                    %\"val_783\"<FLOAT,[batch,64,64,512]>  ::Pow(%\"linear_22\", %\"val_16\"{3.0})\n",
       "             447 |  # node_Mul_749\n",
       "                    %\"val_785\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_783\")\n",
       "             448 |  # node_Add_750\n",
       "                    %\"val_786\"<FLOAT,[batch,64,64,512]>  ::Add(%\"linear_22\", %\"val_785\")\n",
       "             449 |  # node_Mul_752\n",
       "                    %\"val_788\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_786\")\n",
       "             450 |  # node_Tanh_753\n",
       "                    %\"val_789\"<FLOAT,[batch,64,64,512]>  ::Tanh(%\"val_788\")\n",
       "             451 |  # node_Add_755\n",
       "                    %\"val_791\"<FLOAT,[batch,64,64,512]>  ::Add(%\"val_789\", %\"scalar_tensor_default\"{1.0})\n",
       "             452 |  # node_Mul_757\n",
       "                    %\"val_793\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"val_26\"{0.5}, %\"val_791\")\n",
       "             453 |  # node_gelu_12\n",
       "                    %\"gelu_12\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"linear_22\", %\"val_793\")\n",
       "             454 |  # node_MatMul_759\n",
       "                    %\"val_795\"<FLOAT,[batch,64,64,128]>  ::MatMul(%\"gelu_12\", %\"val_794\"{...})\n",
       "             455 |  # node_linear_23\n",
       "                    %\"linear_23\"<FLOAT,[batch,64,64,128]>  ::Add(%\"val_795\", %\"model.encoder.model.stages_1.blocks.0.attn_grid.mlp.fc2.bias\"{...})\n",
       "             456 |  # node_add_1455\n",
       "                    %\"add_1455\"<FLOAT,[batch,64,64,128]>  ::Add(%\"add_1419\", %\"linear_23\")\n",
       "             457 |  # node_permute_17\n",
       "                    %\"permute_17\"<FLOAT,[batch,128,64,64]>  ::Transpose(%\"add_1455\") {perm=(0, 3, 1, 2)}\n",
       "             458 |  # node__native_batch_norm_legit_no_training_10__0\n",
       "                    %\"getitem_48\"<FLOAT,[batch,128,64,64]>  ::BatchNormalization(%\"permute_17\", %\"model.encoder.model.stages_1.blocks.1.conv.pre_norm.weight\"{...}, %\"model.encoder.model.stages_1.blocks.1.conv.pre_norm.bias\"{...}, %\"model.encoder.model.stages_1.blocks.1.conv.pre_norm.running_mean\"{...}, %\"model.encoder.model.stages_1.blocks.1.conv.pre_norm.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             459 |  # node_conv2d_18\n",
       "                    %\"conv2d_18\"<FLOAT,[batch,512,64,64]>  ::Conv(%\"getitem_48\", %\"model.encoder.model.stages_1.blocks.1.conv.conv1_1x1.weight\"{...}, %\"val_550\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             460 |  # node__native_batch_norm_legit_no_training_11__0\n",
       "                    %\"getitem_51\"<FLOAT,[batch,512,64,64]>  ::BatchNormalization(%\"conv2d_18\", %\"model.encoder.model.stages_1.blocks.1.conv.norm1.weight\"{...}, %\"model.encoder.model.stages_1.blocks.1.conv.norm1.bias\"{...}, %\"model.encoder.model.stages_1.blocks.1.conv.norm1.running_mean\"{...}, %\"model.encoder.model.stages_1.blocks.1.conv.norm1.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             461 |  # node_Pow_774\n",
       "                    %\"val_810\"<FLOAT,[batch,512,64,64]>  ::Pow(%\"getitem_51\", %\"val_16\"{3.0})\n",
       "             462 |  # node_Mul_776\n",
       "                    %\"val_812\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_810\")\n",
       "             463 |  # node_Add_777\n",
       "                    %\"val_813\"<FLOAT,[batch,512,64,64]>  ::Add(%\"getitem_51\", %\"val_812\")\n",
       "             464 |  # node_Mul_779\n",
       "                    %\"val_815\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_813\")\n",
       "             465 |  # node_Tanh_780\n",
       "                    %\"val_816\"<FLOAT,[batch,512,64,64]>  ::Tanh(%\"val_815\")\n",
       "             466 |  # node_Add_782\n",
       "                    %\"val_818\"<FLOAT,[batch,512,64,64]>  ::Add(%\"val_816\", %\"scalar_tensor_default\"{1.0})\n",
       "             467 |  # node_Mul_784\n",
       "                    %\"val_820\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"val_26\"{0.5}, %\"val_818\")\n",
       "             468 |  # node_gelu_13\n",
       "                    %\"gelu_13\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"getitem_51\", %\"val_820\")\n",
       "             469 |  # node_conv2d_19\n",
       "                    %\"conv2d_19\"<FLOAT,[batch,512,64,64]>  ::Conv(%\"gelu_13\", %\"model.encoder.model.stages_1.blocks.1.conv.conv2_kxk.weight\"{...}, %\"val_550\"{...}) {group=512, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             470 |  # node__native_batch_norm_legit_no_training_12__0\n",
       "                    %\"getitem_54\"<FLOAT,[batch,512,64,64]>  ::BatchNormalization(%\"conv2d_19\", %\"model.encoder.model.stages_1.blocks.1.conv.norm2.weight\"{...}, %\"model.encoder.model.stages_1.blocks.1.conv.norm2.bias\"{...}, %\"model.encoder.model.stages_1.blocks.1.conv.norm2.running_mean\"{...}, %\"model.encoder.model.stages_1.blocks.1.conv.norm2.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             471 |  # node_Pow_795\n",
       "                    %\"val_831\"<FLOAT,[batch,512,64,64]>  ::Pow(%\"getitem_54\", %\"val_16\"{3.0})\n",
       "             472 |  # node_Mul_797\n",
       "                    %\"val_833\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_831\")\n",
       "             473 |  # node_Add_798\n",
       "                    %\"val_834\"<FLOAT,[batch,512,64,64]>  ::Add(%\"getitem_54\", %\"val_833\")\n",
       "             474 |  # node_Mul_800\n",
       "                    %\"val_836\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_834\")\n",
       "             475 |  # node_Tanh_801\n",
       "                    %\"val_837\"<FLOAT,[batch,512,64,64]>  ::Tanh(%\"val_836\")\n",
       "             476 |  # node_Add_803\n",
       "                    %\"val_839\"<FLOAT,[batch,512,64,64]>  ::Add(%\"val_837\", %\"scalar_tensor_default\"{1.0})\n",
       "             477 |  # node_Mul_805\n",
       "                    %\"val_841\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"val_26\"{0.5}, %\"val_839\")\n",
       "             478 |  # node_gelu_14\n",
       "                    %\"gelu_14\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"getitem_54\", %\"val_841\")\n",
       "             479 |  # node_mean_3\n",
       "                    %\"mean_3\"<FLOAT,[batch,512,1,1]>  ::ReduceMean(%\"gelu_14\", %\"val_81\"{[2, 3]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             480 |  # node_conv2d_20\n",
       "                    %\"conv2d_20\"<FLOAT,[batch,32,1,1]>  ::Conv(%\"mean_3\", %\"model.encoder.model.stages_1.blocks.1.conv.se.fc1.weight\"{...}, %\"model.encoder.model.stages_1.blocks.1.conv.se.fc1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             481 |  # node_Sigmoid_808\n",
       "                    %\"val_844\"<FLOAT,[batch,32,1,1]>  ::Sigmoid(%\"conv2d_20\")\n",
       "             482 |  # node_silu_3\n",
       "                    %\"silu_3\"<FLOAT,[batch,32,1,1]>  ::Mul(%\"conv2d_20\", %\"val_844\")\n",
       "             483 |  # node_conv2d_21\n",
       "                    %\"conv2d_21\"<FLOAT,[batch,512,1,1]>  ::Conv(%\"silu_3\", %\"model.encoder.model.stages_1.blocks.1.conv.se.fc2.weight\"{...}, %\"model.encoder.model.stages_1.blocks.1.conv.se.fc2.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             484 |  # node_sigmoid_3\n",
       "                    %\"sigmoid_3\"<FLOAT,[batch,512,1,1]>  ::Sigmoid(%\"conv2d_21\")\n",
       "             485 |  # node_mul_868\n",
       "                    %\"mul_868\"<FLOAT,[batch,512,64,64]>  ::Mul(%\"gelu_14\", %\"sigmoid_3\")\n",
       "             486 |  # node_conv2d_22\n",
       "                    %\"conv2d_22\"<FLOAT,[batch,128,64,64]>  ::Conv(%\"mul_868\", %\"model.encoder.model.stages_1.blocks.1.conv.conv3_1x1.weight\"{...}, %\"model.encoder.model.stages_1.blocks.1.conv.conv3_1x1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             487 |  # node_add_1542\n",
       "                    %\"add_1542\"<FLOAT,[batch,128,64,64]>  ::Add(%\"conv2d_22\", %\"permute_17\")\n",
       "             488 |  # node_permute_18\n",
       "                    %\"permute_18\"<FLOAT,[batch,64,64,128]>  ::Transpose(%\"add_1542\") {perm=(0, 2, 3, 1)}\n",
       "             489 |  # node_layer_norm_12\n",
       "                    %\"layer_norm_12\"<FLOAT,[batch,64,64,128]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_18\", %\"model.encoder.model.stages_1.blocks.1.attn_block.norm1.weight\"{...}, %\"model.encoder.model.stages_1.blocks.1.attn_block.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             490 |  # node_view_30\n",
       "                    %\"view_30\"<FLOAT,[batch,4,16,4,16,128]>  ::Reshape(%\"layer_norm_12\", %\"val_602\") {allowzero=1}\n",
       "             491 |  # node_permute_19\n",
       "                    %\"permute_19\"<FLOAT,[batch,4,4,16,16,128]>  ::Transpose(%\"view_30\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "             492 |  # node_view_31\n",
       "                    %\"view_31\"<FLOAT,[16*batch,16,16,128]>  ::Reshape(%\"permute_19\", %\"val_608\"{[-1, 16, 16, 128]}) {allowzero=1}\n",
       "             493 |  # node_MatMul_824\n",
       "                    %\"val_862\"<FLOAT,[16*batch,16,16,384]>  ::MatMul(%\"view_31\", %\"val_861\"{...})\n",
       "             494 |  # node_linear_24\n",
       "                    %\"linear_24\"<FLOAT,[16*batch,16,16,384]>  ::Add(%\"val_862\", %\"model.encoder.model.stages_1.blocks.1.attn_block.attn.qkv.bias\"{...})\n",
       "             495 |  # node_view_32\n",
       "                    %\"view_32\"<FLOAT,[16*batch,256,3,4,32]>  ::Reshape(%\"linear_24\", %\"val_618\") {allowzero=1}\n",
       "             496 |  # node_transpose_12\n",
       "                    %\"transpose_12\"<FLOAT,[16*batch,4,3,256,32]>  ::Transpose(%\"view_32\") {perm=(0, 3, 2, 1, 4)}\n",
       "             497 |  # node_Split_832\n",
       "                    %\"val_870\"<FLOAT,[16*batch,4,1,256,32]>, %\"val_871\"<FLOAT,[16*batch,4,1,256,32]>, %\"val_872\"<FLOAT,[16*batch,4,1,256,32]>  ::Split(%\"transpose_12\") {num_outputs=3, axis=2}\n",
       "             498 |  # node_unbind_6__0\n",
       "                    %\"getitem_57\"<FLOAT,[16*batch,4,256,32]>  ::Squeeze(%\"val_870\", %\"val_106\"{[2]})\n",
       "             499 |  # node_unbind_6__1\n",
       "                    %\"getitem_58\"<FLOAT,[16*batch,4,256,32]>  ::Squeeze(%\"val_871\", %\"val_106\"{[2]})\n",
       "             500 |  # node_unbind_6__2\n",
       "                    %\"getitem_59\"<FLOAT,[16*batch,4,256,32]>  ::Squeeze(%\"val_872\", %\"val_106\"{[2]})\n",
       "             501 |  # node_einsum_13\n",
       "                    %\"einsum_13\"<FLOAT,[4,16,16,16,16]>  ::Einsum(%\"einsum_12\"{...}, %\"model.encoder.model.stages_1.blocks.1.attn_block.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "             502 |  # node__unsafe_view_12\n",
       "                    %\"_unsafe_view_12\"<FLOAT,[4,256,256]>  ::Reshape(%\"einsum_13\", %\"val_626\"{[4, 256, 256]}) {allowzero=1}\n",
       "             503 |  # node_Shape_847\n",
       "                    %\"val_887\"<INT64,[4]>  ::Shape(%\"getitem_58\") {start=0}\n",
       "             504 |  # node_Slice_849\n",
       "                    %\"val_889\"<INT64,[1]>  ::Slice(%\"val_887\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "             505 |  # node_Slice_850\n",
       "                    %\"val_890\"<INT64,[1]>  ::Slice(%\"val_887\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "             506 |  # node_Slice_852\n",
       "                    %\"val_892\"<INT64,[2]>  ::Slice(%\"val_887\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "             507 |  # node_Concat_854\n",
       "                    %\"val_894\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_890\", %\"val_889\") {axis=0}\n",
       "             508 |  # node_Reshape_855\n",
       "                    %\"val_895\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_58\", %\"val_894\") {allowzero=0}\n",
       "             509 |  # node_Transpose_856\n",
       "                    %\"val_896\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_895\") {perm=(0, 2, 1)}\n",
       "             510 |  # node_Concat_857\n",
       "                    %\"val_897\"<INT64,[4]>  ::Concat(%\"val_892\", %\"val_889\", %\"val_890\") {axis=0}\n",
       "             511 |  # node_Reshape_858\n",
       "                    %\"val_898\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_896\", %\"val_897\") {allowzero=0}\n",
       "             512 |  # node_Mul_860\n",
       "                    %\"val_900\"<FLOAT,[16*batch,4,256,32]>  ::Mul(%\"getitem_57\", %\"val_141\"{[0.4204482138156891]})\n",
       "             513 |  # node_Mul_862\n",
       "                    %\"val_902\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_898\", %\"val_141\"{[0.4204482138156891]})\n",
       "             514 |  # node_MatMul_863\n",
       "                    %\"val_903\"<FLOAT,[None,4,256,None]>  ::MatMul(%\"val_900\", %\"val_902\")\n",
       "             515 |  # node_Add_864\n",
       "                    %\"val_904\"<FLOAT,[None,4,256,256]>  ::Add(%\"val_903\", %\"_unsafe_view_12\")\n",
       "             516 |  # node_Softmax_865\n",
       "                    %\"val_905\"<FLOAT,[None,4,256,256]>  ::Softmax(%\"val_904\") {axis=-1}\n",
       "             517 |  # node_scaled_dot_product_attention_6\n",
       "                    %\"scaled_dot_product_attention_6\"<FLOAT,[16*batch,4,256,32]>  ::MatMul(%\"val_905\", %\"getitem_59\")\n",
       "             518 |  # node_transpose_13\n",
       "                    %\"transpose_13\"<FLOAT,[16*batch,256,4,32]>  ::Transpose(%\"scaled_dot_product_attention_6\") {perm=(0, 2, 1, 3)}\n",
       "             519 |  # node__unsafe_view_13\n",
       "                    %\"_unsafe_view_13\"<FLOAT,[16*batch,16,16,128]>  ::Reshape(%\"transpose_13\", %\"val_660\") {allowzero=1}\n",
       "             520 |  # node_MatMul_873\n",
       "                    %\"val_913\"<FLOAT,[16*batch,16,16,128]>  ::MatMul(%\"_unsafe_view_13\", %\"val_912\"{...})\n",
       "             521 |  # node_linear_25\n",
       "                    %\"linear_25\"<FLOAT,[16*batch,16,16,128]>  ::Add(%\"val_913\", %\"model.encoder.model.stages_1.blocks.1.attn_block.attn.proj.bias\"{...})\n",
       "             522 |  # node_view_33\n",
       "                    %\"view_33\"<FLOAT,[batch,4,4,16,16,128]>  ::Reshape(%\"linear_25\", %\"val_670\"{[-1, 4, 4, 16, 16, 128]}) {allowzero=1}\n",
       "             523 |  # node_permute_20\n",
       "                    %\"permute_20\"<FLOAT,[batch,4,16,4,16,128]>  ::Transpose(%\"view_33\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "             524 |  # node_view_34\n",
       "                    %\"view_34\"<FLOAT,[batch,64,64,128]>  ::Reshape(%\"permute_20\", %\"val_676\"{[-1, 64, 64, 128]}) {allowzero=1}\n",
       "             525 |  # node_add_1672\n",
       "                    %\"add_1672\"<FLOAT,[batch,64,64,128]>  ::Add(%\"permute_18\", %\"view_34\")\n",
       "             526 |  # node_layer_norm_13\n",
       "                    %\"layer_norm_13\"<FLOAT,[batch,64,64,128]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_1672\", %\"model.encoder.model.stages_1.blocks.1.attn_block.norm2.weight\"{...}, %\"model.encoder.model.stages_1.blocks.1.attn_block.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             527 |  # node_MatMul_889\n",
       "                    %\"val_931\"<FLOAT,[batch,64,64,512]>  ::MatMul(%\"layer_norm_13\", %\"val_930\"{...})\n",
       "             528 |  # node_linear_26\n",
       "                    %\"linear_26\"<FLOAT,[batch,64,64,512]>  ::Add(%\"val_931\", %\"model.encoder.model.stages_1.blocks.1.attn_block.mlp.fc1.bias\"{...})\n",
       "             529 |  # node_Pow_891\n",
       "                    %\"val_933\"<FLOAT,[batch,64,64,512]>  ::Pow(%\"linear_26\", %\"val_16\"{3.0})\n",
       "             530 |  # node_Mul_893\n",
       "                    %\"val_935\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_933\")\n",
       "             531 |  # node_Add_894\n",
       "                    %\"val_936\"<FLOAT,[batch,64,64,512]>  ::Add(%\"linear_26\", %\"val_935\")\n",
       "             532 |  # node_Mul_896\n",
       "                    %\"val_938\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_936\")\n",
       "             533 |  # node_Tanh_897\n",
       "                    %\"val_939\"<FLOAT,[batch,64,64,512]>  ::Tanh(%\"val_938\")\n",
       "             534 |  # node_Add_899\n",
       "                    %\"val_941\"<FLOAT,[batch,64,64,512]>  ::Add(%\"val_939\", %\"scalar_tensor_default\"{1.0})\n",
       "             535 |  # node_Mul_901\n",
       "                    %\"val_943\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"val_26\"{0.5}, %\"val_941\")\n",
       "             536 |  # node_gelu_15\n",
       "                    %\"gelu_15\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"linear_26\", %\"val_943\")\n",
       "             537 |  # node_MatMul_903\n",
       "                    %\"val_945\"<FLOAT,[batch,64,64,128]>  ::MatMul(%\"gelu_15\", %\"val_944\"{...})\n",
       "             538 |  # node_linear_27\n",
       "                    %\"linear_27\"<FLOAT,[batch,64,64,128]>  ::Add(%\"val_945\", %\"model.encoder.model.stages_1.blocks.1.attn_block.mlp.fc2.bias\"{...})\n",
       "             539 |  # node_add_1708\n",
       "                    %\"add_1708\"<FLOAT,[batch,64,64,128]>  ::Add(%\"add_1672\", %\"linear_27\")\n",
       "             540 |  # node_layer_norm_14\n",
       "                    %\"layer_norm_14\"<FLOAT,[batch,64,64,128]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_1708\", %\"model.encoder.model.stages_1.blocks.1.attn_grid.norm1.weight\"{...}, %\"model.encoder.model.stages_1.blocks.1.attn_grid.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             541 |  # node_view_35\n",
       "                    %\"view_35\"<FLOAT,[batch,16,4,16,4,128]>  ::Reshape(%\"layer_norm_14\", %\"val_704\") {allowzero=1}\n",
       "             542 |  # node_permute_21\n",
       "                    %\"permute_21\"<FLOAT,[batch,4,4,16,16,128]>  ::Transpose(%\"view_35\") {perm=(0, 2, 4, 1, 3, 5)}\n",
       "             543 |  # node_view_36\n",
       "                    %\"view_36\"<FLOAT,[16*batch,16,16,128]>  ::Reshape(%\"permute_21\", %\"val_608\"{[-1, 16, 16, 128]}) {allowzero=1}\n",
       "             544 |  # node_MatMul_919\n",
       "                    %\"val_963\"<FLOAT,[16*batch,16,16,384]>  ::MatMul(%\"view_36\", %\"val_962\"{...})\n",
       "             545 |  # node_linear_28\n",
       "                    %\"linear_28\"<FLOAT,[16*batch,16,16,384]>  ::Add(%\"val_963\", %\"model.encoder.model.stages_1.blocks.1.attn_grid.attn.qkv.bias\"{...})\n",
       "             546 |  # node_view_37\n",
       "                    %\"view_37\"<FLOAT,[16*batch,256,3,4,32]>  ::Reshape(%\"linear_28\", %\"val_618\") {allowzero=1}\n",
       "             547 |  # node_transpose_14\n",
       "                    %\"transpose_14\"<FLOAT,[16*batch,4,3,256,32]>  ::Transpose(%\"view_37\") {perm=(0, 3, 2, 1, 4)}\n",
       "             548 |  # node_Split_927\n",
       "                    %\"val_971\"<FLOAT,[16*batch,4,1,256,32]>, %\"val_972\"<FLOAT,[16*batch,4,1,256,32]>, %\"val_973\"<FLOAT,[16*batch,4,1,256,32]>  ::Split(%\"transpose_14\") {num_outputs=3, axis=2}\n",
       "             549 |  # node_unbind_7__0\n",
       "                    %\"getitem_60\"<FLOAT,[16*batch,4,256,32]>  ::Squeeze(%\"val_971\", %\"val_106\"{[2]})\n",
       "             550 |  # node_unbind_7__1\n",
       "                    %\"getitem_61\"<FLOAT,[16*batch,4,256,32]>  ::Squeeze(%\"val_972\", %\"val_106\"{[2]})\n",
       "             551 |  # node_unbind_7__2\n",
       "                    %\"getitem_62\"<FLOAT,[16*batch,4,256,32]>  ::Squeeze(%\"val_973\", %\"val_106\"{[2]})\n",
       "             552 |  # node_einsum_15\n",
       "                    %\"einsum_15\"<FLOAT,[4,16,16,16,16]>  ::Einsum(%\"einsum_14\"{...}, %\"model.encoder.model.stages_1.blocks.1.attn_grid.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "             553 |  # node__unsafe_view_14\n",
       "                    %\"_unsafe_view_14\"<FLOAT,[4,256,256]>  ::Reshape(%\"einsum_15\", %\"val_626\"{[4, 256, 256]}) {allowzero=1}\n",
       "             554 |  # node_Shape_942\n",
       "                    %\"val_988\"<INT64,[4]>  ::Shape(%\"getitem_61\") {start=0}\n",
       "             555 |  # node_Slice_944\n",
       "                    %\"val_990\"<INT64,[1]>  ::Slice(%\"val_988\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "             556 |  # node_Slice_945\n",
       "                    %\"val_991\"<INT64,[1]>  ::Slice(%\"val_988\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "             557 |  # node_Slice_947\n",
       "                    %\"val_993\"<INT64,[2]>  ::Slice(%\"val_988\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "             558 |  # node_Concat_949\n",
       "                    %\"val_995\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_991\", %\"val_990\") {axis=0}\n",
       "             559 |  # node_Reshape_950\n",
       "                    %\"val_996\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_61\", %\"val_995\") {allowzero=0}\n",
       "             560 |  # node_Transpose_951\n",
       "                    %\"val_997\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_996\") {perm=(0, 2, 1)}\n",
       "             561 |  # node_Concat_952\n",
       "                    %\"val_998\"<INT64,[4]>  ::Concat(%\"val_993\", %\"val_990\", %\"val_991\") {axis=0}\n",
       "             562 |  # node_Reshape_953\n",
       "                    %\"val_999\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_997\", %\"val_998\") {allowzero=0}\n",
       "             563 |  # node_Mul_955\n",
       "                    %\"val_1001\"<FLOAT,[16*batch,4,256,32]>  ::Mul(%\"getitem_60\", %\"val_141\"{[0.4204482138156891]})\n",
       "             564 |  # node_Mul_957\n",
       "                    %\"val_1003\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_999\", %\"val_141\"{[0.4204482138156891]})\n",
       "             565 |  # node_MatMul_958\n",
       "                    %\"val_1004\"<FLOAT,[None,4,256,None]>  ::MatMul(%\"val_1001\", %\"val_1003\")\n",
       "             566 |  # node_Add_959\n",
       "                    %\"val_1005\"<FLOAT,[None,4,256,256]>  ::Add(%\"val_1004\", %\"_unsafe_view_14\")\n",
       "             567 |  # node_Softmax_960\n",
       "                    %\"val_1006\"<FLOAT,[None,4,256,256]>  ::Softmax(%\"val_1005\") {axis=-1}\n",
       "             568 |  # node_scaled_dot_product_attention_7\n",
       "                    %\"scaled_dot_product_attention_7\"<FLOAT,[16*batch,4,256,32]>  ::MatMul(%\"val_1006\", %\"getitem_62\")\n",
       "             569 |  # node_transpose_15\n",
       "                    %\"transpose_15\"<FLOAT,[16*batch,256,4,32]>  ::Transpose(%\"scaled_dot_product_attention_7\") {perm=(0, 2, 1, 3)}\n",
       "             570 |  # node__unsafe_view_15\n",
       "                    %\"_unsafe_view_15\"<FLOAT,[16*batch,16,16,128]>  ::Reshape(%\"transpose_15\", %\"val_660\") {allowzero=1}\n",
       "             571 |  # node_MatMul_968\n",
       "                    %\"val_1014\"<FLOAT,[16*batch,16,16,128]>  ::MatMul(%\"_unsafe_view_15\", %\"val_1013\"{...})\n",
       "             572 |  # node_linear_29\n",
       "                    %\"linear_29\"<FLOAT,[16*batch,16,16,128]>  ::Add(%\"val_1014\", %\"model.encoder.model.stages_1.blocks.1.attn_grid.attn.proj.bias\"{...})\n",
       "             573 |  # node_view_38\n",
       "                    %\"view_38\"<FLOAT,[batch,4,4,16,16,128]>  ::Reshape(%\"linear_29\", %\"val_670\"{[-1, 4, 4, 16, 16, 128]}) {allowzero=1}\n",
       "             574 |  # node_permute_22\n",
       "                    %\"permute_22\"<FLOAT,[batch,16,4,16,4,128]>  ::Transpose(%\"view_38\") {perm=(0, 3, 1, 4, 2, 5)}\n",
       "             575 |  # node_view_39\n",
       "                    %\"view_39\"<FLOAT,[batch,64,64,128]>  ::Reshape(%\"permute_22\", %\"val_676\"{[-1, 64, 64, 128]}) {allowzero=1}\n",
       "             576 |  # node_add_1833\n",
       "                    %\"add_1833\"<FLOAT,[batch,64,64,128]>  ::Add(%\"add_1708\", %\"view_39\")\n",
       "             577 |  # node_layer_norm_15\n",
       "                    %\"layer_norm_15\"<FLOAT,[batch,64,64,128]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_1833\", %\"model.encoder.model.stages_1.blocks.1.attn_grid.norm2.weight\"{...}, %\"model.encoder.model.stages_1.blocks.1.attn_grid.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             578 |  # node_MatMul_984\n",
       "                    %\"val_1032\"<FLOAT,[batch,64,64,512]>  ::MatMul(%\"layer_norm_15\", %\"val_1031\"{...})\n",
       "             579 |  # node_linear_30\n",
       "                    %\"linear_30\"<FLOAT,[batch,64,64,512]>  ::Add(%\"val_1032\", %\"model.encoder.model.stages_1.blocks.1.attn_grid.mlp.fc1.bias\"{...})\n",
       "             580 |  # node_Pow_986\n",
       "                    %\"val_1034\"<FLOAT,[batch,64,64,512]>  ::Pow(%\"linear_30\", %\"val_16\"{3.0})\n",
       "             581 |  # node_Mul_988\n",
       "                    %\"val_1036\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1034\")\n",
       "             582 |  # node_Add_989\n",
       "                    %\"val_1037\"<FLOAT,[batch,64,64,512]>  ::Add(%\"linear_30\", %\"val_1036\")\n",
       "             583 |  # node_Mul_991\n",
       "                    %\"val_1039\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1037\")\n",
       "             584 |  # node_Tanh_992\n",
       "                    %\"val_1040\"<FLOAT,[batch,64,64,512]>  ::Tanh(%\"val_1039\")\n",
       "             585 |  # node_Add_994\n",
       "                    %\"val_1042\"<FLOAT,[batch,64,64,512]>  ::Add(%\"val_1040\", %\"scalar_tensor_default\"{1.0})\n",
       "             586 |  # node_Mul_996\n",
       "                    %\"val_1044\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"val_26\"{0.5}, %\"val_1042\")\n",
       "             587 |  # node_gelu_16\n",
       "                    %\"gelu_16\"<FLOAT,[batch,64,64,512]>  ::Mul(%\"linear_30\", %\"val_1044\")\n",
       "             588 |  # node_MatMul_998\n",
       "                    %\"val_1046\"<FLOAT,[batch,64,64,128]>  ::MatMul(%\"gelu_16\", %\"val_1045\"{...})\n",
       "             589 |  # node_linear_31\n",
       "                    %\"linear_31\"<FLOAT,[batch,64,64,128]>  ::Add(%\"val_1046\", %\"model.encoder.model.stages_1.blocks.1.attn_grid.mlp.fc2.bias\"{...})\n",
       "             590 |  # node_add_1869\n",
       "                    %\"add_1869\"<FLOAT,[batch,64,64,128]>  ::Add(%\"add_1833\", %\"linear_31\")\n",
       "             591 |  # node_permute_23\n",
       "                    %\"permute_23\"<FLOAT,[batch,128,64,64]>  ::Transpose(%\"add_1869\") {perm=(0, 3, 1, 2)}\n",
       "             592 |  # node_pad_5\n",
       "                    %\"pad_5\"<FLOAT,[batch,128,64,64]>  ::Pad(%\"permute_23\", %\"val_28\"{[0, 0, 0, 0, 0, 0, 0, 0]}, %\"val_29\"{0.0}) {mode='constant'}\n",
       "             593 |  # node_avg_pool2d_2\n",
       "                    %\"avg_pool2d_2\"<FLOAT,[batch,128,32,32]>  ::AveragePool(%\"pad_5\") {count_include_pad=1, ceil_mode=0, pads=(0, 0, 0, 0), kernel_shape=(2, 2), strides=(2, 2), auto_pad='NOTSET'}\n",
       "             594 |  # node_conv2d_23\n",
       "                    %\"conv2d_23\"<FLOAT,[batch,256,32,32]>  ::Conv(%\"avg_pool2d_2\", %\"model.encoder.model.stages_2.blocks.0.conv.shortcut.expand.weight\"{...}, %\"model.encoder.model.stages_2.blocks.0.conv.shortcut.expand.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             595 |  # node__native_batch_norm_legit_no_training_13__0\n",
       "                    %\"getitem_63\"<FLOAT,[batch,128,64,64]>  ::BatchNormalization(%\"permute_23\", %\"model.encoder.model.stages_2.blocks.0.conv.pre_norm.weight\"{...}, %\"model.encoder.model.stages_2.blocks.0.conv.pre_norm.bias\"{...}, %\"model.encoder.model.stages_2.blocks.0.conv.pre_norm.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.0.conv.pre_norm.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             596 |  # node_conv2d_24\n",
       "                    %\"conv2d_24\"<FLOAT,[batch,1024,64,64]>  ::Conv(%\"getitem_63\", %\"model.encoder.model.stages_2.blocks.0.conv.conv1_1x1.weight\"{...}, %\"val_1057\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             597 |  # node__native_batch_norm_legit_no_training_14__0\n",
       "                    %\"getitem_66\"<FLOAT,[batch,1024,64,64]>  ::BatchNormalization(%\"conv2d_24\", %\"model.encoder.model.stages_2.blocks.0.conv.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.0.conv.norm1.bias\"{...}, %\"model.encoder.model.stages_2.blocks.0.conv.norm1.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.0.conv.norm1.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             598 |  # node_Pow_1015\n",
       "                    %\"val_1063\"<FLOAT,[batch,1024,64,64]>  ::Pow(%\"getitem_66\", %\"val_16\"{3.0})\n",
       "             599 |  # node_Mul_1017\n",
       "                    %\"val_1065\"<FLOAT,[batch,1024,64,64]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1063\")\n",
       "             600 |  # node_Add_1018\n",
       "                    %\"val_1066\"<FLOAT,[batch,1024,64,64]>  ::Add(%\"getitem_66\", %\"val_1065\")\n",
       "             601 |  # node_Mul_1020\n",
       "                    %\"val_1068\"<FLOAT,[batch,1024,64,64]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1066\")\n",
       "             602 |  # node_Tanh_1021\n",
       "                    %\"val_1069\"<FLOAT,[batch,1024,64,64]>  ::Tanh(%\"val_1068\")\n",
       "             603 |  # node_Add_1023\n",
       "                    %\"val_1071\"<FLOAT,[batch,1024,64,64]>  ::Add(%\"val_1069\", %\"scalar_tensor_default\"{1.0})\n",
       "             604 |  # node_Mul_1025\n",
       "                    %\"val_1073\"<FLOAT,[batch,1024,64,64]>  ::Mul(%\"val_26\"{0.5}, %\"val_1071\")\n",
       "             605 |  # node_gelu_17\n",
       "                    %\"gelu_17\"<FLOAT,[batch,1024,64,64]>  ::Mul(%\"getitem_66\", %\"val_1073\")\n",
       "             606 |  # node_conv2d_25\n",
       "                    %\"conv2d_25\"<FLOAT,[batch,1024,32,32]>  ::Conv(%\"gelu_17\", %\"model.encoder.model.stages_2.blocks.0.conv.conv2_kxk.weight\"{...}, %\"val_1057\"{...}) {group=1024, pads=(0, 0, 1, 1), strides=(2, 2), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             607 |  # node__native_batch_norm_legit_no_training_15__0\n",
       "                    %\"getitem_69\"<FLOAT,[batch,1024,32,32]>  ::BatchNormalization(%\"conv2d_25\", %\"model.encoder.model.stages_2.blocks.0.conv.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.0.conv.norm2.bias\"{...}, %\"model.encoder.model.stages_2.blocks.0.conv.norm2.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.0.conv.norm2.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             608 |  # node_Pow_1038\n",
       "                    %\"val_1086\"<FLOAT,[batch,1024,32,32]>  ::Pow(%\"getitem_69\", %\"val_16\"{3.0})\n",
       "             609 |  # node_Mul_1040\n",
       "                    %\"val_1088\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1086\")\n",
       "             610 |  # node_Add_1041\n",
       "                    %\"val_1089\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"getitem_69\", %\"val_1088\")\n",
       "             611 |  # node_Mul_1043\n",
       "                    %\"val_1091\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1089\")\n",
       "             612 |  # node_Tanh_1044\n",
       "                    %\"val_1092\"<FLOAT,[batch,1024,32,32]>  ::Tanh(%\"val_1091\")\n",
       "             613 |  # node_Add_1046\n",
       "                    %\"val_1094\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"val_1092\", %\"scalar_tensor_default\"{1.0})\n",
       "             614 |  # node_Mul_1048\n",
       "                    %\"val_1096\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_26\"{0.5}, %\"val_1094\")\n",
       "             615 |  # node_gelu_18\n",
       "                    %\"gelu_18\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"getitem_69\", %\"val_1096\")\n",
       "             616 |  # node_mean_4\n",
       "                    %\"mean_4\"<FLOAT,[batch,1024,1,1]>  ::ReduceMean(%\"gelu_18\", %\"val_81\"{[2, 3]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             617 |  # node_conv2d_26\n",
       "                    %\"conv2d_26\"<FLOAT,[batch,64,1,1]>  ::Conv(%\"mean_4\", %\"model.encoder.model.stages_2.blocks.0.conv.se.fc1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.0.conv.se.fc1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             618 |  # node_Sigmoid_1051\n",
       "                    %\"val_1099\"<FLOAT,[batch,64,1,1]>  ::Sigmoid(%\"conv2d_26\")\n",
       "             619 |  # node_silu_4\n",
       "                    %\"silu_4\"<FLOAT,[batch,64,1,1]>  ::Mul(%\"conv2d_26\", %\"val_1099\")\n",
       "             620 |  # node_conv2d_27\n",
       "                    %\"conv2d_27\"<FLOAT,[batch,1024,1,1]>  ::Conv(%\"silu_4\", %\"model.encoder.model.stages_2.blocks.0.conv.se.fc2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.0.conv.se.fc2.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             621 |  # node_sigmoid_4\n",
       "                    %\"sigmoid_4\"<FLOAT,[batch,1024,1,1]>  ::Sigmoid(%\"conv2d_27\")\n",
       "             622 |  # node_mul_1120\n",
       "                    %\"mul_1120\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"gelu_18\", %\"sigmoid_4\")\n",
       "             623 |  # node_conv2d_28\n",
       "                    %\"conv2d_28\"<FLOAT,[batch,256,32,32]>  ::Conv(%\"mul_1120\", %\"model.encoder.model.stages_2.blocks.0.conv.conv3_1x1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.0.conv.shortcut.expand.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             624 |  # node_add_1976\n",
       "                    %\"add_1976\"<FLOAT,[batch,256,32,32]>  ::Add(%\"conv2d_28\", %\"conv2d_23\")\n",
       "             625 |  # node_permute_24\n",
       "                    %\"permute_24\"<FLOAT,[batch,32,32,256]>  ::Transpose(%\"add_1976\") {perm=(0, 2, 3, 1)}\n",
       "             626 |  # node_layer_norm_16\n",
       "                    %\"layer_norm_16\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_24\", %\"model.encoder.model.stages_2.blocks.0.attn_block.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.0.attn_block.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             627 |  # node_Concat_1059\n",
       "                    %\"val_1109\"<INT64,[6]>  ::Concat(%\"val_0\", %\"val_106\"{[2]}, %\"val_88\"{[16]}, %\"val_106\"{[2]}, %\"val_88\"{[16]}, %\"val_1108\"{[256]}) {axis=0}\n",
       "             628 |  # node_view_40\n",
       "                    %\"view_40\"<FLOAT,[batch,2,16,2,16,256]>  ::Reshape(%\"layer_norm_16\", %\"val_1109\") {allowzero=1}\n",
       "             629 |  # node_permute_25\n",
       "                    %\"permute_25\"<FLOAT,[batch,2,2,16,16,256]>  ::Transpose(%\"view_40\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "             630 |  # node_view_41\n",
       "                    %\"view_41\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"permute_25\", %\"val_1115\"{[-1, 16, 16, 256]}) {allowzero=1}\n",
       "             631 |  # node_MatMul_1067\n",
       "                    %\"val_1117\"<FLOAT,[4*batch,16,16,768]>  ::MatMul(%\"view_41\", %\"val_1116\"{...})\n",
       "             632 |  # node_linear_32\n",
       "                    %\"linear_32\"<FLOAT,[4*batch,16,16,768]>  ::Add(%\"val_1117\", %\"model.encoder.model.stages_2.blocks.0.attn_block.attn.qkv.bias\"{...})\n",
       "             633 |  # node_mul_1147\n",
       "                    %\"mul_1147\"<INT64,[]>  ::Mul(%\"val_6\"{4}, %\"sym_size_int_37\")\n",
       "             634 |  # node_Reshape_1069\n",
       "                    %\"val_1119\"<INT64,[1]>  ::Reshape(%\"mul_1147\", %\"val_102\"{[-1]}) {allowzero=0}\n",
       "             635 |  # node_Concat_1074\n",
       "                    %\"val_1124\"<INT64,[5]>  ::Concat(%\"val_1119\", %\"val_102\"{[-1]}, %\"val_105\"{[3]}, %\"val_87\"{[8]}, %\"val_107\"{[32]}) {axis=0}\n",
       "             636 |  # node_view_42\n",
       "                    %\"view_42\"<FLOAT,[4*batch,256,3,8,32]>  ::Reshape(%\"linear_32\", %\"val_1124\") {allowzero=1}\n",
       "             637 |  # node_transpose_16\n",
       "                    %\"transpose_16\"<FLOAT,[4*batch,8,3,256,32]>  ::Transpose(%\"view_42\") {perm=(0, 3, 2, 1, 4)}\n",
       "             638 |  # node_Split_1075\n",
       "                    %\"val_1125\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1126\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1127\"<FLOAT,[4*batch,8,1,256,32]>  ::Split(%\"transpose_16\") {num_outputs=3, axis=2}\n",
       "             639 |  # node_unbind_8__0\n",
       "                    %\"getitem_72\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1125\", %\"val_106\"{[2]})\n",
       "             640 |  # node_unbind_8__1\n",
       "                    %\"getitem_73\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1126\", %\"val_106\"{[2]})\n",
       "             641 |  # node_unbind_8__2\n",
       "                    %\"getitem_74\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1127\", %\"val_106\"{[2]})\n",
       "             642 |  # node_einsum_17\n",
       "                    %\"einsum_17\"<FLOAT,[8,16,16,16,16]>  ::Einsum(%\"einsum_16\"{...}, %\"model.encoder.model.stages_2.blocks.0.attn_block.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "             643 |  # node__unsafe_view_16\n",
       "                    %\"_unsafe_view_16\"<FLOAT,[8,256,256]>  ::Reshape(%\"einsum_17\", %\"val_1132\"{[8, 256, 256]}) {allowzero=1}\n",
       "             644 |  # node_Shape_1090\n",
       "                    %\"val_1142\"<INT64,[4]>  ::Shape(%\"getitem_73\") {start=0}\n",
       "             645 |  # node_Slice_1092\n",
       "                    %\"val_1144\"<INT64,[1]>  ::Slice(%\"val_1142\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "             646 |  # node_Slice_1093\n",
       "                    %\"val_1145\"<INT64,[1]>  ::Slice(%\"val_1142\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "             647 |  # node_Slice_1095\n",
       "                    %\"val_1147\"<INT64,[2]>  ::Slice(%\"val_1142\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "             648 |  # node_Concat_1097\n",
       "                    %\"val_1149\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_1145\", %\"val_1144\") {axis=0}\n",
       "             649 |  # node_Reshape_1098\n",
       "                    %\"val_1150\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_73\", %\"val_1149\") {allowzero=0}\n",
       "             650 |  # node_Transpose_1099\n",
       "                    %\"val_1151\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_1150\") {perm=(0, 2, 1)}\n",
       "             651 |  # node_Concat_1100\n",
       "                    %\"val_1152\"<INT64,[4]>  ::Concat(%\"val_1147\", %\"val_1144\", %\"val_1145\") {axis=0}\n",
       "             652 |  # node_Reshape_1101\n",
       "                    %\"val_1153\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_1151\", %\"val_1152\") {allowzero=0}\n",
       "             653 |  # node_Mul_1103\n",
       "                    %\"val_1155\"<FLOAT,[4*batch,8,256,32]>  ::Mul(%\"getitem_72\", %\"val_141\"{[0.4204482138156891]})\n",
       "             654 |  # node_Mul_1105\n",
       "                    %\"val_1157\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_1153\", %\"val_141\"{[0.4204482138156891]})\n",
       "             655 |  # node_MatMul_1106\n",
       "                    %\"val_1158\"<FLOAT,[None,8,256,None]>  ::MatMul(%\"val_1155\", %\"val_1157\")\n",
       "             656 |  # node_Add_1107\n",
       "                    %\"val_1159\"<FLOAT,[None,8,256,256]>  ::Add(%\"val_1158\", %\"_unsafe_view_16\")\n",
       "             657 |  # node_Softmax_1108\n",
       "                    %\"val_1160\"<FLOAT,[None,8,256,256]>  ::Softmax(%\"val_1159\") {axis=-1}\n",
       "             658 |  # node_scaled_dot_product_attention_8\n",
       "                    %\"scaled_dot_product_attention_8\"<FLOAT,[4*batch,8,256,32]>  ::MatMul(%\"val_1160\", %\"getitem_74\")\n",
       "             659 |  # node_transpose_17\n",
       "                    %\"transpose_17\"<FLOAT,[4*batch,256,8,32]>  ::Transpose(%\"scaled_dot_product_attention_8\") {perm=(0, 2, 1, 3)}\n",
       "             660 |  # node_Concat_1114\n",
       "                    %\"val_1166\"<INT64,[4]>  ::Concat(%\"val_1119\", %\"val_88\"{[16]}, %\"val_88\"{[16]}, %\"val_1108\"{[256]}) {axis=0}\n",
       "             661 |  # node__unsafe_view_17\n",
       "                    %\"_unsafe_view_17\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"transpose_17\", %\"val_1166\") {allowzero=1}\n",
       "             662 |  # node_MatMul_1116\n",
       "                    %\"val_1168\"<FLOAT,[4*batch,16,16,256]>  ::MatMul(%\"_unsafe_view_17\", %\"val_1167\"{...})\n",
       "             663 |  # node_linear_33\n",
       "                    %\"linear_33\"<FLOAT,[4*batch,16,16,256]>  ::Add(%\"val_1168\", %\"model.encoder.model.stages_2.blocks.0.attn_block.attn.proj.bias\"{...})\n",
       "             664 |  # node_view_43\n",
       "                    %\"view_43\"<FLOAT,[batch,2,2,16,16,256]>  ::Reshape(%\"linear_33\", %\"val_1176\"{[-1, 2, 2, 16, 16, 256]}) {allowzero=1}\n",
       "             665 |  # node_permute_26\n",
       "                    %\"permute_26\"<FLOAT,[batch,2,16,2,16,256]>  ::Transpose(%\"view_43\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "             666 |  # node_view_44\n",
       "                    %\"view_44\"<FLOAT,[batch,32,32,256]>  ::Reshape(%\"permute_26\", %\"val_1182\"{[-1, 32, 32, 256]}) {allowzero=1}\n",
       "             667 |  # node_add_2106\n",
       "                    %\"add_2106\"<FLOAT,[batch,32,32,256]>  ::Add(%\"permute_24\", %\"view_44\")\n",
       "             668 |  # node_layer_norm_17\n",
       "                    %\"layer_norm_17\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_2106\", %\"model.encoder.model.stages_2.blocks.0.attn_block.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.0.attn_block.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             669 |  # node_MatMul_1132\n",
       "                    %\"val_1186\"<FLOAT,[batch,32,32,1024]>  ::MatMul(%\"layer_norm_17\", %\"val_1185\"{...})\n",
       "             670 |  # node_linear_34\n",
       "                    %\"linear_34\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1186\", %\"model.encoder.model.stages_2.blocks.0.attn_block.mlp.fc1.bias\"{...})\n",
       "             671 |  # node_Pow_1134\n",
       "                    %\"val_1188\"<FLOAT,[batch,32,32,1024]>  ::Pow(%\"linear_34\", %\"val_16\"{3.0})\n",
       "             672 |  # node_Mul_1136\n",
       "                    %\"val_1190\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1188\")\n",
       "             673 |  # node_Add_1137\n",
       "                    %\"val_1191\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"linear_34\", %\"val_1190\")\n",
       "             674 |  # node_Mul_1139\n",
       "                    %\"val_1193\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1191\")\n",
       "             675 |  # node_Tanh_1140\n",
       "                    %\"val_1194\"<FLOAT,[batch,32,32,1024]>  ::Tanh(%\"val_1193\")\n",
       "             676 |  # node_Add_1142\n",
       "                    %\"val_1196\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1194\", %\"scalar_tensor_default\"{1.0})\n",
       "             677 |  # node_Mul_1144\n",
       "                    %\"val_1198\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_26\"{0.5}, %\"val_1196\")\n",
       "             678 |  # node_gelu_19\n",
       "                    %\"gelu_19\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"linear_34\", %\"val_1198\")\n",
       "             679 |  # node_MatMul_1146\n",
       "                    %\"val_1200\"<FLOAT,[batch,32,32,256]>  ::MatMul(%\"gelu_19\", %\"val_1199\"{...})\n",
       "             680 |  # node_linear_35\n",
       "                    %\"linear_35\"<FLOAT,[batch,32,32,256]>  ::Add(%\"val_1200\", %\"model.encoder.model.stages_2.blocks.0.attn_block.mlp.fc2.bias\"{...})\n",
       "             681 |  # node_add_2142\n",
       "                    %\"add_2142\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_2106\", %\"linear_35\")\n",
       "             682 |  # node_layer_norm_18\n",
       "                    %\"layer_norm_18\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_2142\", %\"model.encoder.model.stages_2.blocks.0.attn_grid.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.0.attn_grid.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             683 |  # node_Concat_1154\n",
       "                    %\"val_1210\"<INT64,[6]>  ::Concat(%\"val_0\", %\"val_88\"{[16]}, %\"val_106\"{[2]}, %\"val_88\"{[16]}, %\"val_106\"{[2]}, %\"val_1108\"{[256]}) {axis=0}\n",
       "             684 |  # node_view_45\n",
       "                    %\"view_45\"<FLOAT,[batch,16,2,16,2,256]>  ::Reshape(%\"layer_norm_18\", %\"val_1210\") {allowzero=1}\n",
       "             685 |  # node_permute_27\n",
       "                    %\"permute_27\"<FLOAT,[batch,2,2,16,16,256]>  ::Transpose(%\"view_45\") {perm=(0, 2, 4, 1, 3, 5)}\n",
       "             686 |  # node_view_46\n",
       "                    %\"view_46\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"permute_27\", %\"val_1115\"{[-1, 16, 16, 256]}) {allowzero=1}\n",
       "             687 |  # node_MatMul_1162\n",
       "                    %\"val_1218\"<FLOAT,[4*batch,16,16,768]>  ::MatMul(%\"view_46\", %\"val_1217\"{...})\n",
       "             688 |  # node_linear_36\n",
       "                    %\"linear_36\"<FLOAT,[4*batch,16,16,768]>  ::Add(%\"val_1218\", %\"model.encoder.model.stages_2.blocks.0.attn_grid.attn.qkv.bias\"{...})\n",
       "             689 |  # node_view_47\n",
       "                    %\"view_47\"<FLOAT,[4*batch,256,3,8,32]>  ::Reshape(%\"linear_36\", %\"val_1124\") {allowzero=1}\n",
       "             690 |  # node_transpose_18\n",
       "                    %\"transpose_18\"<FLOAT,[4*batch,8,3,256,32]>  ::Transpose(%\"view_47\") {perm=(0, 3, 2, 1, 4)}\n",
       "             691 |  # node_Split_1170\n",
       "                    %\"val_1226\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1227\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1228\"<FLOAT,[4*batch,8,1,256,32]>  ::Split(%\"transpose_18\") {num_outputs=3, axis=2}\n",
       "             692 |  # node_unbind_9__0\n",
       "                    %\"getitem_75\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1226\", %\"val_106\"{[2]})\n",
       "             693 |  # node_unbind_9__1\n",
       "                    %\"getitem_76\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1227\", %\"val_106\"{[2]})\n",
       "             694 |  # node_unbind_9__2\n",
       "                    %\"getitem_77\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1228\", %\"val_106\"{[2]})\n",
       "             695 |  # node_einsum_19\n",
       "                    %\"einsum_19\"<FLOAT,[8,16,16,16,16]>  ::Einsum(%\"einsum_18\"{...}, %\"model.encoder.model.stages_2.blocks.0.attn_grid.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "             696 |  # node__unsafe_view_18\n",
       "                    %\"_unsafe_view_18\"<FLOAT,[8,256,256]>  ::Reshape(%\"einsum_19\", %\"val_1132\"{[8, 256, 256]}) {allowzero=1}\n",
       "             697 |  # node_Shape_1185\n",
       "                    %\"val_1243\"<INT64,[4]>  ::Shape(%\"getitem_76\") {start=0}\n",
       "             698 |  # node_Slice_1187\n",
       "                    %\"val_1245\"<INT64,[1]>  ::Slice(%\"val_1243\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "             699 |  # node_Slice_1188\n",
       "                    %\"val_1246\"<INT64,[1]>  ::Slice(%\"val_1243\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "             700 |  # node_Slice_1190\n",
       "                    %\"val_1248\"<INT64,[2]>  ::Slice(%\"val_1243\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "             701 |  # node_Concat_1192\n",
       "                    %\"val_1250\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_1246\", %\"val_1245\") {axis=0}\n",
       "             702 |  # node_Reshape_1193\n",
       "                    %\"val_1251\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_76\", %\"val_1250\") {allowzero=0}\n",
       "             703 |  # node_Transpose_1194\n",
       "                    %\"val_1252\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_1251\") {perm=(0, 2, 1)}\n",
       "             704 |  # node_Concat_1195\n",
       "                    %\"val_1253\"<INT64,[4]>  ::Concat(%\"val_1248\", %\"val_1245\", %\"val_1246\") {axis=0}\n",
       "             705 |  # node_Reshape_1196\n",
       "                    %\"val_1254\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_1252\", %\"val_1253\") {allowzero=0}\n",
       "             706 |  # node_Mul_1198\n",
       "                    %\"val_1256\"<FLOAT,[4*batch,8,256,32]>  ::Mul(%\"getitem_75\", %\"val_141\"{[0.4204482138156891]})\n",
       "             707 |  # node_Mul_1200\n",
       "                    %\"val_1258\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_1254\", %\"val_141\"{[0.4204482138156891]})\n",
       "             708 |  # node_MatMul_1201\n",
       "                    %\"val_1259\"<FLOAT,[None,8,256,None]>  ::MatMul(%\"val_1256\", %\"val_1258\")\n",
       "             709 |  # node_Add_1202\n",
       "                    %\"val_1260\"<FLOAT,[None,8,256,256]>  ::Add(%\"val_1259\", %\"_unsafe_view_18\")\n",
       "             710 |  # node_Softmax_1203\n",
       "                    %\"val_1261\"<FLOAT,[None,8,256,256]>  ::Softmax(%\"val_1260\") {axis=-1}\n",
       "             711 |  # node_scaled_dot_product_attention_9\n",
       "                    %\"scaled_dot_product_attention_9\"<FLOAT,[4*batch,8,256,32]>  ::MatMul(%\"val_1261\", %\"getitem_77\")\n",
       "             712 |  # node_transpose_19\n",
       "                    %\"transpose_19\"<FLOAT,[4*batch,256,8,32]>  ::Transpose(%\"scaled_dot_product_attention_9\") {perm=(0, 2, 1, 3)}\n",
       "             713 |  # node__unsafe_view_19\n",
       "                    %\"_unsafe_view_19\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"transpose_19\", %\"val_1166\") {allowzero=1}\n",
       "             714 |  # node_MatMul_1211\n",
       "                    %\"val_1269\"<FLOAT,[4*batch,16,16,256]>  ::MatMul(%\"_unsafe_view_19\", %\"val_1268\"{...})\n",
       "             715 |  # node_linear_37\n",
       "                    %\"linear_37\"<FLOAT,[4*batch,16,16,256]>  ::Add(%\"val_1269\", %\"model.encoder.model.stages_2.blocks.0.attn_grid.attn.proj.bias\"{...})\n",
       "             716 |  # node_view_48\n",
       "                    %\"view_48\"<FLOAT,[batch,2,2,16,16,256]>  ::Reshape(%\"linear_37\", %\"val_1176\"{[-1, 2, 2, 16, 16, 256]}) {allowzero=1}\n",
       "             717 |  # node_permute_28\n",
       "                    %\"permute_28\"<FLOAT,[batch,16,2,16,2,256]>  ::Transpose(%\"view_48\") {perm=(0, 3, 1, 4, 2, 5)}\n",
       "             718 |  # node_view_49\n",
       "                    %\"view_49\"<FLOAT,[batch,32,32,256]>  ::Reshape(%\"permute_28\", %\"val_1182\"{[-1, 32, 32, 256]}) {allowzero=1}\n",
       "             719 |  # node_add_2267\n",
       "                    %\"add_2267\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_2142\", %\"view_49\")\n",
       "             720 |  # node_layer_norm_19\n",
       "                    %\"layer_norm_19\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_2267\", %\"model.encoder.model.stages_2.blocks.0.attn_grid.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.0.attn_grid.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             721 |  # node_MatMul_1227\n",
       "                    %\"val_1287\"<FLOAT,[batch,32,32,1024]>  ::MatMul(%\"layer_norm_19\", %\"val_1286\"{...})\n",
       "             722 |  # node_linear_38\n",
       "                    %\"linear_38\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1287\", %\"model.encoder.model.stages_2.blocks.0.attn_grid.mlp.fc1.bias\"{...})\n",
       "             723 |  # node_Pow_1229\n",
       "                    %\"val_1289\"<FLOAT,[batch,32,32,1024]>  ::Pow(%\"linear_38\", %\"val_16\"{3.0})\n",
       "             724 |  # node_Mul_1231\n",
       "                    %\"val_1291\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1289\")\n",
       "             725 |  # node_Add_1232\n",
       "                    %\"val_1292\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"linear_38\", %\"val_1291\")\n",
       "             726 |  # node_Mul_1234\n",
       "                    %\"val_1294\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1292\")\n",
       "             727 |  # node_Tanh_1235\n",
       "                    %\"val_1295\"<FLOAT,[batch,32,32,1024]>  ::Tanh(%\"val_1294\")\n",
       "             728 |  # node_Add_1237\n",
       "                    %\"val_1297\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1295\", %\"scalar_tensor_default\"{1.0})\n",
       "             729 |  # node_Mul_1239\n",
       "                    %\"val_1299\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_26\"{0.5}, %\"val_1297\")\n",
       "             730 |  # node_gelu_20\n",
       "                    %\"gelu_20\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"linear_38\", %\"val_1299\")\n",
       "             731 |  # node_MatMul_1241\n",
       "                    %\"val_1301\"<FLOAT,[batch,32,32,256]>  ::MatMul(%\"gelu_20\", %\"val_1300\"{...})\n",
       "             732 |  # node_linear_39\n",
       "                    %\"linear_39\"<FLOAT,[batch,32,32,256]>  ::Add(%\"val_1301\", %\"model.encoder.model.stages_2.blocks.0.attn_grid.mlp.fc2.bias\"{...})\n",
       "             733 |  # node_add_2303\n",
       "                    %\"add_2303\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_2267\", %\"linear_39\")\n",
       "             734 |  # node_permute_29\n",
       "                    %\"permute_29\"<FLOAT,[batch,256,32,32]>  ::Transpose(%\"add_2303\") {perm=(0, 3, 1, 2)}\n",
       "             735 |  # node__native_batch_norm_legit_no_training_16__0\n",
       "                    %\"getitem_78\"<FLOAT,[batch,256,32,32]>  ::BatchNormalization(%\"permute_29\", %\"model.encoder.model.stages_2.blocks.1.conv.pre_norm.weight\"{...}, %\"model.encoder.model.stages_2.blocks.1.conv.pre_norm.bias\"{...}, %\"model.encoder.model.stages_2.blocks.1.conv.pre_norm.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.1.conv.pre_norm.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             736 |  # node_conv2d_29\n",
       "                    %\"conv2d_29\"<FLOAT,[batch,1024,32,32]>  ::Conv(%\"getitem_78\", %\"model.encoder.model.stages_2.blocks.1.conv.conv1_1x1.weight\"{...}, %\"val_1057\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             737 |  # node__native_batch_norm_legit_no_training_17__0\n",
       "                    %\"getitem_81\"<FLOAT,[batch,1024,32,32]>  ::BatchNormalization(%\"conv2d_29\", %\"model.encoder.model.stages_2.blocks.1.conv.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.1.conv.norm1.bias\"{...}, %\"model.encoder.model.stages_2.blocks.1.conv.norm1.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.1.conv.norm1.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             738 |  # node_Pow_1256\n",
       "                    %\"val_1316\"<FLOAT,[batch,1024,32,32]>  ::Pow(%\"getitem_81\", %\"val_16\"{3.0})\n",
       "             739 |  # node_Mul_1258\n",
       "                    %\"val_1318\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1316\")\n",
       "             740 |  # node_Add_1259\n",
       "                    %\"val_1319\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"getitem_81\", %\"val_1318\")\n",
       "             741 |  # node_Mul_1261\n",
       "                    %\"val_1321\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1319\")\n",
       "             742 |  # node_Tanh_1262\n",
       "                    %\"val_1322\"<FLOAT,[batch,1024,32,32]>  ::Tanh(%\"val_1321\")\n",
       "             743 |  # node_Add_1264\n",
       "                    %\"val_1324\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"val_1322\", %\"scalar_tensor_default\"{1.0})\n",
       "             744 |  # node_Mul_1266\n",
       "                    %\"val_1326\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_26\"{0.5}, %\"val_1324\")\n",
       "             745 |  # node_gelu_21\n",
       "                    %\"gelu_21\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"getitem_81\", %\"val_1326\")\n",
       "             746 |  # node_conv2d_30\n",
       "                    %\"conv2d_30\"<FLOAT,[batch,1024,32,32]>  ::Conv(%\"gelu_21\", %\"model.encoder.model.stages_2.blocks.1.conv.conv2_kxk.weight\"{...}, %\"val_1057\"{...}) {group=1024, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             747 |  # node__native_batch_norm_legit_no_training_18__0\n",
       "                    %\"getitem_84\"<FLOAT,[batch,1024,32,32]>  ::BatchNormalization(%\"conv2d_30\", %\"model.encoder.model.stages_2.blocks.1.conv.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.1.conv.norm2.bias\"{...}, %\"model.encoder.model.stages_2.blocks.1.conv.norm2.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.1.conv.norm2.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             748 |  # node_Pow_1277\n",
       "                    %\"val_1337\"<FLOAT,[batch,1024,32,32]>  ::Pow(%\"getitem_84\", %\"val_16\"{3.0})\n",
       "             749 |  # node_Mul_1279\n",
       "                    %\"val_1339\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1337\")\n",
       "             750 |  # node_Add_1280\n",
       "                    %\"val_1340\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"getitem_84\", %\"val_1339\")\n",
       "             751 |  # node_Mul_1282\n",
       "                    %\"val_1342\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1340\")\n",
       "             752 |  # node_Tanh_1283\n",
       "                    %\"val_1343\"<FLOAT,[batch,1024,32,32]>  ::Tanh(%\"val_1342\")\n",
       "             753 |  # node_Add_1285\n",
       "                    %\"val_1345\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"val_1343\", %\"scalar_tensor_default\"{1.0})\n",
       "             754 |  # node_Mul_1287\n",
       "                    %\"val_1347\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_26\"{0.5}, %\"val_1345\")\n",
       "             755 |  # node_gelu_22\n",
       "                    %\"gelu_22\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"getitem_84\", %\"val_1347\")\n",
       "             756 |  # node_mean_5\n",
       "                    %\"mean_5\"<FLOAT,[batch,1024,1,1]>  ::ReduceMean(%\"gelu_22\", %\"val_81\"{[2, 3]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             757 |  # node_conv2d_31\n",
       "                    %\"conv2d_31\"<FLOAT,[batch,64,1,1]>  ::Conv(%\"mean_5\", %\"model.encoder.model.stages_2.blocks.1.conv.se.fc1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.1.conv.se.fc1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             758 |  # node_Sigmoid_1290\n",
       "                    %\"val_1350\"<FLOAT,[batch,64,1,1]>  ::Sigmoid(%\"conv2d_31\")\n",
       "             759 |  # node_silu_5\n",
       "                    %\"silu_5\"<FLOAT,[batch,64,1,1]>  ::Mul(%\"conv2d_31\", %\"val_1350\")\n",
       "             760 |  # node_conv2d_32\n",
       "                    %\"conv2d_32\"<FLOAT,[batch,1024,1,1]>  ::Conv(%\"silu_5\", %\"model.encoder.model.stages_2.blocks.1.conv.se.fc2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.1.conv.se.fc2.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             761 |  # node_sigmoid_5\n",
       "                    %\"sigmoid_5\"<FLOAT,[batch,1024,1,1]>  ::Sigmoid(%\"conv2d_32\")\n",
       "             762 |  # node_mul_1365\n",
       "                    %\"mul_1365\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"gelu_22\", %\"sigmoid_5\")\n",
       "             763 |  # node_conv2d_33\n",
       "                    %\"conv2d_33\"<FLOAT,[batch,256,32,32]>  ::Conv(%\"mul_1365\", %\"model.encoder.model.stages_2.blocks.1.conv.conv3_1x1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.1.conv.conv3_1x1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             764 |  # node_add_2390\n",
       "                    %\"add_2390\"<FLOAT,[batch,256,32,32]>  ::Add(%\"conv2d_33\", %\"permute_29\")\n",
       "             765 |  # node_permute_30\n",
       "                    %\"permute_30\"<FLOAT,[batch,32,32,256]>  ::Transpose(%\"add_2390\") {perm=(0, 2, 3, 1)}\n",
       "             766 |  # node_layer_norm_20\n",
       "                    %\"layer_norm_20\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_30\", %\"model.encoder.model.stages_2.blocks.1.attn_block.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.1.attn_block.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             767 |  # node_view_50\n",
       "                    %\"view_50\"<FLOAT,[batch,2,16,2,16,256]>  ::Reshape(%\"layer_norm_20\", %\"val_1109\") {allowzero=1}\n",
       "             768 |  # node_permute_31\n",
       "                    %\"permute_31\"<FLOAT,[batch,2,2,16,16,256]>  ::Transpose(%\"view_50\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "             769 |  # node_view_51\n",
       "                    %\"view_51\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"permute_31\", %\"val_1115\"{[-1, 16, 16, 256]}) {allowzero=1}\n",
       "             770 |  # node_MatMul_1306\n",
       "                    %\"val_1368\"<FLOAT,[4*batch,16,16,768]>  ::MatMul(%\"view_51\", %\"val_1367\"{...})\n",
       "             771 |  # node_linear_40\n",
       "                    %\"linear_40\"<FLOAT,[4*batch,16,16,768]>  ::Add(%\"val_1368\", %\"model.encoder.model.stages_2.blocks.1.attn_block.attn.qkv.bias\"{...})\n",
       "             772 |  # node_view_52\n",
       "                    %\"view_52\"<FLOAT,[4*batch,256,3,8,32]>  ::Reshape(%\"linear_40\", %\"val_1124\") {allowzero=1}\n",
       "             773 |  # node_transpose_20\n",
       "                    %\"transpose_20\"<FLOAT,[4*batch,8,3,256,32]>  ::Transpose(%\"view_52\") {perm=(0, 3, 2, 1, 4)}\n",
       "             774 |  # node_Split_1314\n",
       "                    %\"val_1376\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1377\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1378\"<FLOAT,[4*batch,8,1,256,32]>  ::Split(%\"transpose_20\") {num_outputs=3, axis=2}\n",
       "             775 |  # node_unbind_10__0\n",
       "                    %\"getitem_87\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1376\", %\"val_106\"{[2]})\n",
       "             776 |  # node_unbind_10__1\n",
       "                    %\"getitem_88\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1377\", %\"val_106\"{[2]})\n",
       "             777 |  # node_unbind_10__2\n",
       "                    %\"getitem_89\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1378\", %\"val_106\"{[2]})\n",
       "             778 |  # node_einsum_21\n",
       "                    %\"einsum_21\"<FLOAT,[8,16,16,16,16]>  ::Einsum(%\"einsum_20\"{...}, %\"model.encoder.model.stages_2.blocks.1.attn_block.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "             779 |  # node__unsafe_view_20\n",
       "                    %\"_unsafe_view_20\"<FLOAT,[8,256,256]>  ::Reshape(%\"einsum_21\", %\"val_1132\"{[8, 256, 256]}) {allowzero=1}\n",
       "             780 |  # node_Shape_1329\n",
       "                    %\"val_1393\"<INT64,[4]>  ::Shape(%\"getitem_88\") {start=0}\n",
       "             781 |  # node_Slice_1331\n",
       "                    %\"val_1395\"<INT64,[1]>  ::Slice(%\"val_1393\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "             782 |  # node_Slice_1332\n",
       "                    %\"val_1396\"<INT64,[1]>  ::Slice(%\"val_1393\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "             783 |  # node_Slice_1334\n",
       "                    %\"val_1398\"<INT64,[2]>  ::Slice(%\"val_1393\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "             784 |  # node_Concat_1336\n",
       "                    %\"val_1400\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_1396\", %\"val_1395\") {axis=0}\n",
       "             785 |  # node_Reshape_1337\n",
       "                    %\"val_1401\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_88\", %\"val_1400\") {allowzero=0}\n",
       "             786 |  # node_Transpose_1338\n",
       "                    %\"val_1402\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_1401\") {perm=(0, 2, 1)}\n",
       "             787 |  # node_Concat_1339\n",
       "                    %\"val_1403\"<INT64,[4]>  ::Concat(%\"val_1398\", %\"val_1395\", %\"val_1396\") {axis=0}\n",
       "             788 |  # node_Reshape_1340\n",
       "                    %\"val_1404\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_1402\", %\"val_1403\") {allowzero=0}\n",
       "             789 |  # node_Mul_1342\n",
       "                    %\"val_1406\"<FLOAT,[4*batch,8,256,32]>  ::Mul(%\"getitem_87\", %\"val_141\"{[0.4204482138156891]})\n",
       "             790 |  # node_Mul_1344\n",
       "                    %\"val_1408\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_1404\", %\"val_141\"{[0.4204482138156891]})\n",
       "             791 |  # node_MatMul_1345\n",
       "                    %\"val_1409\"<FLOAT,[None,8,256,None]>  ::MatMul(%\"val_1406\", %\"val_1408\")\n",
       "             792 |  # node_Add_1346\n",
       "                    %\"val_1410\"<FLOAT,[None,8,256,256]>  ::Add(%\"val_1409\", %\"_unsafe_view_20\")\n",
       "             793 |  # node_Softmax_1347\n",
       "                    %\"val_1411\"<FLOAT,[None,8,256,256]>  ::Softmax(%\"val_1410\") {axis=-1}\n",
       "             794 |  # node_scaled_dot_product_attention_10\n",
       "                    %\"scaled_dot_product_attention_10\"<FLOAT,[4*batch,8,256,32]>  ::MatMul(%\"val_1411\", %\"getitem_89\")\n",
       "             795 |  # node_transpose_21\n",
       "                    %\"transpose_21\"<FLOAT,[4*batch,256,8,32]>  ::Transpose(%\"scaled_dot_product_attention_10\") {perm=(0, 2, 1, 3)}\n",
       "             796 |  # node__unsafe_view_21\n",
       "                    %\"_unsafe_view_21\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"transpose_21\", %\"val_1166\") {allowzero=1}\n",
       "             797 |  # node_MatMul_1355\n",
       "                    %\"val_1419\"<FLOAT,[4*batch,16,16,256]>  ::MatMul(%\"_unsafe_view_21\", %\"val_1418\"{...})\n",
       "             798 |  # node_linear_41\n",
       "                    %\"linear_41\"<FLOAT,[4*batch,16,16,256]>  ::Add(%\"val_1419\", %\"model.encoder.model.stages_2.blocks.1.attn_block.attn.proj.bias\"{...})\n",
       "             799 |  # node_view_53\n",
       "                    %\"view_53\"<FLOAT,[batch,2,2,16,16,256]>  ::Reshape(%\"linear_41\", %\"val_1176\"{[-1, 2, 2, 16, 16, 256]}) {allowzero=1}\n",
       "             800 |  # node_permute_32\n",
       "                    %\"permute_32\"<FLOAT,[batch,2,16,2,16,256]>  ::Transpose(%\"view_53\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "             801 |  # node_view_54\n",
       "                    %\"view_54\"<FLOAT,[batch,32,32,256]>  ::Reshape(%\"permute_32\", %\"val_1182\"{[-1, 32, 32, 256]}) {allowzero=1}\n",
       "             802 |  # node_add_2520\n",
       "                    %\"add_2520\"<FLOAT,[batch,32,32,256]>  ::Add(%\"permute_30\", %\"view_54\")\n",
       "             803 |  # node_layer_norm_21\n",
       "                    %\"layer_norm_21\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_2520\", %\"model.encoder.model.stages_2.blocks.1.attn_block.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.1.attn_block.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             804 |  # node_MatMul_1371\n",
       "                    %\"val_1437\"<FLOAT,[batch,32,32,1024]>  ::MatMul(%\"layer_norm_21\", %\"val_1436\"{...})\n",
       "             805 |  # node_linear_42\n",
       "                    %\"linear_42\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1437\", %\"model.encoder.model.stages_2.blocks.1.attn_block.mlp.fc1.bias\"{...})\n",
       "             806 |  # node_Pow_1373\n",
       "                    %\"val_1439\"<FLOAT,[batch,32,32,1024]>  ::Pow(%\"linear_42\", %\"val_16\"{3.0})\n",
       "             807 |  # node_Mul_1375\n",
       "                    %\"val_1441\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1439\")\n",
       "             808 |  # node_Add_1376\n",
       "                    %\"val_1442\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"linear_42\", %\"val_1441\")\n",
       "             809 |  # node_Mul_1378\n",
       "                    %\"val_1444\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1442\")\n",
       "             810 |  # node_Tanh_1379\n",
       "                    %\"val_1445\"<FLOAT,[batch,32,32,1024]>  ::Tanh(%\"val_1444\")\n",
       "             811 |  # node_Add_1381\n",
       "                    %\"val_1447\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1445\", %\"scalar_tensor_default\"{1.0})\n",
       "             812 |  # node_Mul_1383\n",
       "                    %\"val_1449\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_26\"{0.5}, %\"val_1447\")\n",
       "             813 |  # node_gelu_23\n",
       "                    %\"gelu_23\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"linear_42\", %\"val_1449\")\n",
       "             814 |  # node_MatMul_1385\n",
       "                    %\"val_1451\"<FLOAT,[batch,32,32,256]>  ::MatMul(%\"gelu_23\", %\"val_1450\"{...})\n",
       "             815 |  # node_linear_43\n",
       "                    %\"linear_43\"<FLOAT,[batch,32,32,256]>  ::Add(%\"val_1451\", %\"model.encoder.model.stages_2.blocks.1.attn_block.mlp.fc2.bias\"{...})\n",
       "             816 |  # node_add_2556\n",
       "                    %\"add_2556\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_2520\", %\"linear_43\")\n",
       "             817 |  # node_layer_norm_22\n",
       "                    %\"layer_norm_22\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_2556\", %\"model.encoder.model.stages_2.blocks.1.attn_grid.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.1.attn_grid.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             818 |  # node_view_55\n",
       "                    %\"view_55\"<FLOAT,[batch,16,2,16,2,256]>  ::Reshape(%\"layer_norm_22\", %\"val_1210\") {allowzero=1}\n",
       "             819 |  # node_permute_33\n",
       "                    %\"permute_33\"<FLOAT,[batch,2,2,16,16,256]>  ::Transpose(%\"view_55\") {perm=(0, 2, 4, 1, 3, 5)}\n",
       "             820 |  # node_view_56\n",
       "                    %\"view_56\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"permute_33\", %\"val_1115\"{[-1, 16, 16, 256]}) {allowzero=1}\n",
       "             821 |  # node_MatMul_1401\n",
       "                    %\"val_1469\"<FLOAT,[4*batch,16,16,768]>  ::MatMul(%\"view_56\", %\"val_1468\"{...})\n",
       "             822 |  # node_linear_44\n",
       "                    %\"linear_44\"<FLOAT,[4*batch,16,16,768]>  ::Add(%\"val_1469\", %\"model.encoder.model.stages_2.blocks.1.attn_grid.attn.qkv.bias\"{...})\n",
       "             823 |  # node_view_57\n",
       "                    %\"view_57\"<FLOAT,[4*batch,256,3,8,32]>  ::Reshape(%\"linear_44\", %\"val_1124\") {allowzero=1}\n",
       "             824 |  # node_transpose_22\n",
       "                    %\"transpose_22\"<FLOAT,[4*batch,8,3,256,32]>  ::Transpose(%\"view_57\") {perm=(0, 3, 2, 1, 4)}\n",
       "             825 |  # node_Split_1409\n",
       "                    %\"val_1477\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1478\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1479\"<FLOAT,[4*batch,8,1,256,32]>  ::Split(%\"transpose_22\") {num_outputs=3, axis=2}\n",
       "             826 |  # node_unbind_11__0\n",
       "                    %\"getitem_90\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1477\", %\"val_106\"{[2]})\n",
       "             827 |  # node_unbind_11__1\n",
       "                    %\"getitem_91\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1478\", %\"val_106\"{[2]})\n",
       "             828 |  # node_unbind_11__2\n",
       "                    %\"getitem_92\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1479\", %\"val_106\"{[2]})\n",
       "             829 |  # node_einsum_23\n",
       "                    %\"einsum_23\"<FLOAT,[8,16,16,16,16]>  ::Einsum(%\"einsum_22\"{...}, %\"model.encoder.model.stages_2.blocks.1.attn_grid.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "             830 |  # node__unsafe_view_22\n",
       "                    %\"_unsafe_view_22\"<FLOAT,[8,256,256]>  ::Reshape(%\"einsum_23\", %\"val_1132\"{[8, 256, 256]}) {allowzero=1}\n",
       "             831 |  # node_Shape_1424\n",
       "                    %\"val_1494\"<INT64,[4]>  ::Shape(%\"getitem_91\") {start=0}\n",
       "             832 |  # node_Slice_1426\n",
       "                    %\"val_1496\"<INT64,[1]>  ::Slice(%\"val_1494\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "             833 |  # node_Slice_1427\n",
       "                    %\"val_1497\"<INT64,[1]>  ::Slice(%\"val_1494\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "             834 |  # node_Slice_1429\n",
       "                    %\"val_1499\"<INT64,[2]>  ::Slice(%\"val_1494\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "             835 |  # node_Concat_1431\n",
       "                    %\"val_1501\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_1497\", %\"val_1496\") {axis=0}\n",
       "             836 |  # node_Reshape_1432\n",
       "                    %\"val_1502\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_91\", %\"val_1501\") {allowzero=0}\n",
       "             837 |  # node_Transpose_1433\n",
       "                    %\"val_1503\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_1502\") {perm=(0, 2, 1)}\n",
       "             838 |  # node_Concat_1434\n",
       "                    %\"val_1504\"<INT64,[4]>  ::Concat(%\"val_1499\", %\"val_1496\", %\"val_1497\") {axis=0}\n",
       "             839 |  # node_Reshape_1435\n",
       "                    %\"val_1505\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_1503\", %\"val_1504\") {allowzero=0}\n",
       "             840 |  # node_Mul_1437\n",
       "                    %\"val_1507\"<FLOAT,[4*batch,8,256,32]>  ::Mul(%\"getitem_90\", %\"val_141\"{[0.4204482138156891]})\n",
       "             841 |  # node_Mul_1439\n",
       "                    %\"val_1509\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_1505\", %\"val_141\"{[0.4204482138156891]})\n",
       "             842 |  # node_MatMul_1440\n",
       "                    %\"val_1510\"<FLOAT,[None,8,256,None]>  ::MatMul(%\"val_1507\", %\"val_1509\")\n",
       "             843 |  # node_Add_1441\n",
       "                    %\"val_1511\"<FLOAT,[None,8,256,256]>  ::Add(%\"val_1510\", %\"_unsafe_view_22\")\n",
       "             844 |  # node_Softmax_1442\n",
       "                    %\"val_1512\"<FLOAT,[None,8,256,256]>  ::Softmax(%\"val_1511\") {axis=-1}\n",
       "             845 |  # node_scaled_dot_product_attention_11\n",
       "                    %\"scaled_dot_product_attention_11\"<FLOAT,[4*batch,8,256,32]>  ::MatMul(%\"val_1512\", %\"getitem_92\")\n",
       "             846 |  # node_transpose_23\n",
       "                    %\"transpose_23\"<FLOAT,[4*batch,256,8,32]>  ::Transpose(%\"scaled_dot_product_attention_11\") {perm=(0, 2, 1, 3)}\n",
       "             847 |  # node__unsafe_view_23\n",
       "                    %\"_unsafe_view_23\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"transpose_23\", %\"val_1166\") {allowzero=1}\n",
       "             848 |  # node_MatMul_1450\n",
       "                    %\"val_1520\"<FLOAT,[4*batch,16,16,256]>  ::MatMul(%\"_unsafe_view_23\", %\"val_1519\"{...})\n",
       "             849 |  # node_linear_45\n",
       "                    %\"linear_45\"<FLOAT,[4*batch,16,16,256]>  ::Add(%\"val_1520\", %\"model.encoder.model.stages_2.blocks.1.attn_grid.attn.proj.bias\"{...})\n",
       "             850 |  # node_view_58\n",
       "                    %\"view_58\"<FLOAT,[batch,2,2,16,16,256]>  ::Reshape(%\"linear_45\", %\"val_1176\"{[-1, 2, 2, 16, 16, 256]}) {allowzero=1}\n",
       "             851 |  # node_permute_34\n",
       "                    %\"permute_34\"<FLOAT,[batch,16,2,16,2,256]>  ::Transpose(%\"view_58\") {perm=(0, 3, 1, 4, 2, 5)}\n",
       "             852 |  # node_view_59\n",
       "                    %\"view_59\"<FLOAT,[batch,32,32,256]>  ::Reshape(%\"permute_34\", %\"val_1182\"{[-1, 32, 32, 256]}) {allowzero=1}\n",
       "             853 |  # node_add_2681\n",
       "                    %\"add_2681\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_2556\", %\"view_59\")\n",
       "             854 |  # node_layer_norm_23\n",
       "                    %\"layer_norm_23\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_2681\", %\"model.encoder.model.stages_2.blocks.1.attn_grid.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.1.attn_grid.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             855 |  # node_MatMul_1466\n",
       "                    %\"val_1538\"<FLOAT,[batch,32,32,1024]>  ::MatMul(%\"layer_norm_23\", %\"val_1537\"{...})\n",
       "             856 |  # node_linear_46\n",
       "                    %\"linear_46\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1538\", %\"model.encoder.model.stages_2.blocks.1.attn_grid.mlp.fc1.bias\"{...})\n",
       "             857 |  # node_Pow_1468\n",
       "                    %\"val_1540\"<FLOAT,[batch,32,32,1024]>  ::Pow(%\"linear_46\", %\"val_16\"{3.0})\n",
       "             858 |  # node_Mul_1470\n",
       "                    %\"val_1542\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1540\")\n",
       "             859 |  # node_Add_1471\n",
       "                    %\"val_1543\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"linear_46\", %\"val_1542\")\n",
       "             860 |  # node_Mul_1473\n",
       "                    %\"val_1545\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1543\")\n",
       "             861 |  # node_Tanh_1474\n",
       "                    %\"val_1546\"<FLOAT,[batch,32,32,1024]>  ::Tanh(%\"val_1545\")\n",
       "             862 |  # node_Add_1476\n",
       "                    %\"val_1548\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1546\", %\"scalar_tensor_default\"{1.0})\n",
       "             863 |  # node_Mul_1478\n",
       "                    %\"val_1550\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_26\"{0.5}, %\"val_1548\")\n",
       "             864 |  # node_gelu_24\n",
       "                    %\"gelu_24\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"linear_46\", %\"val_1550\")\n",
       "             865 |  # node_MatMul_1480\n",
       "                    %\"val_1552\"<FLOAT,[batch,32,32,256]>  ::MatMul(%\"gelu_24\", %\"val_1551\"{...})\n",
       "             866 |  # node_linear_47\n",
       "                    %\"linear_47\"<FLOAT,[batch,32,32,256]>  ::Add(%\"val_1552\", %\"model.encoder.model.stages_2.blocks.1.attn_grid.mlp.fc2.bias\"{...})\n",
       "             867 |  # node_add_2717\n",
       "                    %\"add_2717\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_2681\", %\"linear_47\")\n",
       "             868 |  # node_permute_35\n",
       "                    %\"permute_35\"<FLOAT,[batch,256,32,32]>  ::Transpose(%\"add_2717\") {perm=(0, 3, 1, 2)}\n",
       "             869 |  # node__native_batch_norm_legit_no_training_19__0\n",
       "                    %\"getitem_93\"<FLOAT,[batch,256,32,32]>  ::BatchNormalization(%\"permute_35\", %\"model.encoder.model.stages_2.blocks.2.conv.pre_norm.weight\"{...}, %\"model.encoder.model.stages_2.blocks.2.conv.pre_norm.bias\"{...}, %\"model.encoder.model.stages_2.blocks.2.conv.pre_norm.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.2.conv.pre_norm.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             870 |  # node_conv2d_34\n",
       "                    %\"conv2d_34\"<FLOAT,[batch,1024,32,32]>  ::Conv(%\"getitem_93\", %\"model.encoder.model.stages_2.blocks.2.conv.conv1_1x1.weight\"{...}, %\"val_1057\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             871 |  # node__native_batch_norm_legit_no_training_20__0\n",
       "                    %\"getitem_96\"<FLOAT,[batch,1024,32,32]>  ::BatchNormalization(%\"conv2d_34\", %\"model.encoder.model.stages_2.blocks.2.conv.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.2.conv.norm1.bias\"{...}, %\"model.encoder.model.stages_2.blocks.2.conv.norm1.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.2.conv.norm1.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             872 |  # node_Pow_1495\n",
       "                    %\"val_1567\"<FLOAT,[batch,1024,32,32]>  ::Pow(%\"getitem_96\", %\"val_16\"{3.0})\n",
       "             873 |  # node_Mul_1497\n",
       "                    %\"val_1569\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1567\")\n",
       "             874 |  # node_Add_1498\n",
       "                    %\"val_1570\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"getitem_96\", %\"val_1569\")\n",
       "             875 |  # node_Mul_1500\n",
       "                    %\"val_1572\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1570\")\n",
       "             876 |  # node_Tanh_1501\n",
       "                    %\"val_1573\"<FLOAT,[batch,1024,32,32]>  ::Tanh(%\"val_1572\")\n",
       "             877 |  # node_Add_1503\n",
       "                    %\"val_1575\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"val_1573\", %\"scalar_tensor_default\"{1.0})\n",
       "             878 |  # node_Mul_1505\n",
       "                    %\"val_1577\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_26\"{0.5}, %\"val_1575\")\n",
       "             879 |  # node_gelu_25\n",
       "                    %\"gelu_25\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"getitem_96\", %\"val_1577\")\n",
       "             880 |  # node_conv2d_35\n",
       "                    %\"conv2d_35\"<FLOAT,[batch,1024,32,32]>  ::Conv(%\"gelu_25\", %\"model.encoder.model.stages_2.blocks.2.conv.conv2_kxk.weight\"{...}, %\"val_1057\"{...}) {group=1024, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             881 |  # node__native_batch_norm_legit_no_training_21__0\n",
       "                    %\"getitem_99\"<FLOAT,[batch,1024,32,32]>  ::BatchNormalization(%\"conv2d_35\", %\"model.encoder.model.stages_2.blocks.2.conv.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.2.conv.norm2.bias\"{...}, %\"model.encoder.model.stages_2.blocks.2.conv.norm2.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.2.conv.norm2.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "             882 |  # node_Pow_1516\n",
       "                    %\"val_1588\"<FLOAT,[batch,1024,32,32]>  ::Pow(%\"getitem_99\", %\"val_16\"{3.0})\n",
       "             883 |  # node_Mul_1518\n",
       "                    %\"val_1590\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1588\")\n",
       "             884 |  # node_Add_1519\n",
       "                    %\"val_1591\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"getitem_99\", %\"val_1590\")\n",
       "             885 |  # node_Mul_1521\n",
       "                    %\"val_1593\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1591\")\n",
       "             886 |  # node_Tanh_1522\n",
       "                    %\"val_1594\"<FLOAT,[batch,1024,32,32]>  ::Tanh(%\"val_1593\")\n",
       "             887 |  # node_Add_1524\n",
       "                    %\"val_1596\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"val_1594\", %\"scalar_tensor_default\"{1.0})\n",
       "             888 |  # node_Mul_1526\n",
       "                    %\"val_1598\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_26\"{0.5}, %\"val_1596\")\n",
       "             889 |  # node_gelu_26\n",
       "                    %\"gelu_26\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"getitem_99\", %\"val_1598\")\n",
       "             890 |  # node_mean_6\n",
       "                    %\"mean_6\"<FLOAT,[batch,1024,1,1]>  ::ReduceMean(%\"gelu_26\", %\"val_81\"{[2, 3]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "             891 |  # node_conv2d_36\n",
       "                    %\"conv2d_36\"<FLOAT,[batch,64,1,1]>  ::Conv(%\"mean_6\", %\"model.encoder.model.stages_2.blocks.2.conv.se.fc1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.2.conv.se.fc1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             892 |  # node_Sigmoid_1529\n",
       "                    %\"val_1601\"<FLOAT,[batch,64,1,1]>  ::Sigmoid(%\"conv2d_36\")\n",
       "             893 |  # node_silu_6\n",
       "                    %\"silu_6\"<FLOAT,[batch,64,1,1]>  ::Mul(%\"conv2d_36\", %\"val_1601\")\n",
       "             894 |  # node_conv2d_37\n",
       "                    %\"conv2d_37\"<FLOAT,[batch,1024,1,1]>  ::Conv(%\"silu_6\", %\"model.encoder.model.stages_2.blocks.2.conv.se.fc2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.2.conv.se.fc2.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             895 |  # node_sigmoid_6\n",
       "                    %\"sigmoid_6\"<FLOAT,[batch,1024,1,1]>  ::Sigmoid(%\"conv2d_37\")\n",
       "             896 |  # node_mul_1609\n",
       "                    %\"mul_1609\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"gelu_26\", %\"sigmoid_6\")\n",
       "             897 |  # node_conv2d_38\n",
       "                    %\"conv2d_38\"<FLOAT,[batch,256,32,32]>  ::Conv(%\"mul_1609\", %\"model.encoder.model.stages_2.blocks.2.conv.conv3_1x1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.2.conv.conv3_1x1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             898 |  # node_add_2804\n",
       "                    %\"add_2804\"<FLOAT,[batch,256,32,32]>  ::Add(%\"conv2d_38\", %\"permute_35\")\n",
       "             899 |  # node_permute_36\n",
       "                    %\"permute_36\"<FLOAT,[batch,32,32,256]>  ::Transpose(%\"add_2804\") {perm=(0, 2, 3, 1)}\n",
       "             900 |  # node_layer_norm_24\n",
       "                    %\"layer_norm_24\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_36\", %\"model.encoder.model.stages_2.blocks.2.attn_block.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.2.attn_block.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             901 |  # node_view_60\n",
       "                    %\"view_60\"<FLOAT,[batch,2,16,2,16,256]>  ::Reshape(%\"layer_norm_24\", %\"val_1109\") {allowzero=1}\n",
       "             902 |  # node_permute_37\n",
       "                    %\"permute_37\"<FLOAT,[batch,2,2,16,16,256]>  ::Transpose(%\"view_60\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "             903 |  # node_view_61\n",
       "                    %\"view_61\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"permute_37\", %\"val_1115\"{[-1, 16, 16, 256]}) {allowzero=1}\n",
       "             904 |  # node_MatMul_1545\n",
       "                    %\"val_1619\"<FLOAT,[4*batch,16,16,768]>  ::MatMul(%\"view_61\", %\"val_1618\"{...})\n",
       "             905 |  # node_linear_48\n",
       "                    %\"linear_48\"<FLOAT,[4*batch,16,16,768]>  ::Add(%\"val_1619\", %\"model.encoder.model.stages_2.blocks.2.attn_block.attn.qkv.bias\"{...})\n",
       "             906 |  # node_view_62\n",
       "                    %\"view_62\"<FLOAT,[4*batch,256,3,8,32]>  ::Reshape(%\"linear_48\", %\"val_1124\") {allowzero=1}\n",
       "             907 |  # node_transpose_24\n",
       "                    %\"transpose_24\"<FLOAT,[4*batch,8,3,256,32]>  ::Transpose(%\"view_62\") {perm=(0, 3, 2, 1, 4)}\n",
       "             908 |  # node_Split_1553\n",
       "                    %\"val_1627\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1628\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1629\"<FLOAT,[4*batch,8,1,256,32]>  ::Split(%\"transpose_24\") {num_outputs=3, axis=2}\n",
       "             909 |  # node_unbind_12__0\n",
       "                    %\"getitem_102\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1627\", %\"val_106\"{[2]})\n",
       "             910 |  # node_unbind_12__1\n",
       "                    %\"getitem_103\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1628\", %\"val_106\"{[2]})\n",
       "             911 |  # node_unbind_12__2\n",
       "                    %\"getitem_104\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1629\", %\"val_106\"{[2]})\n",
       "             912 |  # node_einsum_25\n",
       "                    %\"einsum_25\"<FLOAT,[8,16,16,16,16]>  ::Einsum(%\"einsum_24\"{...}, %\"model.encoder.model.stages_2.blocks.2.attn_block.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "             913 |  # node__unsafe_view_24\n",
       "                    %\"_unsafe_view_24\"<FLOAT,[8,256,256]>  ::Reshape(%\"einsum_25\", %\"val_1132\"{[8, 256, 256]}) {allowzero=1}\n",
       "             914 |  # node_Shape_1568\n",
       "                    %\"val_1644\"<INT64,[4]>  ::Shape(%\"getitem_103\") {start=0}\n",
       "             915 |  # node_Slice_1570\n",
       "                    %\"val_1646\"<INT64,[1]>  ::Slice(%\"val_1644\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "             916 |  # node_Slice_1571\n",
       "                    %\"val_1647\"<INT64,[1]>  ::Slice(%\"val_1644\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "             917 |  # node_Slice_1573\n",
       "                    %\"val_1649\"<INT64,[2]>  ::Slice(%\"val_1644\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "             918 |  # node_Concat_1575\n",
       "                    %\"val_1651\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_1647\", %\"val_1646\") {axis=0}\n",
       "             919 |  # node_Reshape_1576\n",
       "                    %\"val_1652\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_103\", %\"val_1651\") {allowzero=0}\n",
       "             920 |  # node_Transpose_1577\n",
       "                    %\"val_1653\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_1652\") {perm=(0, 2, 1)}\n",
       "             921 |  # node_Concat_1578\n",
       "                    %\"val_1654\"<INT64,[4]>  ::Concat(%\"val_1649\", %\"val_1646\", %\"val_1647\") {axis=0}\n",
       "             922 |  # node_Reshape_1579\n",
       "                    %\"val_1655\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_1653\", %\"val_1654\") {allowzero=0}\n",
       "             923 |  # node_Mul_1581\n",
       "                    %\"val_1657\"<FLOAT,[4*batch,8,256,32]>  ::Mul(%\"getitem_102\", %\"val_141\"{[0.4204482138156891]})\n",
       "             924 |  # node_Mul_1583\n",
       "                    %\"val_1659\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_1655\", %\"val_141\"{[0.4204482138156891]})\n",
       "             925 |  # node_MatMul_1584\n",
       "                    %\"val_1660\"<FLOAT,[None,8,256,None]>  ::MatMul(%\"val_1657\", %\"val_1659\")\n",
       "             926 |  # node_Add_1585\n",
       "                    %\"val_1661\"<FLOAT,[None,8,256,256]>  ::Add(%\"val_1660\", %\"_unsafe_view_24\")\n",
       "             927 |  # node_Softmax_1586\n",
       "                    %\"val_1662\"<FLOAT,[None,8,256,256]>  ::Softmax(%\"val_1661\") {axis=-1}\n",
       "             928 |  # node_scaled_dot_product_attention_12\n",
       "                    %\"scaled_dot_product_attention_12\"<FLOAT,[4*batch,8,256,32]>  ::MatMul(%\"val_1662\", %\"getitem_104\")\n",
       "             929 |  # node_transpose_25\n",
       "                    %\"transpose_25\"<FLOAT,[4*batch,256,8,32]>  ::Transpose(%\"scaled_dot_product_attention_12\") {perm=(0, 2, 1, 3)}\n",
       "             930 |  # node__unsafe_view_25\n",
       "                    %\"_unsafe_view_25\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"transpose_25\", %\"val_1166\") {allowzero=1}\n",
       "             931 |  # node_MatMul_1594\n",
       "                    %\"val_1670\"<FLOAT,[4*batch,16,16,256]>  ::MatMul(%\"_unsafe_view_25\", %\"val_1669\"{...})\n",
       "             932 |  # node_linear_49\n",
       "                    %\"linear_49\"<FLOAT,[4*batch,16,16,256]>  ::Add(%\"val_1670\", %\"model.encoder.model.stages_2.blocks.2.attn_block.attn.proj.bias\"{...})\n",
       "             933 |  # node_view_63\n",
       "                    %\"view_63\"<FLOAT,[batch,2,2,16,16,256]>  ::Reshape(%\"linear_49\", %\"val_1176\"{[-1, 2, 2, 16, 16, 256]}) {allowzero=1}\n",
       "             934 |  # node_permute_38\n",
       "                    %\"permute_38\"<FLOAT,[batch,2,16,2,16,256]>  ::Transpose(%\"view_63\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "             935 |  # node_view_64\n",
       "                    %\"view_64\"<FLOAT,[batch,32,32,256]>  ::Reshape(%\"permute_38\", %\"val_1182\"{[-1, 32, 32, 256]}) {allowzero=1}\n",
       "             936 |  # node_add_2934\n",
       "                    %\"add_2934\"<FLOAT,[batch,32,32,256]>  ::Add(%\"permute_36\", %\"view_64\")\n",
       "             937 |  # node_layer_norm_25\n",
       "                    %\"layer_norm_25\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_2934\", %\"model.encoder.model.stages_2.blocks.2.attn_block.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.2.attn_block.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             938 |  # node_MatMul_1610\n",
       "                    %\"val_1688\"<FLOAT,[batch,32,32,1024]>  ::MatMul(%\"layer_norm_25\", %\"val_1687\"{...})\n",
       "             939 |  # node_linear_50\n",
       "                    %\"linear_50\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1688\", %\"model.encoder.model.stages_2.blocks.2.attn_block.mlp.fc1.bias\"{...})\n",
       "             940 |  # node_Pow_1612\n",
       "                    %\"val_1690\"<FLOAT,[batch,32,32,1024]>  ::Pow(%\"linear_50\", %\"val_16\"{3.0})\n",
       "             941 |  # node_Mul_1614\n",
       "                    %\"val_1692\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1690\")\n",
       "             942 |  # node_Add_1615\n",
       "                    %\"val_1693\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"linear_50\", %\"val_1692\")\n",
       "             943 |  # node_Mul_1617\n",
       "                    %\"val_1695\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1693\")\n",
       "             944 |  # node_Tanh_1618\n",
       "                    %\"val_1696\"<FLOAT,[batch,32,32,1024]>  ::Tanh(%\"val_1695\")\n",
       "             945 |  # node_Add_1620\n",
       "                    %\"val_1698\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1696\", %\"scalar_tensor_default\"{1.0})\n",
       "             946 |  # node_Mul_1622\n",
       "                    %\"val_1700\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_26\"{0.5}, %\"val_1698\")\n",
       "             947 |  # node_gelu_27\n",
       "                    %\"gelu_27\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"linear_50\", %\"val_1700\")\n",
       "             948 |  # node_MatMul_1624\n",
       "                    %\"val_1702\"<FLOAT,[batch,32,32,256]>  ::MatMul(%\"gelu_27\", %\"val_1701\"{...})\n",
       "             949 |  # node_linear_51\n",
       "                    %\"linear_51\"<FLOAT,[batch,32,32,256]>  ::Add(%\"val_1702\", %\"model.encoder.model.stages_2.blocks.2.attn_block.mlp.fc2.bias\"{...})\n",
       "             950 |  # node_add_2970\n",
       "                    %\"add_2970\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_2934\", %\"linear_51\")\n",
       "             951 |  # node_layer_norm_26\n",
       "                    %\"layer_norm_26\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_2970\", %\"model.encoder.model.stages_2.blocks.2.attn_grid.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.2.attn_grid.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             952 |  # node_view_65\n",
       "                    %\"view_65\"<FLOAT,[batch,16,2,16,2,256]>  ::Reshape(%\"layer_norm_26\", %\"val_1210\") {allowzero=1}\n",
       "             953 |  # node_permute_39\n",
       "                    %\"permute_39\"<FLOAT,[batch,2,2,16,16,256]>  ::Transpose(%\"view_65\") {perm=(0, 2, 4, 1, 3, 5)}\n",
       "             954 |  # node_view_66\n",
       "                    %\"view_66\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"permute_39\", %\"val_1115\"{[-1, 16, 16, 256]}) {allowzero=1}\n",
       "             955 |  # node_MatMul_1640\n",
       "                    %\"val_1720\"<FLOAT,[4*batch,16,16,768]>  ::MatMul(%\"view_66\", %\"val_1719\"{...})\n",
       "             956 |  # node_linear_52\n",
       "                    %\"linear_52\"<FLOAT,[4*batch,16,16,768]>  ::Add(%\"val_1720\", %\"model.encoder.model.stages_2.blocks.2.attn_grid.attn.qkv.bias\"{...})\n",
       "             957 |  # node_view_67\n",
       "                    %\"view_67\"<FLOAT,[4*batch,256,3,8,32]>  ::Reshape(%\"linear_52\", %\"val_1124\") {allowzero=1}\n",
       "             958 |  # node_transpose_26\n",
       "                    %\"transpose_26\"<FLOAT,[4*batch,8,3,256,32]>  ::Transpose(%\"view_67\") {perm=(0, 3, 2, 1, 4)}\n",
       "             959 |  # node_Split_1648\n",
       "                    %\"val_1728\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1729\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1730\"<FLOAT,[4*batch,8,1,256,32]>  ::Split(%\"transpose_26\") {num_outputs=3, axis=2}\n",
       "             960 |  # node_unbind_13__0\n",
       "                    %\"getitem_105\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1728\", %\"val_106\"{[2]})\n",
       "             961 |  # node_unbind_13__1\n",
       "                    %\"getitem_106\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1729\", %\"val_106\"{[2]})\n",
       "             962 |  # node_unbind_13__2\n",
       "                    %\"getitem_107\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1730\", %\"val_106\"{[2]})\n",
       "             963 |  # node_einsum_27\n",
       "                    %\"einsum_27\"<FLOAT,[8,16,16,16,16]>  ::Einsum(%\"einsum_26\"{...}, %\"model.encoder.model.stages_2.blocks.2.attn_grid.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "             964 |  # node__unsafe_view_26\n",
       "                    %\"_unsafe_view_26\"<FLOAT,[8,256,256]>  ::Reshape(%\"einsum_27\", %\"val_1132\"{[8, 256, 256]}) {allowzero=1}\n",
       "             965 |  # node_Shape_1663\n",
       "                    %\"val_1745\"<INT64,[4]>  ::Shape(%\"getitem_106\") {start=0}\n",
       "             966 |  # node_Slice_1665\n",
       "                    %\"val_1747\"<INT64,[1]>  ::Slice(%\"val_1745\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "             967 |  # node_Slice_1666\n",
       "                    %\"val_1748\"<INT64,[1]>  ::Slice(%\"val_1745\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "             968 |  # node_Slice_1668\n",
       "                    %\"val_1750\"<INT64,[2]>  ::Slice(%\"val_1745\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "             969 |  # node_Concat_1670\n",
       "                    %\"val_1752\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_1748\", %\"val_1747\") {axis=0}\n",
       "             970 |  # node_Reshape_1671\n",
       "                    %\"val_1753\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_106\", %\"val_1752\") {allowzero=0}\n",
       "             971 |  # node_Transpose_1672\n",
       "                    %\"val_1754\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_1753\") {perm=(0, 2, 1)}\n",
       "             972 |  # node_Concat_1673\n",
       "                    %\"val_1755\"<INT64,[4]>  ::Concat(%\"val_1750\", %\"val_1747\", %\"val_1748\") {axis=0}\n",
       "             973 |  # node_Reshape_1674\n",
       "                    %\"val_1756\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_1754\", %\"val_1755\") {allowzero=0}\n",
       "             974 |  # node_Mul_1676\n",
       "                    %\"val_1758\"<FLOAT,[4*batch,8,256,32]>  ::Mul(%\"getitem_105\", %\"val_141\"{[0.4204482138156891]})\n",
       "             975 |  # node_Mul_1678\n",
       "                    %\"val_1760\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_1756\", %\"val_141\"{[0.4204482138156891]})\n",
       "             976 |  # node_MatMul_1679\n",
       "                    %\"val_1761\"<FLOAT,[None,8,256,None]>  ::MatMul(%\"val_1758\", %\"val_1760\")\n",
       "             977 |  # node_Add_1680\n",
       "                    %\"val_1762\"<FLOAT,[None,8,256,256]>  ::Add(%\"val_1761\", %\"_unsafe_view_26\")\n",
       "             978 |  # node_Softmax_1681\n",
       "                    %\"val_1763\"<FLOAT,[None,8,256,256]>  ::Softmax(%\"val_1762\") {axis=-1}\n",
       "             979 |  # node_scaled_dot_product_attention_13\n",
       "                    %\"scaled_dot_product_attention_13\"<FLOAT,[4*batch,8,256,32]>  ::MatMul(%\"val_1763\", %\"getitem_107\")\n",
       "             980 |  # node_transpose_27\n",
       "                    %\"transpose_27\"<FLOAT,[4*batch,256,8,32]>  ::Transpose(%\"scaled_dot_product_attention_13\") {perm=(0, 2, 1, 3)}\n",
       "             981 |  # node__unsafe_view_27\n",
       "                    %\"_unsafe_view_27\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"transpose_27\", %\"val_1166\") {allowzero=1}\n",
       "             982 |  # node_MatMul_1689\n",
       "                    %\"val_1771\"<FLOAT,[4*batch,16,16,256]>  ::MatMul(%\"_unsafe_view_27\", %\"val_1770\"{...})\n",
       "             983 |  # node_linear_53\n",
       "                    %\"linear_53\"<FLOAT,[4*batch,16,16,256]>  ::Add(%\"val_1771\", %\"model.encoder.model.stages_2.blocks.2.attn_grid.attn.proj.bias\"{...})\n",
       "             984 |  # node_view_68\n",
       "                    %\"view_68\"<FLOAT,[batch,2,2,16,16,256]>  ::Reshape(%\"linear_53\", %\"val_1176\"{[-1, 2, 2, 16, 16, 256]}) {allowzero=1}\n",
       "             985 |  # node_permute_40\n",
       "                    %\"permute_40\"<FLOAT,[batch,16,2,16,2,256]>  ::Transpose(%\"view_68\") {perm=(0, 3, 1, 4, 2, 5)}\n",
       "             986 |  # node_view_69\n",
       "                    %\"view_69\"<FLOAT,[batch,32,32,256]>  ::Reshape(%\"permute_40\", %\"val_1182\"{[-1, 32, 32, 256]}) {allowzero=1}\n",
       "             987 |  # node_add_3095\n",
       "                    %\"add_3095\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_2970\", %\"view_69\")\n",
       "             988 |  # node_layer_norm_27\n",
       "                    %\"layer_norm_27\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_3095\", %\"model.encoder.model.stages_2.blocks.2.attn_grid.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.2.attn_grid.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "             989 |  # node_MatMul_1705\n",
       "                    %\"val_1789\"<FLOAT,[batch,32,32,1024]>  ::MatMul(%\"layer_norm_27\", %\"val_1788\"{...})\n",
       "             990 |  # node_linear_54\n",
       "                    %\"linear_54\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1789\", %\"model.encoder.model.stages_2.blocks.2.attn_grid.mlp.fc1.bias\"{...})\n",
       "             991 |  # node_Pow_1707\n",
       "                    %\"val_1791\"<FLOAT,[batch,32,32,1024]>  ::Pow(%\"linear_54\", %\"val_16\"{3.0})\n",
       "             992 |  # node_Mul_1709\n",
       "                    %\"val_1793\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1791\")\n",
       "             993 |  # node_Add_1710\n",
       "                    %\"val_1794\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"linear_54\", %\"val_1793\")\n",
       "             994 |  # node_Mul_1712\n",
       "                    %\"val_1796\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1794\")\n",
       "             995 |  # node_Tanh_1713\n",
       "                    %\"val_1797\"<FLOAT,[batch,32,32,1024]>  ::Tanh(%\"val_1796\")\n",
       "             996 |  # node_Add_1715\n",
       "                    %\"val_1799\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1797\", %\"scalar_tensor_default\"{1.0})\n",
       "             997 |  # node_Mul_1717\n",
       "                    %\"val_1801\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_26\"{0.5}, %\"val_1799\")\n",
       "             998 |  # node_gelu_28\n",
       "                    %\"gelu_28\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"linear_54\", %\"val_1801\")\n",
       "             999 |  # node_MatMul_1719\n",
       "                    %\"val_1803\"<FLOAT,[batch,32,32,256]>  ::MatMul(%\"gelu_28\", %\"val_1802\"{...})\n",
       "            1000 |  # node_linear_55\n",
       "                    %\"linear_55\"<FLOAT,[batch,32,32,256]>  ::Add(%\"val_1803\", %\"model.encoder.model.stages_2.blocks.2.attn_grid.mlp.fc2.bias\"{...})\n",
       "            1001 |  # node_add_3131\n",
       "                    %\"add_3131\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_3095\", %\"linear_55\")\n",
       "            1002 |  # node_permute_41\n",
       "                    %\"permute_41\"<FLOAT,[batch,256,32,32]>  ::Transpose(%\"add_3131\") {perm=(0, 3, 1, 2)}\n",
       "            1003 |  # node__native_batch_norm_legit_no_training_22__0\n",
       "                    %\"getitem_108\"<FLOAT,[batch,256,32,32]>  ::BatchNormalization(%\"permute_41\", %\"model.encoder.model.stages_2.blocks.3.conv.pre_norm.weight\"{...}, %\"model.encoder.model.stages_2.blocks.3.conv.pre_norm.bias\"{...}, %\"model.encoder.model.stages_2.blocks.3.conv.pre_norm.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.3.conv.pre_norm.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "            1004 |  # node_conv2d_39\n",
       "                    %\"conv2d_39\"<FLOAT,[batch,1024,32,32]>  ::Conv(%\"getitem_108\", %\"model.encoder.model.stages_2.blocks.3.conv.conv1_1x1.weight\"{...}, %\"val_1057\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1005 |  # node__native_batch_norm_legit_no_training_23__0\n",
       "                    %\"getitem_111\"<FLOAT,[batch,1024,32,32]>  ::BatchNormalization(%\"conv2d_39\", %\"model.encoder.model.stages_2.blocks.3.conv.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.3.conv.norm1.bias\"{...}, %\"model.encoder.model.stages_2.blocks.3.conv.norm1.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.3.conv.norm1.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "            1006 |  # node_Pow_1734\n",
       "                    %\"val_1818\"<FLOAT,[batch,1024,32,32]>  ::Pow(%\"getitem_111\", %\"val_16\"{3.0})\n",
       "            1007 |  # node_Mul_1736\n",
       "                    %\"val_1820\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1818\")\n",
       "            1008 |  # node_Add_1737\n",
       "                    %\"val_1821\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"getitem_111\", %\"val_1820\")\n",
       "            1009 |  # node_Mul_1739\n",
       "                    %\"val_1823\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1821\")\n",
       "            1010 |  # node_Tanh_1740\n",
       "                    %\"val_1824\"<FLOAT,[batch,1024,32,32]>  ::Tanh(%\"val_1823\")\n",
       "            1011 |  # node_Add_1742\n",
       "                    %\"val_1826\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"val_1824\", %\"scalar_tensor_default\"{1.0})\n",
       "            1012 |  # node_Mul_1744\n",
       "                    %\"val_1828\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_26\"{0.5}, %\"val_1826\")\n",
       "            1013 |  # node_gelu_29\n",
       "                    %\"gelu_29\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"getitem_111\", %\"val_1828\")\n",
       "            1014 |  # node_conv2d_40\n",
       "                    %\"conv2d_40\"<FLOAT,[batch,1024,32,32]>  ::Conv(%\"gelu_29\", %\"model.encoder.model.stages_2.blocks.3.conv.conv2_kxk.weight\"{...}, %\"val_1057\"{...}) {group=1024, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1015 |  # node__native_batch_norm_legit_no_training_24__0\n",
       "                    %\"getitem_114\"<FLOAT,[batch,1024,32,32]>  ::BatchNormalization(%\"conv2d_40\", %\"model.encoder.model.stages_2.blocks.3.conv.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.3.conv.norm2.bias\"{...}, %\"model.encoder.model.stages_2.blocks.3.conv.norm2.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.3.conv.norm2.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "            1016 |  # node_Pow_1755\n",
       "                    %\"val_1839\"<FLOAT,[batch,1024,32,32]>  ::Pow(%\"getitem_114\", %\"val_16\"{3.0})\n",
       "            1017 |  # node_Mul_1757\n",
       "                    %\"val_1841\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1839\")\n",
       "            1018 |  # node_Add_1758\n",
       "                    %\"val_1842\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"getitem_114\", %\"val_1841\")\n",
       "            1019 |  # node_Mul_1760\n",
       "                    %\"val_1844\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1842\")\n",
       "            1020 |  # node_Tanh_1761\n",
       "                    %\"val_1845\"<FLOAT,[batch,1024,32,32]>  ::Tanh(%\"val_1844\")\n",
       "            1021 |  # node_Add_1763\n",
       "                    %\"val_1847\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"val_1845\", %\"scalar_tensor_default\"{1.0})\n",
       "            1022 |  # node_Mul_1765\n",
       "                    %\"val_1849\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_26\"{0.5}, %\"val_1847\")\n",
       "            1023 |  # node_gelu_30\n",
       "                    %\"gelu_30\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"getitem_114\", %\"val_1849\")\n",
       "            1024 |  # node_mean_7\n",
       "                    %\"mean_7\"<FLOAT,[batch,1024,1,1]>  ::ReduceMean(%\"gelu_30\", %\"val_81\"{[2, 3]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "            1025 |  # node_conv2d_41\n",
       "                    %\"conv2d_41\"<FLOAT,[batch,64,1,1]>  ::Conv(%\"mean_7\", %\"model.encoder.model.stages_2.blocks.3.conv.se.fc1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.3.conv.se.fc1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1026 |  # node_Sigmoid_1768\n",
       "                    %\"val_1852\"<FLOAT,[batch,64,1,1]>  ::Sigmoid(%\"conv2d_41\")\n",
       "            1027 |  # node_silu_7\n",
       "                    %\"silu_7\"<FLOAT,[batch,64,1,1]>  ::Mul(%\"conv2d_41\", %\"val_1852\")\n",
       "            1028 |  # node_conv2d_42\n",
       "                    %\"conv2d_42\"<FLOAT,[batch,1024,1,1]>  ::Conv(%\"silu_7\", %\"model.encoder.model.stages_2.blocks.3.conv.se.fc2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.3.conv.se.fc2.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1029 |  # node_sigmoid_7\n",
       "                    %\"sigmoid_7\"<FLOAT,[batch,1024,1,1]>  ::Sigmoid(%\"conv2d_42\")\n",
       "            1030 |  # node_mul_1853\n",
       "                    %\"mul_1853\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"gelu_30\", %\"sigmoid_7\")\n",
       "            1031 |  # node_conv2d_43\n",
       "                    %\"conv2d_43\"<FLOAT,[batch,256,32,32]>  ::Conv(%\"mul_1853\", %\"model.encoder.model.stages_2.blocks.3.conv.conv3_1x1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.3.conv.conv3_1x1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1032 |  # node_add_3218\n",
       "                    %\"add_3218\"<FLOAT,[batch,256,32,32]>  ::Add(%\"conv2d_43\", %\"permute_41\")\n",
       "            1033 |  # node_permute_42\n",
       "                    %\"permute_42\"<FLOAT,[batch,32,32,256]>  ::Transpose(%\"add_3218\") {perm=(0, 2, 3, 1)}\n",
       "            1034 |  # node_layer_norm_28\n",
       "                    %\"layer_norm_28\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_42\", %\"model.encoder.model.stages_2.blocks.3.attn_block.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.3.attn_block.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1035 |  # node_view_70\n",
       "                    %\"view_70\"<FLOAT,[batch,2,16,2,16,256]>  ::Reshape(%\"layer_norm_28\", %\"val_1109\") {allowzero=1}\n",
       "            1036 |  # node_permute_43\n",
       "                    %\"permute_43\"<FLOAT,[batch,2,2,16,16,256]>  ::Transpose(%\"view_70\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "            1037 |  # node_view_71\n",
       "                    %\"view_71\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"permute_43\", %\"val_1115\"{[-1, 16, 16, 256]}) {allowzero=1}\n",
       "            1038 |  # node_MatMul_1784\n",
       "                    %\"val_1870\"<FLOAT,[4*batch,16,16,768]>  ::MatMul(%\"view_71\", %\"val_1869\"{...})\n",
       "            1039 |  # node_linear_56\n",
       "                    %\"linear_56\"<FLOAT,[4*batch,16,16,768]>  ::Add(%\"val_1870\", %\"model.encoder.model.stages_2.blocks.3.attn_block.attn.qkv.bias\"{...})\n",
       "            1040 |  # node_view_72\n",
       "                    %\"view_72\"<FLOAT,[4*batch,256,3,8,32]>  ::Reshape(%\"linear_56\", %\"val_1124\") {allowzero=1}\n",
       "            1041 |  # node_transpose_28\n",
       "                    %\"transpose_28\"<FLOAT,[4*batch,8,3,256,32]>  ::Transpose(%\"view_72\") {perm=(0, 3, 2, 1, 4)}\n",
       "            1042 |  # node_Split_1792\n",
       "                    %\"val_1878\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1879\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1880\"<FLOAT,[4*batch,8,1,256,32]>  ::Split(%\"transpose_28\") {num_outputs=3, axis=2}\n",
       "            1043 |  # node_unbind_14__0\n",
       "                    %\"getitem_117\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1878\", %\"val_106\"{[2]})\n",
       "            1044 |  # node_unbind_14__1\n",
       "                    %\"getitem_118\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1879\", %\"val_106\"{[2]})\n",
       "            1045 |  # node_unbind_14__2\n",
       "                    %\"getitem_119\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1880\", %\"val_106\"{[2]})\n",
       "            1046 |  # node_einsum_29\n",
       "                    %\"einsum_29\"<FLOAT,[8,16,16,16,16]>  ::Einsum(%\"einsum_28\"{...}, %\"model.encoder.model.stages_2.blocks.3.attn_block.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "            1047 |  # node__unsafe_view_28\n",
       "                    %\"_unsafe_view_28\"<FLOAT,[8,256,256]>  ::Reshape(%\"einsum_29\", %\"val_1132\"{[8, 256, 256]}) {allowzero=1}\n",
       "            1048 |  # node_Shape_1807\n",
       "                    %\"val_1895\"<INT64,[4]>  ::Shape(%\"getitem_118\") {start=0}\n",
       "            1049 |  # node_Slice_1809\n",
       "                    %\"val_1897\"<INT64,[1]>  ::Slice(%\"val_1895\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "            1050 |  # node_Slice_1810\n",
       "                    %\"val_1898\"<INT64,[1]>  ::Slice(%\"val_1895\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "            1051 |  # node_Slice_1812\n",
       "                    %\"val_1900\"<INT64,[2]>  ::Slice(%\"val_1895\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "            1052 |  # node_Concat_1814\n",
       "                    %\"val_1902\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_1898\", %\"val_1897\") {axis=0}\n",
       "            1053 |  # node_Reshape_1815\n",
       "                    %\"val_1903\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_118\", %\"val_1902\") {allowzero=0}\n",
       "            1054 |  # node_Transpose_1816\n",
       "                    %\"val_1904\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_1903\") {perm=(0, 2, 1)}\n",
       "            1055 |  # node_Concat_1817\n",
       "                    %\"val_1905\"<INT64,[4]>  ::Concat(%\"val_1900\", %\"val_1897\", %\"val_1898\") {axis=0}\n",
       "            1056 |  # node_Reshape_1818\n",
       "                    %\"val_1906\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_1904\", %\"val_1905\") {allowzero=0}\n",
       "            1057 |  # node_Mul_1820\n",
       "                    %\"val_1908\"<FLOAT,[4*batch,8,256,32]>  ::Mul(%\"getitem_117\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1058 |  # node_Mul_1822\n",
       "                    %\"val_1910\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_1906\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1059 |  # node_MatMul_1823\n",
       "                    %\"val_1911\"<FLOAT,[None,8,256,None]>  ::MatMul(%\"val_1908\", %\"val_1910\")\n",
       "            1060 |  # node_Add_1824\n",
       "                    %\"val_1912\"<FLOAT,[None,8,256,256]>  ::Add(%\"val_1911\", %\"_unsafe_view_28\")\n",
       "            1061 |  # node_Softmax_1825\n",
       "                    %\"val_1913\"<FLOAT,[None,8,256,256]>  ::Softmax(%\"val_1912\") {axis=-1}\n",
       "            1062 |  # node_scaled_dot_product_attention_14\n",
       "                    %\"scaled_dot_product_attention_14\"<FLOAT,[4*batch,8,256,32]>  ::MatMul(%\"val_1913\", %\"getitem_119\")\n",
       "            1063 |  # node_transpose_29\n",
       "                    %\"transpose_29\"<FLOAT,[4*batch,256,8,32]>  ::Transpose(%\"scaled_dot_product_attention_14\") {perm=(0, 2, 1, 3)}\n",
       "            1064 |  # node__unsafe_view_29\n",
       "                    %\"_unsafe_view_29\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"transpose_29\", %\"val_1166\") {allowzero=1}\n",
       "            1065 |  # node_MatMul_1833\n",
       "                    %\"val_1921\"<FLOAT,[4*batch,16,16,256]>  ::MatMul(%\"_unsafe_view_29\", %\"val_1920\"{...})\n",
       "            1066 |  # node_linear_57\n",
       "                    %\"linear_57\"<FLOAT,[4*batch,16,16,256]>  ::Add(%\"val_1921\", %\"model.encoder.model.stages_2.blocks.3.attn_block.attn.proj.bias\"{...})\n",
       "            1067 |  # node_view_73\n",
       "                    %\"view_73\"<FLOAT,[batch,2,2,16,16,256]>  ::Reshape(%\"linear_57\", %\"val_1176\"{[-1, 2, 2, 16, 16, 256]}) {allowzero=1}\n",
       "            1068 |  # node_permute_44\n",
       "                    %\"permute_44\"<FLOAT,[batch,2,16,2,16,256]>  ::Transpose(%\"view_73\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "            1069 |  # node_view_74\n",
       "                    %\"view_74\"<FLOAT,[batch,32,32,256]>  ::Reshape(%\"permute_44\", %\"val_1182\"{[-1, 32, 32, 256]}) {allowzero=1}\n",
       "            1070 |  # node_add_3348\n",
       "                    %\"add_3348\"<FLOAT,[batch,32,32,256]>  ::Add(%\"permute_42\", %\"view_74\")\n",
       "            1071 |  # node_layer_norm_29\n",
       "                    %\"layer_norm_29\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_3348\", %\"model.encoder.model.stages_2.blocks.3.attn_block.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.3.attn_block.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1072 |  # node_MatMul_1849\n",
       "                    %\"val_1939\"<FLOAT,[batch,32,32,1024]>  ::MatMul(%\"layer_norm_29\", %\"val_1938\"{...})\n",
       "            1073 |  # node_linear_58\n",
       "                    %\"linear_58\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1939\", %\"model.encoder.model.stages_2.blocks.3.attn_block.mlp.fc1.bias\"{...})\n",
       "            1074 |  # node_Pow_1851\n",
       "                    %\"val_1941\"<FLOAT,[batch,32,32,1024]>  ::Pow(%\"linear_58\", %\"val_16\"{3.0})\n",
       "            1075 |  # node_Mul_1853\n",
       "                    %\"val_1943\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_1941\")\n",
       "            1076 |  # node_Add_1854\n",
       "                    %\"val_1944\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"linear_58\", %\"val_1943\")\n",
       "            1077 |  # node_Mul_1856\n",
       "                    %\"val_1946\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_1944\")\n",
       "            1078 |  # node_Tanh_1857\n",
       "                    %\"val_1947\"<FLOAT,[batch,32,32,1024]>  ::Tanh(%\"val_1946\")\n",
       "            1079 |  # node_Add_1859\n",
       "                    %\"val_1949\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_1947\", %\"scalar_tensor_default\"{1.0})\n",
       "            1080 |  # node_Mul_1861\n",
       "                    %\"val_1951\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_26\"{0.5}, %\"val_1949\")\n",
       "            1081 |  # node_gelu_31\n",
       "                    %\"gelu_31\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"linear_58\", %\"val_1951\")\n",
       "            1082 |  # node_MatMul_1863\n",
       "                    %\"val_1953\"<FLOAT,[batch,32,32,256]>  ::MatMul(%\"gelu_31\", %\"val_1952\"{...})\n",
       "            1083 |  # node_linear_59\n",
       "                    %\"linear_59\"<FLOAT,[batch,32,32,256]>  ::Add(%\"val_1953\", %\"model.encoder.model.stages_2.blocks.3.attn_block.mlp.fc2.bias\"{...})\n",
       "            1084 |  # node_add_3384\n",
       "                    %\"add_3384\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_3348\", %\"linear_59\")\n",
       "            1085 |  # node_layer_norm_30\n",
       "                    %\"layer_norm_30\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_3384\", %\"model.encoder.model.stages_2.blocks.3.attn_grid.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.3.attn_grid.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1086 |  # node_view_75\n",
       "                    %\"view_75\"<FLOAT,[batch,16,2,16,2,256]>  ::Reshape(%\"layer_norm_30\", %\"val_1210\") {allowzero=1}\n",
       "            1087 |  # node_permute_45\n",
       "                    %\"permute_45\"<FLOAT,[batch,2,2,16,16,256]>  ::Transpose(%\"view_75\") {perm=(0, 2, 4, 1, 3, 5)}\n",
       "            1088 |  # node_view_76\n",
       "                    %\"view_76\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"permute_45\", %\"val_1115\"{[-1, 16, 16, 256]}) {allowzero=1}\n",
       "            1089 |  # node_MatMul_1879\n",
       "                    %\"val_1971\"<FLOAT,[4*batch,16,16,768]>  ::MatMul(%\"view_76\", %\"val_1970\"{...})\n",
       "            1090 |  # node_linear_60\n",
       "                    %\"linear_60\"<FLOAT,[4*batch,16,16,768]>  ::Add(%\"val_1971\", %\"model.encoder.model.stages_2.blocks.3.attn_grid.attn.qkv.bias\"{...})\n",
       "            1091 |  # node_view_77\n",
       "                    %\"view_77\"<FLOAT,[4*batch,256,3,8,32]>  ::Reshape(%\"linear_60\", %\"val_1124\") {allowzero=1}\n",
       "            1092 |  # node_transpose_30\n",
       "                    %\"transpose_30\"<FLOAT,[4*batch,8,3,256,32]>  ::Transpose(%\"view_77\") {perm=(0, 3, 2, 1, 4)}\n",
       "            1093 |  # node_Split_1887\n",
       "                    %\"val_1979\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1980\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_1981\"<FLOAT,[4*batch,8,1,256,32]>  ::Split(%\"transpose_30\") {num_outputs=3, axis=2}\n",
       "            1094 |  # node_unbind_15__0\n",
       "                    %\"getitem_120\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1979\", %\"val_106\"{[2]})\n",
       "            1095 |  # node_unbind_15__1\n",
       "                    %\"getitem_121\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1980\", %\"val_106\"{[2]})\n",
       "            1096 |  # node_unbind_15__2\n",
       "                    %\"getitem_122\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_1981\", %\"val_106\"{[2]})\n",
       "            1097 |  # node_einsum_31\n",
       "                    %\"einsum_31\"<FLOAT,[8,16,16,16,16]>  ::Einsum(%\"einsum_30\"{...}, %\"model.encoder.model.stages_2.blocks.3.attn_grid.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "            1098 |  # node__unsafe_view_30\n",
       "                    %\"_unsafe_view_30\"<FLOAT,[8,256,256]>  ::Reshape(%\"einsum_31\", %\"val_1132\"{[8, 256, 256]}) {allowzero=1}\n",
       "            1099 |  # node_Shape_1902\n",
       "                    %\"val_1996\"<INT64,[4]>  ::Shape(%\"getitem_121\") {start=0}\n",
       "            1100 |  # node_Slice_1904\n",
       "                    %\"val_1998\"<INT64,[1]>  ::Slice(%\"val_1996\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "            1101 |  # node_Slice_1905\n",
       "                    %\"val_1999\"<INT64,[1]>  ::Slice(%\"val_1996\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "            1102 |  # node_Slice_1907\n",
       "                    %\"val_2001\"<INT64,[2]>  ::Slice(%\"val_1996\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "            1103 |  # node_Concat_1909\n",
       "                    %\"val_2003\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_1999\", %\"val_1998\") {axis=0}\n",
       "            1104 |  # node_Reshape_1910\n",
       "                    %\"val_2004\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_121\", %\"val_2003\") {allowzero=0}\n",
       "            1105 |  # node_Transpose_1911\n",
       "                    %\"val_2005\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_2004\") {perm=(0, 2, 1)}\n",
       "            1106 |  # node_Concat_1912\n",
       "                    %\"val_2006\"<INT64,[4]>  ::Concat(%\"val_2001\", %\"val_1998\", %\"val_1999\") {axis=0}\n",
       "            1107 |  # node_Reshape_1913\n",
       "                    %\"val_2007\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_2005\", %\"val_2006\") {allowzero=0}\n",
       "            1108 |  # node_Mul_1915\n",
       "                    %\"val_2009\"<FLOAT,[4*batch,8,256,32]>  ::Mul(%\"getitem_120\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1109 |  # node_Mul_1917\n",
       "                    %\"val_2011\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_2007\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1110 |  # node_MatMul_1918\n",
       "                    %\"val_2012\"<FLOAT,[None,8,256,None]>  ::MatMul(%\"val_2009\", %\"val_2011\")\n",
       "            1111 |  # node_Add_1919\n",
       "                    %\"val_2013\"<FLOAT,[None,8,256,256]>  ::Add(%\"val_2012\", %\"_unsafe_view_30\")\n",
       "            1112 |  # node_Softmax_1920\n",
       "                    %\"val_2014\"<FLOAT,[None,8,256,256]>  ::Softmax(%\"val_2013\") {axis=-1}\n",
       "            1113 |  # node_scaled_dot_product_attention_15\n",
       "                    %\"scaled_dot_product_attention_15\"<FLOAT,[4*batch,8,256,32]>  ::MatMul(%\"val_2014\", %\"getitem_122\")\n",
       "            1114 |  # node_transpose_31\n",
       "                    %\"transpose_31\"<FLOAT,[4*batch,256,8,32]>  ::Transpose(%\"scaled_dot_product_attention_15\") {perm=(0, 2, 1, 3)}\n",
       "            1115 |  # node__unsafe_view_31\n",
       "                    %\"_unsafe_view_31\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"transpose_31\", %\"val_1166\") {allowzero=1}\n",
       "            1116 |  # node_MatMul_1928\n",
       "                    %\"val_2022\"<FLOAT,[4*batch,16,16,256]>  ::MatMul(%\"_unsafe_view_31\", %\"val_2021\"{...})\n",
       "            1117 |  # node_linear_61\n",
       "                    %\"linear_61\"<FLOAT,[4*batch,16,16,256]>  ::Add(%\"val_2022\", %\"model.encoder.model.stages_2.blocks.3.attn_grid.attn.proj.bias\"{...})\n",
       "            1118 |  # node_view_78\n",
       "                    %\"view_78\"<FLOAT,[batch,2,2,16,16,256]>  ::Reshape(%\"linear_61\", %\"val_1176\"{[-1, 2, 2, 16, 16, 256]}) {allowzero=1}\n",
       "            1119 |  # node_permute_46\n",
       "                    %\"permute_46\"<FLOAT,[batch,16,2,16,2,256]>  ::Transpose(%\"view_78\") {perm=(0, 3, 1, 4, 2, 5)}\n",
       "            1120 |  # node_view_79\n",
       "                    %\"view_79\"<FLOAT,[batch,32,32,256]>  ::Reshape(%\"permute_46\", %\"val_1182\"{[-1, 32, 32, 256]}) {allowzero=1}\n",
       "            1121 |  # node_add_3509\n",
       "                    %\"add_3509\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_3384\", %\"view_79\")\n",
       "            1122 |  # node_layer_norm_31\n",
       "                    %\"layer_norm_31\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_3509\", %\"model.encoder.model.stages_2.blocks.3.attn_grid.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.3.attn_grid.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1123 |  # node_MatMul_1944\n",
       "                    %\"val_2040\"<FLOAT,[batch,32,32,1024]>  ::MatMul(%\"layer_norm_31\", %\"val_2039\"{...})\n",
       "            1124 |  # node_linear_62\n",
       "                    %\"linear_62\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_2040\", %\"model.encoder.model.stages_2.blocks.3.attn_grid.mlp.fc1.bias\"{...})\n",
       "            1125 |  # node_Pow_1946\n",
       "                    %\"val_2042\"<FLOAT,[batch,32,32,1024]>  ::Pow(%\"linear_62\", %\"val_16\"{3.0})\n",
       "            1126 |  # node_Mul_1948\n",
       "                    %\"val_2044\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_2042\")\n",
       "            1127 |  # node_Add_1949\n",
       "                    %\"val_2045\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"linear_62\", %\"val_2044\")\n",
       "            1128 |  # node_Mul_1951\n",
       "                    %\"val_2047\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_2045\")\n",
       "            1129 |  # node_Tanh_1952\n",
       "                    %\"val_2048\"<FLOAT,[batch,32,32,1024]>  ::Tanh(%\"val_2047\")\n",
       "            1130 |  # node_Add_1954\n",
       "                    %\"val_2050\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_2048\", %\"scalar_tensor_default\"{1.0})\n",
       "            1131 |  # node_Mul_1956\n",
       "                    %\"val_2052\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_26\"{0.5}, %\"val_2050\")\n",
       "            1132 |  # node_gelu_32\n",
       "                    %\"gelu_32\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"linear_62\", %\"val_2052\")\n",
       "            1133 |  # node_MatMul_1958\n",
       "                    %\"val_2054\"<FLOAT,[batch,32,32,256]>  ::MatMul(%\"gelu_32\", %\"val_2053\"{...})\n",
       "            1134 |  # node_linear_63\n",
       "                    %\"linear_63\"<FLOAT,[batch,32,32,256]>  ::Add(%\"val_2054\", %\"model.encoder.model.stages_2.blocks.3.attn_grid.mlp.fc2.bias\"{...})\n",
       "            1135 |  # node_add_3545\n",
       "                    %\"add_3545\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_3509\", %\"linear_63\")\n",
       "            1136 |  # node_permute_47\n",
       "                    %\"permute_47\"<FLOAT,[batch,256,32,32]>  ::Transpose(%\"add_3545\") {perm=(0, 3, 1, 2)}\n",
       "            1137 |  # node__native_batch_norm_legit_no_training_25__0\n",
       "                    %\"getitem_123\"<FLOAT,[batch,256,32,32]>  ::BatchNormalization(%\"permute_47\", %\"model.encoder.model.stages_2.blocks.4.conv.pre_norm.weight\"{...}, %\"model.encoder.model.stages_2.blocks.4.conv.pre_norm.bias\"{...}, %\"model.encoder.model.stages_2.blocks.4.conv.pre_norm.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.4.conv.pre_norm.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "            1138 |  # node_conv2d_44\n",
       "                    %\"conv2d_44\"<FLOAT,[batch,1024,32,32]>  ::Conv(%\"getitem_123\", %\"model.encoder.model.stages_2.blocks.4.conv.conv1_1x1.weight\"{...}, %\"val_1057\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1139 |  # node__native_batch_norm_legit_no_training_26__0\n",
       "                    %\"getitem_126\"<FLOAT,[batch,1024,32,32]>  ::BatchNormalization(%\"conv2d_44\", %\"model.encoder.model.stages_2.blocks.4.conv.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.4.conv.norm1.bias\"{...}, %\"model.encoder.model.stages_2.blocks.4.conv.norm1.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.4.conv.norm1.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "            1140 |  # node_Pow_1973\n",
       "                    %\"val_2069\"<FLOAT,[batch,1024,32,32]>  ::Pow(%\"getitem_126\", %\"val_16\"{3.0})\n",
       "            1141 |  # node_Mul_1975\n",
       "                    %\"val_2071\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_2069\")\n",
       "            1142 |  # node_Add_1976\n",
       "                    %\"val_2072\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"getitem_126\", %\"val_2071\")\n",
       "            1143 |  # node_Mul_1978\n",
       "                    %\"val_2074\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_2072\")\n",
       "            1144 |  # node_Tanh_1979\n",
       "                    %\"val_2075\"<FLOAT,[batch,1024,32,32]>  ::Tanh(%\"val_2074\")\n",
       "            1145 |  # node_Add_1981\n",
       "                    %\"val_2077\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"val_2075\", %\"scalar_tensor_default\"{1.0})\n",
       "            1146 |  # node_Mul_1983\n",
       "                    %\"val_2079\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_26\"{0.5}, %\"val_2077\")\n",
       "            1147 |  # node_gelu_33\n",
       "                    %\"gelu_33\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"getitem_126\", %\"val_2079\")\n",
       "            1148 |  # node_conv2d_45\n",
       "                    %\"conv2d_45\"<FLOAT,[batch,1024,32,32]>  ::Conv(%\"gelu_33\", %\"model.encoder.model.stages_2.blocks.4.conv.conv2_kxk.weight\"{...}, %\"val_1057\"{...}) {group=1024, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1149 |  # node__native_batch_norm_legit_no_training_27__0\n",
       "                    %\"getitem_129\"<FLOAT,[batch,1024,32,32]>  ::BatchNormalization(%\"conv2d_45\", %\"model.encoder.model.stages_2.blocks.4.conv.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.4.conv.norm2.bias\"{...}, %\"model.encoder.model.stages_2.blocks.4.conv.norm2.running_mean\"{...}, %\"model.encoder.model.stages_2.blocks.4.conv.norm2.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "            1150 |  # node_Pow_1994\n",
       "                    %\"val_2090\"<FLOAT,[batch,1024,32,32]>  ::Pow(%\"getitem_129\", %\"val_16\"{3.0})\n",
       "            1151 |  # node_Mul_1996\n",
       "                    %\"val_2092\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_2090\")\n",
       "            1152 |  # node_Add_1997\n",
       "                    %\"val_2093\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"getitem_129\", %\"val_2092\")\n",
       "            1153 |  # node_Mul_1999\n",
       "                    %\"val_2095\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_2093\")\n",
       "            1154 |  # node_Tanh_2000\n",
       "                    %\"val_2096\"<FLOAT,[batch,1024,32,32]>  ::Tanh(%\"val_2095\")\n",
       "            1155 |  # node_Add_2002\n",
       "                    %\"val_2098\"<FLOAT,[batch,1024,32,32]>  ::Add(%\"val_2096\", %\"scalar_tensor_default\"{1.0})\n",
       "            1156 |  # node_Mul_2004\n",
       "                    %\"val_2100\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"val_26\"{0.5}, %\"val_2098\")\n",
       "            1157 |  # node_gelu_34\n",
       "                    %\"gelu_34\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"getitem_129\", %\"val_2100\")\n",
       "            1158 |  # node_mean_8\n",
       "                    %\"mean_8\"<FLOAT,[batch,1024,1,1]>  ::ReduceMean(%\"gelu_34\", %\"val_81\"{[2, 3]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "            1159 |  # node_conv2d_46\n",
       "                    %\"conv2d_46\"<FLOAT,[batch,64,1,1]>  ::Conv(%\"mean_8\", %\"model.encoder.model.stages_2.blocks.4.conv.se.fc1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.4.conv.se.fc1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1160 |  # node_Sigmoid_2007\n",
       "                    %\"val_2103\"<FLOAT,[batch,64,1,1]>  ::Sigmoid(%\"conv2d_46\")\n",
       "            1161 |  # node_silu_8\n",
       "                    %\"silu_8\"<FLOAT,[batch,64,1,1]>  ::Mul(%\"conv2d_46\", %\"val_2103\")\n",
       "            1162 |  # node_conv2d_47\n",
       "                    %\"conv2d_47\"<FLOAT,[batch,1024,1,1]>  ::Conv(%\"silu_8\", %\"model.encoder.model.stages_2.blocks.4.conv.se.fc2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.4.conv.se.fc2.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1163 |  # node_sigmoid_8\n",
       "                    %\"sigmoid_8\"<FLOAT,[batch,1024,1,1]>  ::Sigmoid(%\"conv2d_47\")\n",
       "            1164 |  # node_mul_2097\n",
       "                    %\"mul_2097\"<FLOAT,[batch,1024,32,32]>  ::Mul(%\"gelu_34\", %\"sigmoid_8\")\n",
       "            1165 |  # node_conv2d_48\n",
       "                    %\"conv2d_48\"<FLOAT,[batch,256,32,32]>  ::Conv(%\"mul_2097\", %\"model.encoder.model.stages_2.blocks.4.conv.conv3_1x1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.4.conv.conv3_1x1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1166 |  # node_add_3632\n",
       "                    %\"add_3632\"<FLOAT,[batch,256,32,32]>  ::Add(%\"conv2d_48\", %\"permute_47\")\n",
       "            1167 |  # node_permute_48\n",
       "                    %\"permute_48\"<FLOAT,[batch,32,32,256]>  ::Transpose(%\"add_3632\") {perm=(0, 2, 3, 1)}\n",
       "            1168 |  # node_layer_norm_32\n",
       "                    %\"layer_norm_32\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_48\", %\"model.encoder.model.stages_2.blocks.4.attn_block.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.4.attn_block.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1169 |  # node_view_80\n",
       "                    %\"view_80\"<FLOAT,[batch,2,16,2,16,256]>  ::Reshape(%\"layer_norm_32\", %\"val_1109\") {allowzero=1}\n",
       "            1170 |  # node_permute_49\n",
       "                    %\"permute_49\"<FLOAT,[batch,2,2,16,16,256]>  ::Transpose(%\"view_80\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "            1171 |  # node_view_81\n",
       "                    %\"view_81\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"permute_49\", %\"val_1115\"{[-1, 16, 16, 256]}) {allowzero=1}\n",
       "            1172 |  # node_MatMul_2023\n",
       "                    %\"val_2121\"<FLOAT,[4*batch,16,16,768]>  ::MatMul(%\"view_81\", %\"val_2120\"{...})\n",
       "            1173 |  # node_linear_64\n",
       "                    %\"linear_64\"<FLOAT,[4*batch,16,16,768]>  ::Add(%\"val_2121\", %\"model.encoder.model.stages_2.blocks.4.attn_block.attn.qkv.bias\"{...})\n",
       "            1174 |  # node_view_82\n",
       "                    %\"view_82\"<FLOAT,[4*batch,256,3,8,32]>  ::Reshape(%\"linear_64\", %\"val_1124\") {allowzero=1}\n",
       "            1175 |  # node_transpose_32\n",
       "                    %\"transpose_32\"<FLOAT,[4*batch,8,3,256,32]>  ::Transpose(%\"view_82\") {perm=(0, 3, 2, 1, 4)}\n",
       "            1176 |  # node_Split_2031\n",
       "                    %\"val_2129\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_2130\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_2131\"<FLOAT,[4*batch,8,1,256,32]>  ::Split(%\"transpose_32\") {num_outputs=3, axis=2}\n",
       "            1177 |  # node_unbind_16__0\n",
       "                    %\"getitem_132\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_2129\", %\"val_106\"{[2]})\n",
       "            1178 |  # node_unbind_16__1\n",
       "                    %\"getitem_133\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_2130\", %\"val_106\"{[2]})\n",
       "            1179 |  # node_unbind_16__2\n",
       "                    %\"getitem_134\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_2131\", %\"val_106\"{[2]})\n",
       "            1180 |  # node_einsum_33\n",
       "                    %\"einsum_33\"<FLOAT,[8,16,16,16,16]>  ::Einsum(%\"einsum_32\"{...}, %\"model.encoder.model.stages_2.blocks.4.attn_block.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "            1181 |  # node__unsafe_view_32\n",
       "                    %\"_unsafe_view_32\"<FLOAT,[8,256,256]>  ::Reshape(%\"einsum_33\", %\"val_1132\"{[8, 256, 256]}) {allowzero=1}\n",
       "            1182 |  # node_Shape_2046\n",
       "                    %\"val_2146\"<INT64,[4]>  ::Shape(%\"getitem_133\") {start=0}\n",
       "            1183 |  # node_Slice_2048\n",
       "                    %\"val_2148\"<INT64,[1]>  ::Slice(%\"val_2146\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "            1184 |  # node_Slice_2049\n",
       "                    %\"val_2149\"<INT64,[1]>  ::Slice(%\"val_2146\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "            1185 |  # node_Slice_2051\n",
       "                    %\"val_2151\"<INT64,[2]>  ::Slice(%\"val_2146\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "            1186 |  # node_Concat_2053\n",
       "                    %\"val_2153\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_2149\", %\"val_2148\") {axis=0}\n",
       "            1187 |  # node_Reshape_2054\n",
       "                    %\"val_2154\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_133\", %\"val_2153\") {allowzero=0}\n",
       "            1188 |  # node_Transpose_2055\n",
       "                    %\"val_2155\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_2154\") {perm=(0, 2, 1)}\n",
       "            1189 |  # node_Concat_2056\n",
       "                    %\"val_2156\"<INT64,[4]>  ::Concat(%\"val_2151\", %\"val_2148\", %\"val_2149\") {axis=0}\n",
       "            1190 |  # node_Reshape_2057\n",
       "                    %\"val_2157\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_2155\", %\"val_2156\") {allowzero=0}\n",
       "            1191 |  # node_Mul_2059\n",
       "                    %\"val_2159\"<FLOAT,[4*batch,8,256,32]>  ::Mul(%\"getitem_132\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1192 |  # node_Mul_2061\n",
       "                    %\"val_2161\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_2157\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1193 |  # node_MatMul_2062\n",
       "                    %\"val_2162\"<FLOAT,[None,8,256,None]>  ::MatMul(%\"val_2159\", %\"val_2161\")\n",
       "            1194 |  # node_Add_2063\n",
       "                    %\"val_2163\"<FLOAT,[None,8,256,256]>  ::Add(%\"val_2162\", %\"_unsafe_view_32\")\n",
       "            1195 |  # node_Softmax_2064\n",
       "                    %\"val_2164\"<FLOAT,[None,8,256,256]>  ::Softmax(%\"val_2163\") {axis=-1}\n",
       "            1196 |  # node_scaled_dot_product_attention_16\n",
       "                    %\"scaled_dot_product_attention_16\"<FLOAT,[4*batch,8,256,32]>  ::MatMul(%\"val_2164\", %\"getitem_134\")\n",
       "            1197 |  # node_transpose_33\n",
       "                    %\"transpose_33\"<FLOAT,[4*batch,256,8,32]>  ::Transpose(%\"scaled_dot_product_attention_16\") {perm=(0, 2, 1, 3)}\n",
       "            1198 |  # node__unsafe_view_33\n",
       "                    %\"_unsafe_view_33\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"transpose_33\", %\"val_1166\") {allowzero=1}\n",
       "            1199 |  # node_MatMul_2072\n",
       "                    %\"val_2172\"<FLOAT,[4*batch,16,16,256]>  ::MatMul(%\"_unsafe_view_33\", %\"val_2171\"{...})\n",
       "            1200 |  # node_linear_65\n",
       "                    %\"linear_65\"<FLOAT,[4*batch,16,16,256]>  ::Add(%\"val_2172\", %\"model.encoder.model.stages_2.blocks.4.attn_block.attn.proj.bias\"{...})\n",
       "            1201 |  # node_view_83\n",
       "                    %\"view_83\"<FLOAT,[batch,2,2,16,16,256]>  ::Reshape(%\"linear_65\", %\"val_1176\"{[-1, 2, 2, 16, 16, 256]}) {allowzero=1}\n",
       "            1202 |  # node_permute_50\n",
       "                    %\"permute_50\"<FLOAT,[batch,2,16,2,16,256]>  ::Transpose(%\"view_83\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "            1203 |  # node_view_84\n",
       "                    %\"view_84\"<FLOAT,[batch,32,32,256]>  ::Reshape(%\"permute_50\", %\"val_1182\"{[-1, 32, 32, 256]}) {allowzero=1}\n",
       "            1204 |  # node_add_3762\n",
       "                    %\"add_3762\"<FLOAT,[batch,32,32,256]>  ::Add(%\"permute_48\", %\"view_84\")\n",
       "            1205 |  # node_layer_norm_33\n",
       "                    %\"layer_norm_33\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_3762\", %\"model.encoder.model.stages_2.blocks.4.attn_block.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.4.attn_block.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1206 |  # node_MatMul_2088\n",
       "                    %\"val_2190\"<FLOAT,[batch,32,32,1024]>  ::MatMul(%\"layer_norm_33\", %\"val_2189\"{...})\n",
       "            1207 |  # node_linear_66\n",
       "                    %\"linear_66\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_2190\", %\"model.encoder.model.stages_2.blocks.4.attn_block.mlp.fc1.bias\"{...})\n",
       "            1208 |  # node_Pow_2090\n",
       "                    %\"val_2192\"<FLOAT,[batch,32,32,1024]>  ::Pow(%\"linear_66\", %\"val_16\"{3.0})\n",
       "            1209 |  # node_Mul_2092\n",
       "                    %\"val_2194\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_2192\")\n",
       "            1210 |  # node_Add_2093\n",
       "                    %\"val_2195\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"linear_66\", %\"val_2194\")\n",
       "            1211 |  # node_Mul_2095\n",
       "                    %\"val_2197\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_2195\")\n",
       "            1212 |  # node_Tanh_2096\n",
       "                    %\"val_2198\"<FLOAT,[batch,32,32,1024]>  ::Tanh(%\"val_2197\")\n",
       "            1213 |  # node_Add_2098\n",
       "                    %\"val_2200\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_2198\", %\"scalar_tensor_default\"{1.0})\n",
       "            1214 |  # node_Mul_2100\n",
       "                    %\"val_2202\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_26\"{0.5}, %\"val_2200\")\n",
       "            1215 |  # node_gelu_35\n",
       "                    %\"gelu_35\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"linear_66\", %\"val_2202\")\n",
       "            1216 |  # node_MatMul_2102\n",
       "                    %\"val_2204\"<FLOAT,[batch,32,32,256]>  ::MatMul(%\"gelu_35\", %\"val_2203\"{...})\n",
       "            1217 |  # node_linear_67\n",
       "                    %\"linear_67\"<FLOAT,[batch,32,32,256]>  ::Add(%\"val_2204\", %\"model.encoder.model.stages_2.blocks.4.attn_block.mlp.fc2.bias\"{...})\n",
       "            1218 |  # node_add_3798\n",
       "                    %\"add_3798\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_3762\", %\"linear_67\")\n",
       "            1219 |  # node_layer_norm_34\n",
       "                    %\"layer_norm_34\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_3798\", %\"model.encoder.model.stages_2.blocks.4.attn_grid.norm1.weight\"{...}, %\"model.encoder.model.stages_2.blocks.4.attn_grid.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1220 |  # node_view_85\n",
       "                    %\"view_85\"<FLOAT,[batch,16,2,16,2,256]>  ::Reshape(%\"layer_norm_34\", %\"val_1210\") {allowzero=1}\n",
       "            1221 |  # node_permute_51\n",
       "                    %\"permute_51\"<FLOAT,[batch,2,2,16,16,256]>  ::Transpose(%\"view_85\") {perm=(0, 2, 4, 1, 3, 5)}\n",
       "            1222 |  # node_view_86\n",
       "                    %\"view_86\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"permute_51\", %\"val_1115\"{[-1, 16, 16, 256]}) {allowzero=1}\n",
       "            1223 |  # node_MatMul_2118\n",
       "                    %\"val_2222\"<FLOAT,[4*batch,16,16,768]>  ::MatMul(%\"view_86\", %\"val_2221\"{...})\n",
       "            1224 |  # node_linear_68\n",
       "                    %\"linear_68\"<FLOAT,[4*batch,16,16,768]>  ::Add(%\"val_2222\", %\"model.encoder.model.stages_2.blocks.4.attn_grid.attn.qkv.bias\"{...})\n",
       "            1225 |  # node_view_87\n",
       "                    %\"view_87\"<FLOAT,[4*batch,256,3,8,32]>  ::Reshape(%\"linear_68\", %\"val_1124\") {allowzero=1}\n",
       "            1226 |  # node_transpose_34\n",
       "                    %\"transpose_34\"<FLOAT,[4*batch,8,3,256,32]>  ::Transpose(%\"view_87\") {perm=(0, 3, 2, 1, 4)}\n",
       "            1227 |  # node_Split_2126\n",
       "                    %\"val_2230\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_2231\"<FLOAT,[4*batch,8,1,256,32]>, %\"val_2232\"<FLOAT,[4*batch,8,1,256,32]>  ::Split(%\"transpose_34\") {num_outputs=3, axis=2}\n",
       "            1228 |  # node_unbind_17__0\n",
       "                    %\"getitem_135\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_2230\", %\"val_106\"{[2]})\n",
       "            1229 |  # node_unbind_17__1\n",
       "                    %\"getitem_136\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_2231\", %\"val_106\"{[2]})\n",
       "            1230 |  # node_unbind_17__2\n",
       "                    %\"getitem_137\"<FLOAT,[4*batch,8,256,32]>  ::Squeeze(%\"val_2232\", %\"val_106\"{[2]})\n",
       "            1231 |  # node_einsum_35\n",
       "                    %\"einsum_35\"<FLOAT,[8,16,16,16,16]>  ::Einsum(%\"einsum_34\"{...}, %\"model.encoder.model.stages_2.blocks.4.attn_grid.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "            1232 |  # node__unsafe_view_34\n",
       "                    %\"_unsafe_view_34\"<FLOAT,[8,256,256]>  ::Reshape(%\"einsum_35\", %\"val_1132\"{[8, 256, 256]}) {allowzero=1}\n",
       "            1233 |  # node_Shape_2141\n",
       "                    %\"val_2247\"<INT64,[4]>  ::Shape(%\"getitem_136\") {start=0}\n",
       "            1234 |  # node_Slice_2143\n",
       "                    %\"val_2249\"<INT64,[1]>  ::Slice(%\"val_2247\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "            1235 |  # node_Slice_2144\n",
       "                    %\"val_2250\"<INT64,[1]>  ::Slice(%\"val_2247\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "            1236 |  # node_Slice_2146\n",
       "                    %\"val_2252\"<INT64,[2]>  ::Slice(%\"val_2247\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "            1237 |  # node_Concat_2148\n",
       "                    %\"val_2254\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_2250\", %\"val_2249\") {axis=0}\n",
       "            1238 |  # node_Reshape_2149\n",
       "                    %\"val_2255\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_136\", %\"val_2254\") {allowzero=0}\n",
       "            1239 |  # node_Transpose_2150\n",
       "                    %\"val_2256\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_2255\") {perm=(0, 2, 1)}\n",
       "            1240 |  # node_Concat_2151\n",
       "                    %\"val_2257\"<INT64,[4]>  ::Concat(%\"val_2252\", %\"val_2249\", %\"val_2250\") {axis=0}\n",
       "            1241 |  # node_Reshape_2152\n",
       "                    %\"val_2258\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_2256\", %\"val_2257\") {allowzero=0}\n",
       "            1242 |  # node_Mul_2154\n",
       "                    %\"val_2260\"<FLOAT,[4*batch,8,256,32]>  ::Mul(%\"getitem_135\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1243 |  # node_Mul_2156\n",
       "                    %\"val_2262\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_2258\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1244 |  # node_MatMul_2157\n",
       "                    %\"val_2263\"<FLOAT,[None,8,256,None]>  ::MatMul(%\"val_2260\", %\"val_2262\")\n",
       "            1245 |  # node_Add_2158\n",
       "                    %\"val_2264\"<FLOAT,[None,8,256,256]>  ::Add(%\"val_2263\", %\"_unsafe_view_34\")\n",
       "            1246 |  # node_Softmax_2159\n",
       "                    %\"val_2265\"<FLOAT,[None,8,256,256]>  ::Softmax(%\"val_2264\") {axis=-1}\n",
       "            1247 |  # node_scaled_dot_product_attention_17\n",
       "                    %\"scaled_dot_product_attention_17\"<FLOAT,[4*batch,8,256,32]>  ::MatMul(%\"val_2265\", %\"getitem_137\")\n",
       "            1248 |  # node_transpose_35\n",
       "                    %\"transpose_35\"<FLOAT,[4*batch,256,8,32]>  ::Transpose(%\"scaled_dot_product_attention_17\") {perm=(0, 2, 1, 3)}\n",
       "            1249 |  # node__unsafe_view_35\n",
       "                    %\"_unsafe_view_35\"<FLOAT,[4*batch,16,16,256]>  ::Reshape(%\"transpose_35\", %\"val_1166\") {allowzero=1}\n",
       "            1250 |  # node_MatMul_2167\n",
       "                    %\"val_2273\"<FLOAT,[4*batch,16,16,256]>  ::MatMul(%\"_unsafe_view_35\", %\"val_2272\"{...})\n",
       "            1251 |  # node_linear_69\n",
       "                    %\"linear_69\"<FLOAT,[4*batch,16,16,256]>  ::Add(%\"val_2273\", %\"model.encoder.model.stages_2.blocks.4.attn_grid.attn.proj.bias\"{...})\n",
       "            1252 |  # node_view_88\n",
       "                    %\"view_88\"<FLOAT,[batch,2,2,16,16,256]>  ::Reshape(%\"linear_69\", %\"val_1176\"{[-1, 2, 2, 16, 16, 256]}) {allowzero=1}\n",
       "            1253 |  # node_permute_52\n",
       "                    %\"permute_52\"<FLOAT,[batch,16,2,16,2,256]>  ::Transpose(%\"view_88\") {perm=(0, 3, 1, 4, 2, 5)}\n",
       "            1254 |  # node_view_89\n",
       "                    %\"view_89\"<FLOAT,[batch,32,32,256]>  ::Reshape(%\"permute_52\", %\"val_1182\"{[-1, 32, 32, 256]}) {allowzero=1}\n",
       "            1255 |  # node_add_3923\n",
       "                    %\"add_3923\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_3798\", %\"view_89\")\n",
       "            1256 |  # node_layer_norm_35\n",
       "                    %\"layer_norm_35\"<FLOAT,[batch,32,32,256]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_3923\", %\"model.encoder.model.stages_2.blocks.4.attn_grid.norm2.weight\"{...}, %\"model.encoder.model.stages_2.blocks.4.attn_grid.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1257 |  # node_MatMul_2183\n",
       "                    %\"val_2291\"<FLOAT,[batch,32,32,1024]>  ::MatMul(%\"layer_norm_35\", %\"val_2290\"{...})\n",
       "            1258 |  # node_linear_70\n",
       "                    %\"linear_70\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_2291\", %\"model.encoder.model.stages_2.blocks.4.attn_grid.mlp.fc1.bias\"{...})\n",
       "            1259 |  # node_Pow_2185\n",
       "                    %\"val_2293\"<FLOAT,[batch,32,32,1024]>  ::Pow(%\"linear_70\", %\"val_16\"{3.0})\n",
       "            1260 |  # node_Mul_2187\n",
       "                    %\"val_2295\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_2293\")\n",
       "            1261 |  # node_Add_2188\n",
       "                    %\"val_2296\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"linear_70\", %\"val_2295\")\n",
       "            1262 |  # node_Mul_2190\n",
       "                    %\"val_2298\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_2296\")\n",
       "            1263 |  # node_Tanh_2191\n",
       "                    %\"val_2299\"<FLOAT,[batch,32,32,1024]>  ::Tanh(%\"val_2298\")\n",
       "            1264 |  # node_Add_2193\n",
       "                    %\"val_2301\"<FLOAT,[batch,32,32,1024]>  ::Add(%\"val_2299\", %\"scalar_tensor_default\"{1.0})\n",
       "            1265 |  # node_Mul_2195\n",
       "                    %\"val_2303\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"val_26\"{0.5}, %\"val_2301\")\n",
       "            1266 |  # node_gelu_36\n",
       "                    %\"gelu_36\"<FLOAT,[batch,32,32,1024]>  ::Mul(%\"linear_70\", %\"val_2303\")\n",
       "            1267 |  # node_MatMul_2197\n",
       "                    %\"val_2305\"<FLOAT,[batch,32,32,256]>  ::MatMul(%\"gelu_36\", %\"val_2304\"{...})\n",
       "            1268 |  # node_linear_71\n",
       "                    %\"linear_71\"<FLOAT,[batch,32,32,256]>  ::Add(%\"val_2305\", %\"model.encoder.model.stages_2.blocks.4.attn_grid.mlp.fc2.bias\"{...})\n",
       "            1269 |  # node_add_3959\n",
       "                    %\"add_3959\"<FLOAT,[batch,32,32,256]>  ::Add(%\"add_3923\", %\"linear_71\")\n",
       "            1270 |  # node_permute_53\n",
       "                    %\"permute_53\"<FLOAT,[batch,256,32,32]>  ::Transpose(%\"add_3959\") {perm=(0, 3, 1, 2)}\n",
       "            1271 |  # node_pad_7\n",
       "                    %\"pad_7\"<FLOAT,[batch,256,32,32]>  ::Pad(%\"permute_53\", %\"val_28\"{[0, 0, 0, 0, 0, 0, 0, 0]}, %\"val_29\"{0.0}) {mode='constant'}\n",
       "            1272 |  # node_avg_pool2d_3\n",
       "                    %\"avg_pool2d_3\"<FLOAT,[batch,256,16,16]>  ::AveragePool(%\"pad_7\") {count_include_pad=1, ceil_mode=0, pads=(0, 0, 0, 0), kernel_shape=(2, 2), strides=(2, 2), auto_pad='NOTSET'}\n",
       "            1273 |  # node_conv2d_49\n",
       "                    %\"conv2d_49\"<FLOAT,[batch,512,16,16]>  ::Conv(%\"avg_pool2d_3\", %\"model.encoder.model.stages_3.blocks.0.conv.shortcut.expand.weight\"{...}, %\"model.encoder.model.stages_3.blocks.0.conv.shortcut.expand.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1274 |  # node__native_batch_norm_legit_no_training_28__0\n",
       "                    %\"getitem_138\"<FLOAT,[batch,256,32,32]>  ::BatchNormalization(%\"permute_53\", %\"model.encoder.model.stages_3.blocks.0.conv.pre_norm.weight\"{...}, %\"model.encoder.model.stages_3.blocks.0.conv.pre_norm.bias\"{...}, %\"model.encoder.model.stages_3.blocks.0.conv.pre_norm.running_mean\"{...}, %\"model.encoder.model.stages_3.blocks.0.conv.pre_norm.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "            1275 |  # node_conv2d_50\n",
       "                    %\"conv2d_50\"<FLOAT,[batch,2048,32,32]>  ::Conv(%\"getitem_138\", %\"model.encoder.model.stages_3.blocks.0.conv.conv1_1x1.weight\"{...}, %\"val_2316\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1276 |  # node__native_batch_norm_legit_no_training_29__0\n",
       "                    %\"getitem_141\"<FLOAT,[batch,2048,32,32]>  ::BatchNormalization(%\"conv2d_50\", %\"model.encoder.model.stages_3.blocks.0.conv.norm1.weight\"{...}, %\"model.encoder.model.stages_3.blocks.0.conv.norm1.bias\"{...}, %\"model.encoder.model.stages_3.blocks.0.conv.norm1.running_mean\"{...}, %\"model.encoder.model.stages_3.blocks.0.conv.norm1.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "            1277 |  # node_Pow_2214\n",
       "                    %\"val_2322\"<FLOAT,[batch,2048,32,32]>  ::Pow(%\"getitem_141\", %\"val_16\"{3.0})\n",
       "            1278 |  # node_Mul_2216\n",
       "                    %\"val_2324\"<FLOAT,[batch,2048,32,32]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_2322\")\n",
       "            1279 |  # node_Add_2217\n",
       "                    %\"val_2325\"<FLOAT,[batch,2048,32,32]>  ::Add(%\"getitem_141\", %\"val_2324\")\n",
       "            1280 |  # node_Mul_2219\n",
       "                    %\"val_2327\"<FLOAT,[batch,2048,32,32]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_2325\")\n",
       "            1281 |  # node_Tanh_2220\n",
       "                    %\"val_2328\"<FLOAT,[batch,2048,32,32]>  ::Tanh(%\"val_2327\")\n",
       "            1282 |  # node_Add_2222\n",
       "                    %\"val_2330\"<FLOAT,[batch,2048,32,32]>  ::Add(%\"val_2328\", %\"scalar_tensor_default\"{1.0})\n",
       "            1283 |  # node_Mul_2224\n",
       "                    %\"val_2332\"<FLOAT,[batch,2048,32,32]>  ::Mul(%\"val_26\"{0.5}, %\"val_2330\")\n",
       "            1284 |  # node_gelu_37\n",
       "                    %\"gelu_37\"<FLOAT,[batch,2048,32,32]>  ::Mul(%\"getitem_141\", %\"val_2332\")\n",
       "            1285 |  # node_conv2d_51\n",
       "                    %\"conv2d_51\"<FLOAT,[batch,2048,16,16]>  ::Conv(%\"gelu_37\", %\"model.encoder.model.stages_3.blocks.0.conv.conv2_kxk.weight\"{...}, %\"val_2339\"{...}) {group=2048, pads=(0, 0, 1, 1), strides=(2, 2), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1286 |  # node__native_batch_norm_legit_no_training_30__0\n",
       "                    %\"getitem_144\"<FLOAT,[batch,2048,16,16]>  ::BatchNormalization(%\"conv2d_51\", %\"model.encoder.model.stages_3.blocks.0.conv.norm2.weight\"{...}, %\"model.encoder.model.stages_3.blocks.0.conv.norm2.bias\"{...}, %\"model.encoder.model.stages_3.blocks.0.conv.norm2.running_mean\"{...}, %\"model.encoder.model.stages_3.blocks.0.conv.norm2.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "            1287 |  # node_Pow_2237\n",
       "                    %\"val_2345\"<FLOAT,[batch,2048,16,16]>  ::Pow(%\"getitem_144\", %\"val_16\"{3.0})\n",
       "            1288 |  # node_Mul_2239\n",
       "                    %\"val_2347\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_2345\")\n",
       "            1289 |  # node_Add_2240\n",
       "                    %\"val_2348\"<FLOAT,[batch,2048,16,16]>  ::Add(%\"getitem_144\", %\"val_2347\")\n",
       "            1290 |  # node_Mul_2242\n",
       "                    %\"val_2350\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_2348\")\n",
       "            1291 |  # node_Tanh_2243\n",
       "                    %\"val_2351\"<FLOAT,[batch,2048,16,16]>  ::Tanh(%\"val_2350\")\n",
       "            1292 |  # node_Add_2245\n",
       "                    %\"val_2353\"<FLOAT,[batch,2048,16,16]>  ::Add(%\"val_2351\", %\"scalar_tensor_default\"{1.0})\n",
       "            1293 |  # node_Mul_2247\n",
       "                    %\"val_2355\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"val_26\"{0.5}, %\"val_2353\")\n",
       "            1294 |  # node_gelu_38\n",
       "                    %\"gelu_38\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"getitem_144\", %\"val_2355\")\n",
       "            1295 |  # node_mean_9\n",
       "                    %\"mean_9\"<FLOAT,[batch,2048,1,1]>  ::ReduceMean(%\"gelu_38\", %\"val_81\"{[2, 3]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "            1296 |  # node_conv2d_52\n",
       "                    %\"conv2d_52\"<FLOAT,[batch,128,1,1]>  ::Conv(%\"mean_9\", %\"model.encoder.model.stages_3.blocks.0.conv.se.fc1.weight\"{...}, %\"model.encoder.model.stages_3.blocks.0.conv.se.fc1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1297 |  # node_Sigmoid_2250\n",
       "                    %\"val_2358\"<FLOAT,[batch,128,1,1]>  ::Sigmoid(%\"conv2d_52\")\n",
       "            1298 |  # node_silu_9\n",
       "                    %\"silu_9\"<FLOAT,[batch,128,1,1]>  ::Mul(%\"conv2d_52\", %\"val_2358\")\n",
       "            1299 |  # node_conv2d_53\n",
       "                    %\"conv2d_53\"<FLOAT,[batch,2048,1,1]>  ::Conv(%\"silu_9\", %\"model.encoder.model.stages_3.blocks.0.conv.se.fc2.weight\"{...}, %\"model.encoder.model.stages_3.blocks.0.conv.se.fc2.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1300 |  # node_sigmoid_9\n",
       "                    %\"sigmoid_9\"<FLOAT,[batch,2048,1,1]>  ::Sigmoid(%\"conv2d_53\")\n",
       "            1301 |  # node_mul_2349\n",
       "                    %\"mul_2349\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"gelu_38\", %\"sigmoid_9\")\n",
       "            1302 |  # node_conv2d_54\n",
       "                    %\"conv2d_54\"<FLOAT,[batch,512,16,16]>  ::Conv(%\"mul_2349\", %\"model.encoder.model.stages_3.blocks.0.conv.conv3_1x1.weight\"{...}, %\"model.encoder.model.stages_3.blocks.0.conv.shortcut.expand.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1303 |  # node_add_4066\n",
       "                    %\"add_4066\"<FLOAT,[batch,512,16,16]>  ::Add(%\"conv2d_54\", %\"conv2d_49\")\n",
       "            1304 |  # node_permute_54\n",
       "                    %\"permute_54\"<FLOAT,[batch,16,16,512]>  ::Transpose(%\"add_4066\") {perm=(0, 2, 3, 1)}\n",
       "            1305 |  # node_layer_norm_36\n",
       "                    %\"layer_norm_36\"<FLOAT,[batch,16,16,512]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_54\", %\"model.encoder.model.stages_3.blocks.0.attn_block.norm1.weight\"{...}, %\"model.encoder.model.stages_3.blocks.0.attn_block.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1306 |  # node_Concat_2258\n",
       "                    %\"val_2368\"<INT64,[6]>  ::Concat(%\"val_0\", %\"val_2\"{[1]}, %\"val_88\"{[16]}, %\"val_2\"{[1]}, %\"val_88\"{[16]}, %\"val_2367\"{[512]}) {axis=0}\n",
       "            1307 |  # node_view_90\n",
       "                    %\"view_90\"<FLOAT,[batch,1,16,1,16,512]>  ::Reshape(%\"layer_norm_36\", %\"val_2368\") {allowzero=1}\n",
       "            1308 |  # node_permute_55\n",
       "                    %\"permute_55\"<FLOAT,[batch,1,1,16,16,512]>  ::Transpose(%\"view_90\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "            1309 |  # node_view_91\n",
       "                    %\"view_91\"<FLOAT,[batch,16,16,512]>  ::Reshape(%\"permute_55\", %\"val_2374\"{[-1, 16, 16, 512]}) {allowzero=1}\n",
       "            1310 |  # node_MatMul_2266\n",
       "                    %\"val_2376\"<FLOAT,[batch,16,16,1536]>  ::MatMul(%\"view_91\", %\"val_2375\"{...})\n",
       "            1311 |  # node_linear_72\n",
       "                    %\"linear_72\"<FLOAT,[batch,16,16,1536]>  ::Add(%\"val_2376\", %\"model.encoder.model.stages_3.blocks.0.attn_block.attn.qkv.bias\"{...})\n",
       "            1312 |  # node_Concat_2273\n",
       "                    %\"val_2383\"<INT64,[5]>  ::Concat(%\"val_0\", %\"val_102\"{[-1]}, %\"val_105\"{[3]}, %\"val_88\"{[16]}, %\"val_107\"{[32]}) {axis=0}\n",
       "            1313 |  # node_view_92\n",
       "                    %\"view_92\"<FLOAT,[batch,256,3,16,32]>  ::Reshape(%\"linear_72\", %\"val_2383\") {allowzero=1}\n",
       "            1314 |  # node_transpose_36\n",
       "                    %\"transpose_36\"<FLOAT,[batch,16,3,256,32]>  ::Transpose(%\"view_92\") {perm=(0, 3, 2, 1, 4)}\n",
       "            1315 |  # node_Split_2274\n",
       "                    %\"val_2384\"<FLOAT,[batch,16,1,256,32]>, %\"val_2385\"<FLOAT,[batch,16,1,256,32]>, %\"val_2386\"<FLOAT,[batch,16,1,256,32]>  ::Split(%\"transpose_36\") {num_outputs=3, axis=2}\n",
       "            1316 |  # node_unbind_18__0\n",
       "                    %\"getitem_147\"<FLOAT,[batch,16,256,32]>  ::Squeeze(%\"val_2384\", %\"val_106\"{[2]})\n",
       "            1317 |  # node_unbind_18__1\n",
       "                    %\"getitem_148\"<FLOAT,[batch,16,256,32]>  ::Squeeze(%\"val_2385\", %\"val_106\"{[2]})\n",
       "            1318 |  # node_unbind_18__2\n",
       "                    %\"getitem_149\"<FLOAT,[batch,16,256,32]>  ::Squeeze(%\"val_2386\", %\"val_106\"{[2]})\n",
       "            1319 |  # node_einsum_36\n",
       "                    %\"einsum_36\"<FLOAT,[16,16,16,31]>  ::Einsum(%\"model.encoder.model.stages_3.blocks.0.attn_block.attn.rel_pos.relative_position_bias_table\"{...}, %\"model.encoder.model.stages_3.blocks.0.attn_block.attn.rel_pos.height_lookup\"{...}) {equation='nhw,ixh->nixw'}\n",
       "            1320 |  # node_einsum_37\n",
       "                    %\"einsum_37\"<FLOAT,[16,16,16,16,16]>  ::Einsum(%\"einsum_36\", %\"model.encoder.model.stages_3.blocks.0.attn_block.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "            1321 |  # node__unsafe_view_36\n",
       "                    %\"_unsafe_view_36\"<FLOAT,[16,256,256]>  ::Reshape(%\"einsum_37\", %\"val_2391\"{[16, 256, 256]}) {allowzero=1}\n",
       "            1322 |  # node_Shape_2289\n",
       "                    %\"val_2401\"<INT64,[4]>  ::Shape(%\"getitem_148\") {start=0}\n",
       "            1323 |  # node_Slice_2291\n",
       "                    %\"val_2403\"<INT64,[1]>  ::Slice(%\"val_2401\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "            1324 |  # node_Slice_2292\n",
       "                    %\"val_2404\"<INT64,[1]>  ::Slice(%\"val_2401\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "            1325 |  # node_Slice_2294\n",
       "                    %\"val_2406\"<INT64,[2]>  ::Slice(%\"val_2401\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "            1326 |  # node_Concat_2296\n",
       "                    %\"val_2408\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_2404\", %\"val_2403\") {axis=0}\n",
       "            1327 |  # node_Reshape_2297\n",
       "                    %\"val_2409\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_148\", %\"val_2408\") {allowzero=0}\n",
       "            1328 |  # node_Transpose_2298\n",
       "                    %\"val_2410\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_2409\") {perm=(0, 2, 1)}\n",
       "            1329 |  # node_Concat_2299\n",
       "                    %\"val_2411\"<INT64,[4]>  ::Concat(%\"val_2406\", %\"val_2403\", %\"val_2404\") {axis=0}\n",
       "            1330 |  # node_Reshape_2300\n",
       "                    %\"val_2412\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_2410\", %\"val_2411\") {allowzero=0}\n",
       "            1331 |  # node_Mul_2302\n",
       "                    %\"val_2414\"<FLOAT,[batch,16,256,32]>  ::Mul(%\"getitem_147\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1332 |  # node_Mul_2304\n",
       "                    %\"val_2416\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_2412\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1333 |  # node_MatMul_2305\n",
       "                    %\"val_2417\"<FLOAT,[None,16,256,None]>  ::MatMul(%\"val_2414\", %\"val_2416\")\n",
       "            1334 |  # node_Add_2306\n",
       "                    %\"val_2418\"<FLOAT,[None,16,256,256]>  ::Add(%\"val_2417\", %\"_unsafe_view_36\")\n",
       "            1335 |  # node_Softmax_2307\n",
       "                    %\"val_2419\"<FLOAT,[None,16,256,256]>  ::Softmax(%\"val_2418\") {axis=-1}\n",
       "            1336 |  # node_scaled_dot_product_attention_18\n",
       "                    %\"scaled_dot_product_attention_18\"<FLOAT,[batch,16,256,32]>  ::MatMul(%\"val_2419\", %\"getitem_149\")\n",
       "            1337 |  # node_transpose_37\n",
       "                    %\"transpose_37\"<FLOAT,[batch,256,16,32]>  ::Transpose(%\"scaled_dot_product_attention_18\") {perm=(0, 2, 1, 3)}\n",
       "            1338 |  # node_Concat_2313\n",
       "                    %\"val_2425\"<INT64,[4]>  ::Concat(%\"val_0\", %\"val_88\"{[16]}, %\"val_88\"{[16]}, %\"val_2367\"{[512]}) {axis=0}\n",
       "            1339 |  # node__unsafe_view_37\n",
       "                    %\"_unsafe_view_37\"<FLOAT,[batch,16,16,512]>  ::Reshape(%\"transpose_37\", %\"val_2425\") {allowzero=1}\n",
       "            1340 |  # node_MatMul_2315\n",
       "                    %\"val_2427\"<FLOAT,[batch,16,16,512]>  ::MatMul(%\"_unsafe_view_37\", %\"val_2426\"{...})\n",
       "            1341 |  # node_linear_73\n",
       "                    %\"linear_73\"<FLOAT,[batch,16,16,512]>  ::Add(%\"val_2427\", %\"model.encoder.model.stages_3.blocks.0.attn_block.attn.proj.bias\"{...})\n",
       "            1342 |  # node_view_93\n",
       "                    %\"view_93\"<FLOAT,[batch,1,1,16,16,512]>  ::Reshape(%\"linear_73\", %\"val_2435\"{[-1, 1, 1, 16, 16, 512]}) {allowzero=1}\n",
       "            1343 |  # node_permute_56\n",
       "                    %\"permute_56\"<FLOAT,[batch,1,16,1,16,512]>  ::Transpose(%\"view_93\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "            1344 |  # node_view_94\n",
       "                    %\"view_94\"<FLOAT,[batch,16,16,512]>  ::Reshape(%\"permute_56\", %\"val_2374\"{[-1, 16, 16, 512]}) {allowzero=1}\n",
       "            1345 |  # node_add_4182\n",
       "                    %\"add_4182\"<FLOAT,[batch,16,16,512]>  ::Add(%\"permute_54\", %\"view_94\")\n",
       "            1346 |  # node_layer_norm_37\n",
       "                    %\"layer_norm_37\"<FLOAT,[batch,16,16,512]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_4182\", %\"model.encoder.model.stages_3.blocks.0.attn_block.norm2.weight\"{...}, %\"model.encoder.model.stages_3.blocks.0.attn_block.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1347 |  # node_MatMul_2331\n",
       "                    %\"val_2445\"<FLOAT,[batch,16,16,2048]>  ::MatMul(%\"layer_norm_37\", %\"val_2444\"{...})\n",
       "            1348 |  # node_linear_74\n",
       "                    %\"linear_74\"<FLOAT,[batch,16,16,2048]>  ::Add(%\"val_2445\", %\"model.encoder.model.stages_3.blocks.0.attn_block.mlp.fc1.bias\"{...})\n",
       "            1349 |  # node_Pow_2333\n",
       "                    %\"val_2447\"<FLOAT,[batch,16,16,2048]>  ::Pow(%\"linear_74\", %\"val_16\"{3.0})\n",
       "            1350 |  # node_Mul_2335\n",
       "                    %\"val_2449\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_2447\")\n",
       "            1351 |  # node_Add_2336\n",
       "                    %\"val_2450\"<FLOAT,[batch,16,16,2048]>  ::Add(%\"linear_74\", %\"val_2449\")\n",
       "            1352 |  # node_Mul_2338\n",
       "                    %\"val_2452\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_2450\")\n",
       "            1353 |  # node_Tanh_2339\n",
       "                    %\"val_2453\"<FLOAT,[batch,16,16,2048]>  ::Tanh(%\"val_2452\")\n",
       "            1354 |  # node_Add_2341\n",
       "                    %\"val_2455\"<FLOAT,[batch,16,16,2048]>  ::Add(%\"val_2453\", %\"scalar_tensor_default\"{1.0})\n",
       "            1355 |  # node_Mul_2343\n",
       "                    %\"val_2457\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"val_26\"{0.5}, %\"val_2455\")\n",
       "            1356 |  # node_gelu_39\n",
       "                    %\"gelu_39\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"linear_74\", %\"val_2457\")\n",
       "            1357 |  # node_MatMul_2345\n",
       "                    %\"val_2459\"<FLOAT,[batch,16,16,512]>  ::MatMul(%\"gelu_39\", %\"val_2458\"{...})\n",
       "            1358 |  # node_linear_75\n",
       "                    %\"linear_75\"<FLOAT,[batch,16,16,512]>  ::Add(%\"val_2459\", %\"model.encoder.model.stages_3.blocks.0.attn_block.mlp.fc2.bias\"{...})\n",
       "            1359 |  # node_add_4218\n",
       "                    %\"add_4218\"<FLOAT,[batch,16,16,512]>  ::Add(%\"add_4182\", %\"linear_75\")\n",
       "            1360 |  # node_layer_norm_38\n",
       "                    %\"layer_norm_38\"<FLOAT,[batch,16,16,512]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_4218\", %\"model.encoder.model.stages_3.blocks.0.attn_grid.norm1.weight\"{...}, %\"model.encoder.model.stages_3.blocks.0.attn_grid.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1361 |  # node_Concat_2353\n",
       "                    %\"val_2469\"<INT64,[6]>  ::Concat(%\"val_0\", %\"val_88\"{[16]}, %\"val_2\"{[1]}, %\"val_88\"{[16]}, %\"val_2\"{[1]}, %\"val_2367\"{[512]}) {axis=0}\n",
       "            1362 |  # node_view_95\n",
       "                    %\"view_95\"<FLOAT,[batch,16,1,16,1,512]>  ::Reshape(%\"layer_norm_38\", %\"val_2469\") {allowzero=1}\n",
       "            1363 |  # node_permute_57\n",
       "                    %\"permute_57\"<FLOAT,[batch,1,1,16,16,512]>  ::Transpose(%\"view_95\") {perm=(0, 2, 4, 1, 3, 5)}\n",
       "            1364 |  # node_view_96\n",
       "                    %\"view_96\"<FLOAT,[batch,16,16,512]>  ::Reshape(%\"permute_57\", %\"val_2374\"{[-1, 16, 16, 512]}) {allowzero=1}\n",
       "            1365 |  # node_MatMul_2361\n",
       "                    %\"val_2477\"<FLOAT,[batch,16,16,1536]>  ::MatMul(%\"view_96\", %\"val_2476\"{...})\n",
       "            1366 |  # node_linear_76\n",
       "                    %\"linear_76\"<FLOAT,[batch,16,16,1536]>  ::Add(%\"val_2477\", %\"model.encoder.model.stages_3.blocks.0.attn_grid.attn.qkv.bias\"{...})\n",
       "            1367 |  # node_view_97\n",
       "                    %\"view_97\"<FLOAT,[batch,256,3,16,32]>  ::Reshape(%\"linear_76\", %\"val_2383\") {allowzero=1}\n",
       "            1368 |  # node_transpose_38\n",
       "                    %\"transpose_38\"<FLOAT,[batch,16,3,256,32]>  ::Transpose(%\"view_97\") {perm=(0, 3, 2, 1, 4)}\n",
       "            1369 |  # node_Split_2369\n",
       "                    %\"val_2485\"<FLOAT,[batch,16,1,256,32]>, %\"val_2486\"<FLOAT,[batch,16,1,256,32]>, %\"val_2487\"<FLOAT,[batch,16,1,256,32]>  ::Split(%\"transpose_38\") {num_outputs=3, axis=2}\n",
       "            1370 |  # node_unbind_19__0\n",
       "                    %\"getitem_150\"<FLOAT,[batch,16,256,32]>  ::Squeeze(%\"val_2485\", %\"val_106\"{[2]})\n",
       "            1371 |  # node_unbind_19__1\n",
       "                    %\"getitem_151\"<FLOAT,[batch,16,256,32]>  ::Squeeze(%\"val_2486\", %\"val_106\"{[2]})\n",
       "            1372 |  # node_unbind_19__2\n",
       "                    %\"getitem_152\"<FLOAT,[batch,16,256,32]>  ::Squeeze(%\"val_2487\", %\"val_106\"{[2]})\n",
       "            1373 |  # node_einsum_38\n",
       "                    %\"einsum_38\"<FLOAT,[16,16,16,31]>  ::Einsum(%\"model.encoder.model.stages_3.blocks.0.attn_grid.attn.rel_pos.relative_position_bias_table\"{...}, %\"model.encoder.model.stages_3.blocks.0.attn_grid.attn.rel_pos.height_lookup\"{...}) {equation='nhw,ixh->nixw'}\n",
       "            1374 |  # node_einsum_39\n",
       "                    %\"einsum_39\"<FLOAT,[16,16,16,16,16]>  ::Einsum(%\"einsum_38\", %\"model.encoder.model.stages_3.blocks.0.attn_grid.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "            1375 |  # node__unsafe_view_38\n",
       "                    %\"_unsafe_view_38\"<FLOAT,[16,256,256]>  ::Reshape(%\"einsum_39\", %\"val_2391\"{[16, 256, 256]}) {allowzero=1}\n",
       "            1376 |  # node_Shape_2384\n",
       "                    %\"val_2502\"<INT64,[4]>  ::Shape(%\"getitem_151\") {start=0}\n",
       "            1377 |  # node_Slice_2386\n",
       "                    %\"val_2504\"<INT64,[1]>  ::Slice(%\"val_2502\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "            1378 |  # node_Slice_2387\n",
       "                    %\"val_2505\"<INT64,[1]>  ::Slice(%\"val_2502\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "            1379 |  # node_Slice_2389\n",
       "                    %\"val_2507\"<INT64,[2]>  ::Slice(%\"val_2502\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "            1380 |  # node_Concat_2391\n",
       "                    %\"val_2509\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_2505\", %\"val_2504\") {axis=0}\n",
       "            1381 |  # node_Reshape_2392\n",
       "                    %\"val_2510\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_151\", %\"val_2509\") {allowzero=0}\n",
       "            1382 |  # node_Transpose_2393\n",
       "                    %\"val_2511\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_2510\") {perm=(0, 2, 1)}\n",
       "            1383 |  # node_Concat_2394\n",
       "                    %\"val_2512\"<INT64,[4]>  ::Concat(%\"val_2507\", %\"val_2504\", %\"val_2505\") {axis=0}\n",
       "            1384 |  # node_Reshape_2395\n",
       "                    %\"val_2513\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_2511\", %\"val_2512\") {allowzero=0}\n",
       "            1385 |  # node_Mul_2397\n",
       "                    %\"val_2515\"<FLOAT,[batch,16,256,32]>  ::Mul(%\"getitem_150\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1386 |  # node_Mul_2399\n",
       "                    %\"val_2517\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_2513\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1387 |  # node_MatMul_2400\n",
       "                    %\"val_2518\"<FLOAT,[None,16,256,None]>  ::MatMul(%\"val_2515\", %\"val_2517\")\n",
       "            1388 |  # node_Add_2401\n",
       "                    %\"val_2519\"<FLOAT,[None,16,256,256]>  ::Add(%\"val_2518\", %\"_unsafe_view_38\")\n",
       "            1389 |  # node_Softmax_2402\n",
       "                    %\"val_2520\"<FLOAT,[None,16,256,256]>  ::Softmax(%\"val_2519\") {axis=-1}\n",
       "            1390 |  # node_scaled_dot_product_attention_19\n",
       "                    %\"scaled_dot_product_attention_19\"<FLOAT,[batch,16,256,32]>  ::MatMul(%\"val_2520\", %\"getitem_152\")\n",
       "            1391 |  # node_transpose_39\n",
       "                    %\"transpose_39\"<FLOAT,[batch,256,16,32]>  ::Transpose(%\"scaled_dot_product_attention_19\") {perm=(0, 2, 1, 3)}\n",
       "            1392 |  # node__unsafe_view_39\n",
       "                    %\"_unsafe_view_39\"<FLOAT,[batch,16,16,512]>  ::Reshape(%\"transpose_39\", %\"val_2425\") {allowzero=1}\n",
       "            1393 |  # node_MatMul_2410\n",
       "                    %\"val_2528\"<FLOAT,[batch,16,16,512]>  ::MatMul(%\"_unsafe_view_39\", %\"val_2527\"{...})\n",
       "            1394 |  # node_linear_77\n",
       "                    %\"linear_77\"<FLOAT,[batch,16,16,512]>  ::Add(%\"val_2528\", %\"model.encoder.model.stages_3.blocks.0.attn_grid.attn.proj.bias\"{...})\n",
       "            1395 |  # node_view_98\n",
       "                    %\"view_98\"<FLOAT,[batch,1,1,16,16,512]>  ::Reshape(%\"linear_77\", %\"val_2435\"{[-1, 1, 1, 16, 16, 512]}) {allowzero=1}\n",
       "            1396 |  # node_permute_58\n",
       "                    %\"permute_58\"<FLOAT,[batch,16,1,16,1,512]>  ::Transpose(%\"view_98\") {perm=(0, 3, 1, 4, 2, 5)}\n",
       "            1397 |  # node_view_99\n",
       "                    %\"view_99\"<FLOAT,[batch,16,16,512]>  ::Reshape(%\"permute_58\", %\"val_2374\"{[-1, 16, 16, 512]}) {allowzero=1}\n",
       "            1398 |  # node_add_4329\n",
       "                    %\"add_4329\"<FLOAT,[batch,16,16,512]>  ::Add(%\"add_4218\", %\"view_99\")\n",
       "            1399 |  # node_layer_norm_39\n",
       "                    %\"layer_norm_39\"<FLOAT,[batch,16,16,512]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_4329\", %\"model.encoder.model.stages_3.blocks.0.attn_grid.norm2.weight\"{...}, %\"model.encoder.model.stages_3.blocks.0.attn_grid.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1400 |  # node_MatMul_2426\n",
       "                    %\"val_2546\"<FLOAT,[batch,16,16,2048]>  ::MatMul(%\"layer_norm_39\", %\"val_2545\"{...})\n",
       "            1401 |  # node_linear_78\n",
       "                    %\"linear_78\"<FLOAT,[batch,16,16,2048]>  ::Add(%\"val_2546\", %\"model.encoder.model.stages_3.blocks.0.attn_grid.mlp.fc1.bias\"{...})\n",
       "            1402 |  # node_Pow_2428\n",
       "                    %\"val_2548\"<FLOAT,[batch,16,16,2048]>  ::Pow(%\"linear_78\", %\"val_16\"{3.0})\n",
       "            1403 |  # node_Mul_2430\n",
       "                    %\"val_2550\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_2548\")\n",
       "            1404 |  # node_Add_2431\n",
       "                    %\"val_2551\"<FLOAT,[batch,16,16,2048]>  ::Add(%\"linear_78\", %\"val_2550\")\n",
       "            1405 |  # node_Mul_2433\n",
       "                    %\"val_2553\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_2551\")\n",
       "            1406 |  # node_Tanh_2434\n",
       "                    %\"val_2554\"<FLOAT,[batch,16,16,2048]>  ::Tanh(%\"val_2553\")\n",
       "            1407 |  # node_Add_2436\n",
       "                    %\"val_2556\"<FLOAT,[batch,16,16,2048]>  ::Add(%\"val_2554\", %\"scalar_tensor_default\"{1.0})\n",
       "            1408 |  # node_Mul_2438\n",
       "                    %\"val_2558\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"val_26\"{0.5}, %\"val_2556\")\n",
       "            1409 |  # node_gelu_40\n",
       "                    %\"gelu_40\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"linear_78\", %\"val_2558\")\n",
       "            1410 |  # node_MatMul_2440\n",
       "                    %\"val_2560\"<FLOAT,[batch,16,16,512]>  ::MatMul(%\"gelu_40\", %\"val_2559\"{...})\n",
       "            1411 |  # node_linear_79\n",
       "                    %\"linear_79\"<FLOAT,[batch,16,16,512]>  ::Add(%\"val_2560\", %\"model.encoder.model.stages_3.blocks.0.attn_grid.mlp.fc2.bias\"{...})\n",
       "            1412 |  # node_add_4365\n",
       "                    %\"add_4365\"<FLOAT,[batch,16,16,512]>  ::Add(%\"add_4329\", %\"linear_79\")\n",
       "            1413 |  # node_permute_59\n",
       "                    %\"permute_59\"<FLOAT,[batch,512,16,16]>  ::Transpose(%\"add_4365\") {perm=(0, 3, 1, 2)}\n",
       "            1414 |  # node__native_batch_norm_legit_no_training_31__0\n",
       "                    %\"getitem_153\"<FLOAT,[batch,512,16,16]>  ::BatchNormalization(%\"permute_59\", %\"model.encoder.model.stages_3.blocks.1.conv.pre_norm.weight\"{...}, %\"model.encoder.model.stages_3.blocks.1.conv.pre_norm.bias\"{...}, %\"model.encoder.model.stages_3.blocks.1.conv.pre_norm.running_mean\"{...}, %\"model.encoder.model.stages_3.blocks.1.conv.pre_norm.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "            1415 |  # node_conv2d_55\n",
       "                    %\"conv2d_55\"<FLOAT,[batch,2048,16,16]>  ::Conv(%\"getitem_153\", %\"model.encoder.model.stages_3.blocks.1.conv.conv1_1x1.weight\"{...}, %\"val_2569\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1416 |  # node__native_batch_norm_legit_no_training_32__0\n",
       "                    %\"getitem_156\"<FLOAT,[batch,2048,16,16]>  ::BatchNormalization(%\"conv2d_55\", %\"model.encoder.model.stages_3.blocks.1.conv.norm1.weight\"{...}, %\"model.encoder.model.stages_3.blocks.1.conv.norm1.bias\"{...}, %\"model.encoder.model.stages_3.blocks.1.conv.norm1.running_mean\"{...}, %\"model.encoder.model.stages_3.blocks.1.conv.norm1.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "            1417 |  # node_Pow_2455\n",
       "                    %\"val_2575\"<FLOAT,[batch,2048,16,16]>  ::Pow(%\"getitem_156\", %\"val_16\"{3.0})\n",
       "            1418 |  # node_Mul_2457\n",
       "                    %\"val_2577\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_2575\")\n",
       "            1419 |  # node_Add_2458\n",
       "                    %\"val_2578\"<FLOAT,[batch,2048,16,16]>  ::Add(%\"getitem_156\", %\"val_2577\")\n",
       "            1420 |  # node_Mul_2460\n",
       "                    %\"val_2580\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_2578\")\n",
       "            1421 |  # node_Tanh_2461\n",
       "                    %\"val_2581\"<FLOAT,[batch,2048,16,16]>  ::Tanh(%\"val_2580\")\n",
       "            1422 |  # node_Add_2463\n",
       "                    %\"val_2583\"<FLOAT,[batch,2048,16,16]>  ::Add(%\"val_2581\", %\"scalar_tensor_default\"{1.0})\n",
       "            1423 |  # node_Mul_2465\n",
       "                    %\"val_2585\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"val_26\"{0.5}, %\"val_2583\")\n",
       "            1424 |  # node_gelu_41\n",
       "                    %\"gelu_41\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"getitem_156\", %\"val_2585\")\n",
       "            1425 |  # node_conv2d_56\n",
       "                    %\"conv2d_56\"<FLOAT,[batch,2048,16,16]>  ::Conv(%\"gelu_41\", %\"model.encoder.model.stages_3.blocks.1.conv.conv2_kxk.weight\"{...}, %\"val_2590\"{...}) {group=2048, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1426 |  # node__native_batch_norm_legit_no_training_33__0\n",
       "                    %\"getitem_159\"<FLOAT,[batch,2048,16,16]>  ::BatchNormalization(%\"conv2d_56\", %\"model.encoder.model.stages_3.blocks.1.conv.norm2.weight\"{...}, %\"model.encoder.model.stages_3.blocks.1.conv.norm2.bias\"{...}, %\"model.encoder.model.stages_3.blocks.1.conv.norm2.running_mean\"{...}, %\"model.encoder.model.stages_3.blocks.1.conv.norm2.running_var\"{...}) {momentum=0.9, epsilon=0.001}\n",
       "            1427 |  # node_Pow_2476\n",
       "                    %\"val_2596\"<FLOAT,[batch,2048,16,16]>  ::Pow(%\"getitem_159\", %\"val_16\"{3.0})\n",
       "            1428 |  # node_Mul_2478\n",
       "                    %\"val_2598\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_2596\")\n",
       "            1429 |  # node_Add_2479\n",
       "                    %\"val_2599\"<FLOAT,[batch,2048,16,16]>  ::Add(%\"getitem_159\", %\"val_2598\")\n",
       "            1430 |  # node_Mul_2481\n",
       "                    %\"val_2601\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_2599\")\n",
       "            1431 |  # node_Tanh_2482\n",
       "                    %\"val_2602\"<FLOAT,[batch,2048,16,16]>  ::Tanh(%\"val_2601\")\n",
       "            1432 |  # node_Add_2484\n",
       "                    %\"val_2604\"<FLOAT,[batch,2048,16,16]>  ::Add(%\"val_2602\", %\"scalar_tensor_default\"{1.0})\n",
       "            1433 |  # node_Mul_2486\n",
       "                    %\"val_2606\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"val_26\"{0.5}, %\"val_2604\")\n",
       "            1434 |  # node_gelu_42\n",
       "                    %\"gelu_42\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"getitem_159\", %\"val_2606\")\n",
       "            1435 |  # node_mean_10\n",
       "                    %\"mean_10\"<FLOAT,[batch,2048,1,1]>  ::ReduceMean(%\"gelu_42\", %\"val_81\"{[2, 3]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "            1436 |  # node_conv2d_57\n",
       "                    %\"conv2d_57\"<FLOAT,[batch,128,1,1]>  ::Conv(%\"mean_10\", %\"model.encoder.model.stages_3.blocks.1.conv.se.fc1.weight\"{...}, %\"model.encoder.model.stages_3.blocks.1.conv.se.fc1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1437 |  # node_Sigmoid_2489\n",
       "                    %\"val_2609\"<FLOAT,[batch,128,1,1]>  ::Sigmoid(%\"conv2d_57\")\n",
       "            1438 |  # node_silu_10\n",
       "                    %\"silu_10\"<FLOAT,[batch,128,1,1]>  ::Mul(%\"conv2d_57\", %\"val_2609\")\n",
       "            1439 |  # node_conv2d_58\n",
       "                    %\"conv2d_58\"<FLOAT,[batch,2048,1,1]>  ::Conv(%\"silu_10\", %\"model.encoder.model.stages_3.blocks.1.conv.se.fc2.weight\"{...}, %\"model.encoder.model.stages_3.blocks.1.conv.se.fc2.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1440 |  # node_sigmoid_10\n",
       "                    %\"sigmoid_10\"<FLOAT,[batch,2048,1,1]>  ::Sigmoid(%\"conv2d_58\")\n",
       "            1441 |  # node_mul_2561\n",
       "                    %\"mul_2561\"<FLOAT,[batch,2048,16,16]>  ::Mul(%\"gelu_42\", %\"sigmoid_10\")\n",
       "            1442 |  # node_conv2d_59\n",
       "                    %\"conv2d_59\"<FLOAT,[batch,512,16,16]>  ::Conv(%\"mul_2561\", %\"model.encoder.model.stages_3.blocks.1.conv.conv3_1x1.weight\"{...}, %\"model.encoder.model.stages_3.blocks.1.conv.conv3_1x1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1443 |  # node_add_4452\n",
       "                    %\"add_4452\"<FLOAT,[batch,512,16,16]>  ::Add(%\"conv2d_59\", %\"permute_59\")\n",
       "            1444 |  # node_permute_60\n",
       "                    %\"permute_60\"<FLOAT,[batch,16,16,512]>  ::Transpose(%\"add_4452\") {perm=(0, 2, 3, 1)}\n",
       "            1445 |  # node_layer_norm_40\n",
       "                    %\"layer_norm_40\"<FLOAT,[batch,16,16,512]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_60\", %\"model.encoder.model.stages_3.blocks.1.attn_block.norm1.weight\"{...}, %\"model.encoder.model.stages_3.blocks.1.attn_block.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1446 |  # node_view_100\n",
       "                    %\"view_100\"<FLOAT,[batch,1,16,1,16,512]>  ::Reshape(%\"layer_norm_40\", %\"val_2368\") {allowzero=1}\n",
       "            1447 |  # node_permute_61\n",
       "                    %\"permute_61\"<FLOAT,[batch,1,1,16,16,512]>  ::Transpose(%\"view_100\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "            1448 |  # node_view_101\n",
       "                    %\"view_101\"<FLOAT,[batch,16,16,512]>  ::Reshape(%\"permute_61\", %\"val_2374\"{[-1, 16, 16, 512]}) {allowzero=1}\n",
       "            1449 |  # node_MatMul_2505\n",
       "                    %\"val_2627\"<FLOAT,[batch,16,16,1536]>  ::MatMul(%\"view_101\", %\"val_2626\"{...})\n",
       "            1450 |  # node_linear_80\n",
       "                    %\"linear_80\"<FLOAT,[batch,16,16,1536]>  ::Add(%\"val_2627\", %\"model.encoder.model.stages_3.blocks.1.attn_block.attn.qkv.bias\"{...})\n",
       "            1451 |  # node_view_102\n",
       "                    %\"view_102\"<FLOAT,[batch,256,3,16,32]>  ::Reshape(%\"linear_80\", %\"val_2383\") {allowzero=1}\n",
       "            1452 |  # node_transpose_40\n",
       "                    %\"transpose_40\"<FLOAT,[batch,16,3,256,32]>  ::Transpose(%\"view_102\") {perm=(0, 3, 2, 1, 4)}\n",
       "            1453 |  # node_Split_2513\n",
       "                    %\"val_2635\"<FLOAT,[batch,16,1,256,32]>, %\"val_2636\"<FLOAT,[batch,16,1,256,32]>, %\"val_2637\"<FLOAT,[batch,16,1,256,32]>  ::Split(%\"transpose_40\") {num_outputs=3, axis=2}\n",
       "            1454 |  # node_unbind_20__0\n",
       "                    %\"getitem_162\"<FLOAT,[batch,16,256,32]>  ::Squeeze(%\"val_2635\", %\"val_106\"{[2]})\n",
       "            1455 |  # node_unbind_20__1\n",
       "                    %\"getitem_163\"<FLOAT,[batch,16,256,32]>  ::Squeeze(%\"val_2636\", %\"val_106\"{[2]})\n",
       "            1456 |  # node_unbind_20__2\n",
       "                    %\"getitem_164\"<FLOAT,[batch,16,256,32]>  ::Squeeze(%\"val_2637\", %\"val_106\"{[2]})\n",
       "            1457 |  # node_einsum_40\n",
       "                    %\"einsum_40\"<FLOAT,[16,16,16,31]>  ::Einsum(%\"model.encoder.model.stages_3.blocks.1.attn_block.attn.rel_pos.relative_position_bias_table\"{...}, %\"model.encoder.model.stages_3.blocks.1.attn_block.attn.rel_pos.height_lookup\"{...}) {equation='nhw,ixh->nixw'}\n",
       "            1458 |  # node_einsum_41\n",
       "                    %\"einsum_41\"<FLOAT,[16,16,16,16,16]>  ::Einsum(%\"einsum_40\", %\"model.encoder.model.stages_3.blocks.1.attn_block.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "            1459 |  # node__unsafe_view_40\n",
       "                    %\"_unsafe_view_40\"<FLOAT,[16,256,256]>  ::Reshape(%\"einsum_41\", %\"val_2391\"{[16, 256, 256]}) {allowzero=1}\n",
       "            1460 |  # node_Shape_2528\n",
       "                    %\"val_2652\"<INT64,[4]>  ::Shape(%\"getitem_163\") {start=0}\n",
       "            1461 |  # node_Slice_2530\n",
       "                    %\"val_2654\"<INT64,[1]>  ::Slice(%\"val_2652\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "            1462 |  # node_Slice_2531\n",
       "                    %\"val_2655\"<INT64,[1]>  ::Slice(%\"val_2652\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "            1463 |  # node_Slice_2533\n",
       "                    %\"val_2657\"<INT64,[2]>  ::Slice(%\"val_2652\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "            1464 |  # node_Concat_2535\n",
       "                    %\"val_2659\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_2655\", %\"val_2654\") {axis=0}\n",
       "            1465 |  # node_Reshape_2536\n",
       "                    %\"val_2660\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_163\", %\"val_2659\") {allowzero=0}\n",
       "            1466 |  # node_Transpose_2537\n",
       "                    %\"val_2661\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_2660\") {perm=(0, 2, 1)}\n",
       "            1467 |  # node_Concat_2538\n",
       "                    %\"val_2662\"<INT64,[4]>  ::Concat(%\"val_2657\", %\"val_2654\", %\"val_2655\") {axis=0}\n",
       "            1468 |  # node_Reshape_2539\n",
       "                    %\"val_2663\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_2661\", %\"val_2662\") {allowzero=0}\n",
       "            1469 |  # node_Mul_2541\n",
       "                    %\"val_2665\"<FLOAT,[batch,16,256,32]>  ::Mul(%\"getitem_162\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1470 |  # node_Mul_2543\n",
       "                    %\"val_2667\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_2663\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1471 |  # node_MatMul_2544\n",
       "                    %\"val_2668\"<FLOAT,[None,16,256,None]>  ::MatMul(%\"val_2665\", %\"val_2667\")\n",
       "            1472 |  # node_Add_2545\n",
       "                    %\"val_2669\"<FLOAT,[None,16,256,256]>  ::Add(%\"val_2668\", %\"_unsafe_view_40\")\n",
       "            1473 |  # node_Softmax_2546\n",
       "                    %\"val_2670\"<FLOAT,[None,16,256,256]>  ::Softmax(%\"val_2669\") {axis=-1}\n",
       "            1474 |  # node_scaled_dot_product_attention_20\n",
       "                    %\"scaled_dot_product_attention_20\"<FLOAT,[batch,16,256,32]>  ::MatMul(%\"val_2670\", %\"getitem_164\")\n",
       "            1475 |  # node_transpose_41\n",
       "                    %\"transpose_41\"<FLOAT,[batch,256,16,32]>  ::Transpose(%\"scaled_dot_product_attention_20\") {perm=(0, 2, 1, 3)}\n",
       "            1476 |  # node__unsafe_view_41\n",
       "                    %\"_unsafe_view_41\"<FLOAT,[batch,16,16,512]>  ::Reshape(%\"transpose_41\", %\"val_2425\") {allowzero=1}\n",
       "            1477 |  # node_MatMul_2554\n",
       "                    %\"val_2678\"<FLOAT,[batch,16,16,512]>  ::MatMul(%\"_unsafe_view_41\", %\"val_2677\"{...})\n",
       "            1478 |  # node_linear_81\n",
       "                    %\"linear_81\"<FLOAT,[batch,16,16,512]>  ::Add(%\"val_2678\", %\"model.encoder.model.stages_3.blocks.1.attn_block.attn.proj.bias\"{...})\n",
       "            1479 |  # node_view_103\n",
       "                    %\"view_103\"<FLOAT,[batch,1,1,16,16,512]>  ::Reshape(%\"linear_81\", %\"val_2435\"{[-1, 1, 1, 16, 16, 512]}) {allowzero=1}\n",
       "            1480 |  # node_permute_62\n",
       "                    %\"permute_62\"<FLOAT,[batch,1,16,1,16,512]>  ::Transpose(%\"view_103\") {perm=(0, 1, 3, 2, 4, 5)}\n",
       "            1481 |  # node_view_104\n",
       "                    %\"view_104\"<FLOAT,[batch,16,16,512]>  ::Reshape(%\"permute_62\", %\"val_2374\"{[-1, 16, 16, 512]}) {allowzero=1}\n",
       "            1482 |  # node_add_4568\n",
       "                    %\"add_4568\"<FLOAT,[batch,16,16,512]>  ::Add(%\"permute_60\", %\"view_104\")\n",
       "            1483 |  # node_layer_norm_41\n",
       "                    %\"layer_norm_41\"<FLOAT,[batch,16,16,512]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_4568\", %\"model.encoder.model.stages_3.blocks.1.attn_block.norm2.weight\"{...}, %\"model.encoder.model.stages_3.blocks.1.attn_block.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1484 |  # node_MatMul_2570\n",
       "                    %\"val_2696\"<FLOAT,[batch,16,16,2048]>  ::MatMul(%\"layer_norm_41\", %\"val_2695\"{...})\n",
       "            1485 |  # node_linear_82\n",
       "                    %\"linear_82\"<FLOAT,[batch,16,16,2048]>  ::Add(%\"val_2696\", %\"model.encoder.model.stages_3.blocks.1.attn_block.mlp.fc1.bias\"{...})\n",
       "            1486 |  # node_Pow_2572\n",
       "                    %\"val_2698\"<FLOAT,[batch,16,16,2048]>  ::Pow(%\"linear_82\", %\"val_16\"{3.0})\n",
       "            1487 |  # node_Mul_2574\n",
       "                    %\"val_2700\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_2698\")\n",
       "            1488 |  # node_Add_2575\n",
       "                    %\"val_2701\"<FLOAT,[batch,16,16,2048]>  ::Add(%\"linear_82\", %\"val_2700\")\n",
       "            1489 |  # node_Mul_2577\n",
       "                    %\"val_2703\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_2701\")\n",
       "            1490 |  # node_Tanh_2578\n",
       "                    %\"val_2704\"<FLOAT,[batch,16,16,2048]>  ::Tanh(%\"val_2703\")\n",
       "            1491 |  # node_Add_2580\n",
       "                    %\"val_2706\"<FLOAT,[batch,16,16,2048]>  ::Add(%\"val_2704\", %\"scalar_tensor_default\"{1.0})\n",
       "            1492 |  # node_Mul_2582\n",
       "                    %\"val_2708\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"val_26\"{0.5}, %\"val_2706\")\n",
       "            1493 |  # node_gelu_43\n",
       "                    %\"gelu_43\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"linear_82\", %\"val_2708\")\n",
       "            1494 |  # node_MatMul_2584\n",
       "                    %\"val_2710\"<FLOAT,[batch,16,16,512]>  ::MatMul(%\"gelu_43\", %\"val_2709\"{...})\n",
       "            1495 |  # node_linear_83\n",
       "                    %\"linear_83\"<FLOAT,[batch,16,16,512]>  ::Add(%\"val_2710\", %\"model.encoder.model.stages_3.blocks.1.attn_block.mlp.fc2.bias\"{...})\n",
       "            1496 |  # node_add_4604\n",
       "                    %\"add_4604\"<FLOAT,[batch,16,16,512]>  ::Add(%\"add_4568\", %\"linear_83\")\n",
       "            1497 |  # node_layer_norm_42\n",
       "                    %\"layer_norm_42\"<FLOAT,[batch,16,16,512]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_4604\", %\"model.encoder.model.stages_3.blocks.1.attn_grid.norm1.weight\"{...}, %\"model.encoder.model.stages_3.blocks.1.attn_grid.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1498 |  # node_view_105\n",
       "                    %\"view_105\"<FLOAT,[batch,16,1,16,1,512]>  ::Reshape(%\"layer_norm_42\", %\"val_2469\") {allowzero=1}\n",
       "            1499 |  # node_permute_63\n",
       "                    %\"permute_63\"<FLOAT,[batch,1,1,16,16,512]>  ::Transpose(%\"view_105\") {perm=(0, 2, 4, 1, 3, 5)}\n",
       "            1500 |  # node_view_106\n",
       "                    %\"view_106\"<FLOAT,[batch,16,16,512]>  ::Reshape(%\"permute_63\", %\"val_2374\"{[-1, 16, 16, 512]}) {allowzero=1}\n",
       "            1501 |  # node_MatMul_2600\n",
       "                    %\"val_2728\"<FLOAT,[batch,16,16,1536]>  ::MatMul(%\"view_106\", %\"val_2727\"{...})\n",
       "            1502 |  # node_linear_84\n",
       "                    %\"linear_84\"<FLOAT,[batch,16,16,1536]>  ::Add(%\"val_2728\", %\"model.encoder.model.stages_3.blocks.1.attn_grid.attn.qkv.bias\"{...})\n",
       "            1503 |  # node_view_107\n",
       "                    %\"view_107\"<FLOAT,[batch,256,3,16,32]>  ::Reshape(%\"linear_84\", %\"val_2383\") {allowzero=1}\n",
       "            1504 |  # node_transpose_42\n",
       "                    %\"transpose_42\"<FLOAT,[batch,16,3,256,32]>  ::Transpose(%\"view_107\") {perm=(0, 3, 2, 1, 4)}\n",
       "            1505 |  # node_Split_2608\n",
       "                    %\"val_2736\"<FLOAT,[batch,16,1,256,32]>, %\"val_2737\"<FLOAT,[batch,16,1,256,32]>, %\"val_2738\"<FLOAT,[batch,16,1,256,32]>  ::Split(%\"transpose_42\") {num_outputs=3, axis=2}\n",
       "            1506 |  # node_unbind_21__0\n",
       "                    %\"getitem_165\"<FLOAT,[batch,16,256,32]>  ::Squeeze(%\"val_2736\", %\"val_106\"{[2]})\n",
       "            1507 |  # node_unbind_21__1\n",
       "                    %\"getitem_166\"<FLOAT,[batch,16,256,32]>  ::Squeeze(%\"val_2737\", %\"val_106\"{[2]})\n",
       "            1508 |  # node_unbind_21__2\n",
       "                    %\"getitem_167\"<FLOAT,[batch,16,256,32]>  ::Squeeze(%\"val_2738\", %\"val_106\"{[2]})\n",
       "            1509 |  # node_einsum_42\n",
       "                    %\"einsum_42\"<FLOAT,[16,16,16,31]>  ::Einsum(%\"model.encoder.model.stages_3.blocks.1.attn_grid.attn.rel_pos.relative_position_bias_table\"{...}, %\"model.encoder.model.stages_3.blocks.1.attn_grid.attn.rel_pos.height_lookup\"{...}) {equation='nhw,ixh->nixw'}\n",
       "            1510 |  # node_einsum_43\n",
       "                    %\"einsum_43\"<FLOAT,[16,16,16,16,16]>  ::Einsum(%\"einsum_42\", %\"model.encoder.model.stages_3.blocks.1.attn_grid.attn.rel_pos.width_lookup\"{...}) {equation='nixw,jyw->nijxy'}\n",
       "            1511 |  # node__unsafe_view_42\n",
       "                    %\"_unsafe_view_42\"<FLOAT,[16,256,256]>  ::Reshape(%\"einsum_43\", %\"val_2391\"{[16, 256, 256]}) {allowzero=1}\n",
       "            1512 |  # node_Shape_2623\n",
       "                    %\"val_2753\"<INT64,[4]>  ::Shape(%\"getitem_166\") {start=0}\n",
       "            1513 |  # node_Slice_2625\n",
       "                    %\"val_2755\"<INT64,[1]>  ::Slice(%\"val_2753\", %\"val_102\"{[-1]}, %\"val_128\"{[9223372036854775807]})\n",
       "            1514 |  # node_Slice_2626\n",
       "                    %\"val_2756\"<INT64,[1]>  ::Slice(%\"val_2753\", %\"val_131\"{[-2]}, %\"val_102\"{[-1]})\n",
       "            1515 |  # node_Slice_2628\n",
       "                    %\"val_2758\"<INT64,[2]>  ::Slice(%\"val_2753\", %\"val_133\"{[-9223372036854775808]}, %\"val_131\"{[-2]})\n",
       "            1516 |  # node_Concat_2630\n",
       "                    %\"val_2760\"<INT64,[3]>  ::Concat(%\"val_102\"{[-1]}, %\"val_2756\", %\"val_2755\") {axis=0}\n",
       "            1517 |  # node_Reshape_2631\n",
       "                    %\"val_2761\"<FLOAT,[None,None,None]>  ::Reshape(%\"getitem_166\", %\"val_2760\") {allowzero=0}\n",
       "            1518 |  # node_Transpose_2632\n",
       "                    %\"val_2762\"<FLOAT,[None,None,None]>  ::Transpose(%\"val_2761\") {perm=(0, 2, 1)}\n",
       "            1519 |  # node_Concat_2633\n",
       "                    %\"val_2763\"<INT64,[4]>  ::Concat(%\"val_2758\", %\"val_2755\", %\"val_2756\") {axis=0}\n",
       "            1520 |  # node_Reshape_2634\n",
       "                    %\"val_2764\"<FLOAT,[None,None,None,None]>  ::Reshape(%\"val_2762\", %\"val_2763\") {allowzero=0}\n",
       "            1521 |  # node_Mul_2636\n",
       "                    %\"val_2766\"<FLOAT,[batch,16,256,32]>  ::Mul(%\"getitem_165\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1522 |  # node_Mul_2638\n",
       "                    %\"val_2768\"<FLOAT,[None,None,None,None]>  ::Mul(%\"val_2764\", %\"val_141\"{[0.4204482138156891]})\n",
       "            1523 |  # node_MatMul_2639\n",
       "                    %\"val_2769\"<FLOAT,[None,16,256,None]>  ::MatMul(%\"val_2766\", %\"val_2768\")\n",
       "            1524 |  # node_Add_2640\n",
       "                    %\"val_2770\"<FLOAT,[None,16,256,256]>  ::Add(%\"val_2769\", %\"_unsafe_view_42\")\n",
       "            1525 |  # node_Softmax_2641\n",
       "                    %\"val_2771\"<FLOAT,[None,16,256,256]>  ::Softmax(%\"val_2770\") {axis=-1}\n",
       "            1526 |  # node_scaled_dot_product_attention_21\n",
       "                    %\"scaled_dot_product_attention_21\"<FLOAT,[batch,16,256,32]>  ::MatMul(%\"val_2771\", %\"getitem_167\")\n",
       "            1527 |  # node_transpose_43\n",
       "                    %\"transpose_43\"<FLOAT,[batch,256,16,32]>  ::Transpose(%\"scaled_dot_product_attention_21\") {perm=(0, 2, 1, 3)}\n",
       "            1528 |  # node__unsafe_view_43\n",
       "                    %\"_unsafe_view_43\"<FLOAT,[batch,16,16,512]>  ::Reshape(%\"transpose_43\", %\"val_2425\") {allowzero=1}\n",
       "            1529 |  # node_MatMul_2649\n",
       "                    %\"val_2779\"<FLOAT,[batch,16,16,512]>  ::MatMul(%\"_unsafe_view_43\", %\"val_2778\"{...})\n",
       "            1530 |  # node_linear_85\n",
       "                    %\"linear_85\"<FLOAT,[batch,16,16,512]>  ::Add(%\"val_2779\", %\"model.encoder.model.stages_3.blocks.1.attn_grid.attn.proj.bias\"{...})\n",
       "            1531 |  # node_view_108\n",
       "                    %\"view_108\"<FLOAT,[batch,1,1,16,16,512]>  ::Reshape(%\"linear_85\", %\"val_2435\"{[-1, 1, 1, 16, 16, 512]}) {allowzero=1}\n",
       "            1532 |  # node_permute_64\n",
       "                    %\"permute_64\"<FLOAT,[batch,16,1,16,1,512]>  ::Transpose(%\"view_108\") {perm=(0, 3, 1, 4, 2, 5)}\n",
       "            1533 |  # node_view_109\n",
       "                    %\"view_109\"<FLOAT,[batch,16,16,512]>  ::Reshape(%\"permute_64\", %\"val_2374\"{[-1, 16, 16, 512]}) {allowzero=1}\n",
       "            1534 |  # node_add_4715\n",
       "                    %\"add_4715\"<FLOAT,[batch,16,16,512]>  ::Add(%\"add_4604\", %\"view_109\")\n",
       "            1535 |  # node_layer_norm_43\n",
       "                    %\"layer_norm_43\"<FLOAT,[batch,16,16,512]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"add_4715\", %\"model.encoder.model.stages_3.blocks.1.attn_grid.norm2.weight\"{...}, %\"model.encoder.model.stages_3.blocks.1.attn_grid.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
       "            1536 |  # node_MatMul_2665\n",
       "                    %\"val_2797\"<FLOAT,[batch,16,16,2048]>  ::MatMul(%\"layer_norm_43\", %\"val_2796\"{...})\n",
       "            1537 |  # node_linear_86\n",
       "                    %\"linear_86\"<FLOAT,[batch,16,16,2048]>  ::Add(%\"val_2797\", %\"model.encoder.model.stages_3.blocks.1.attn_grid.mlp.fc1.bias\"{...})\n",
       "            1538 |  # node_Pow_2667\n",
       "                    %\"val_2799\"<FLOAT,[batch,16,16,2048]>  ::Pow(%\"linear_86\", %\"val_16\"{3.0})\n",
       "            1539 |  # node_Mul_2669\n",
       "                    %\"val_2801\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"val_18\"{0.044714998453855515}, %\"val_2799\")\n",
       "            1540 |  # node_Add_2670\n",
       "                    %\"val_2802\"<FLOAT,[batch,16,16,2048]>  ::Add(%\"linear_86\", %\"val_2801\")\n",
       "            1541 |  # node_Mul_2672\n",
       "                    %\"val_2804\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"val_21\"{0.7978845834732056}, %\"val_2802\")\n",
       "            1542 |  # node_Tanh_2673\n",
       "                    %\"val_2805\"<FLOAT,[batch,16,16,2048]>  ::Tanh(%\"val_2804\")\n",
       "            1543 |  # node_Add_2675\n",
       "                    %\"val_2807\"<FLOAT,[batch,16,16,2048]>  ::Add(%\"val_2805\", %\"scalar_tensor_default\"{1.0})\n",
       "            1544 |  # node_Mul_2677\n",
       "                    %\"val_2809\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"val_26\"{0.5}, %\"val_2807\")\n",
       "            1545 |  # node_gelu_44\n",
       "                    %\"gelu_44\"<FLOAT,[batch,16,16,2048]>  ::Mul(%\"linear_86\", %\"val_2809\")\n",
       "            1546 |  # node_MatMul_2679\n",
       "                    %\"val_2811\"<FLOAT,[batch,16,16,512]>  ::MatMul(%\"gelu_44\", %\"val_2810\"{...})\n",
       "            1547 |  # node_linear_87\n",
       "                    %\"linear_87\"<FLOAT,[batch,16,16,512]>  ::Add(%\"val_2811\", %\"model.encoder.model.stages_3.blocks.1.attn_grid.mlp.fc2.bias\"{...})\n",
       "            1548 |  # node_add_4751\n",
       "                    %\"add_4751\"<FLOAT,[batch,16,16,512]>  ::Add(%\"add_4715\", %\"linear_87\")\n",
       "            1549 |  # node_permute_65\n",
       "                    %\"permute_65\"<FLOAT,[batch,512,16,16]>  ::Transpose(%\"add_4751\") {perm=(0, 3, 1, 2)}\n",
       "            1550 |  # node_Shape_2680\n",
       "                    %\"val_2812\"<INT64,[2]>  ::Shape(%\"permute_65\") {end=2, start=0}\n",
       "            1551 |  # node_Concat_2683\n",
       "                    %\"val_2815\"<INT64,[4]>  ::Concat(%\"val_2812\", %\"val_2814\"{[32, 32]}) {axis=0}\n",
       "            1552 |  # node_upsample_nearest2d\n",
       "                    %\"upsample_nearest2d\"<FLOAT,[batch,512,32,32]>  ::Resize(%\"permute_65\", None, None, %\"val_2815\") {extrapolation_value=0.0, keep_aspect_ratio_policy='stretch', antialias=0, nearest_mode='floor', exclude_outside=0, cubic_coeff_a=-0.75, coordinate_transformation_mode='asymmetric', mode='nearest'}\n",
       "            1553 |  # node_cat_1\n",
       "                    %\"cat_1\"<FLOAT,[batch,768,32,32]>  ::Concat(%\"upsample_nearest2d\", %\"permute_53\") {axis=1}\n",
       "            1554 |  # node_conv2d_60\n",
       "                    %\"conv2d_60\"<FLOAT,[batch,256,32,32]>  ::Conv(%\"cat_1\", %\"model.decoder.blocks.0.conv1.0.weight\"{...}, %\"val_39\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1555 |  # node__native_batch_norm_legit_no_training_34__0\n",
       "                    %\"getitem_168\"<FLOAT,[batch,256,32,32]>  ::BatchNormalization(%\"conv2d_60\", %\"model.decoder.blocks.0.conv1.1.weight\"{...}, %\"model.decoder.blocks.0.conv1.1.bias\"{...}, %\"model.decoder.blocks.0.conv1.1.running_mean\"{...}, %\"model.decoder.blocks.0.conv1.1.running_var\"{...}) {momentum=0.9, epsilon=1e-05}\n",
       "            1556 |  # node_relu\n",
       "                    %\"relu\"<FLOAT,[batch,256,32,32]>  ::Relu(%\"getitem_168\")\n",
       "            1557 |  # node_conv2d_61\n",
       "                    %\"conv2d_61\"<FLOAT,[batch,256,32,32]>  ::Conv(%\"relu\", %\"model.decoder.blocks.0.conv2.0.weight\"{...}, %\"val_39\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1558 |  # node__native_batch_norm_legit_no_training_35__0\n",
       "                    %\"getitem_171\"<FLOAT,[batch,256,32,32]>  ::BatchNormalization(%\"conv2d_61\", %\"model.decoder.blocks.0.conv2.1.weight\"{...}, %\"model.decoder.blocks.0.conv2.1.bias\"{...}, %\"model.decoder.blocks.0.conv2.1.running_mean\"{...}, %\"model.decoder.blocks.0.conv2.1.running_var\"{...}) {momentum=0.9, epsilon=1e-05}\n",
       "            1559 |  # node_relu_1\n",
       "                    %\"relu_1\"<FLOAT,[batch,256,32,32]>  ::Relu(%\"getitem_171\")\n",
       "            1560 |  # node_Shape_2703\n",
       "                    %\"val_2835\"<INT64,[2]>  ::Shape(%\"relu_1\") {end=2, start=0}\n",
       "            1561 |  # node_Concat_2706\n",
       "                    %\"val_2838\"<INT64,[4]>  ::Concat(%\"val_2835\", %\"val_2837\"{[64, 64]}) {axis=0}\n",
       "            1562 |  # node_upsample_nearest2d_1\n",
       "                    %\"upsample_nearest2d_1\"<FLOAT,[batch,256,64,64]>  ::Resize(%\"relu_1\", None, None, %\"val_2838\") {extrapolation_value=0.0, keep_aspect_ratio_policy='stretch', antialias=0, nearest_mode='floor', exclude_outside=0, cubic_coeff_a=-0.75, coordinate_transformation_mode='asymmetric', mode='nearest'}\n",
       "            1563 |  # node_cat_2\n",
       "                    %\"cat_2\"<FLOAT,[batch,384,64,64]>  ::Concat(%\"upsample_nearest2d_1\", %\"permute_23\") {axis=1}\n",
       "            1564 |  # node_conv2d_62\n",
       "                    %\"conv2d_62\"<FLOAT,[batch,128,64,64]>  ::Conv(%\"cat_2\", %\"model.decoder.blocks.1.conv1.0.weight\"{...}, %\"val_2843\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1565 |  # node__native_batch_norm_legit_no_training_36__0\n",
       "                    %\"getitem_174\"<FLOAT,[batch,128,64,64]>  ::BatchNormalization(%\"conv2d_62\", %\"model.decoder.blocks.1.conv1.1.weight\"{...}, %\"model.decoder.blocks.1.conv1.1.bias\"{...}, %\"model.decoder.blocks.1.conv1.1.running_mean\"{...}, %\"model.decoder.blocks.1.conv1.1.running_var\"{...}) {momentum=0.9, epsilon=1e-05}\n",
       "            1566 |  # node_relu_2\n",
       "                    %\"relu_2\"<FLOAT,[batch,128,64,64]>  ::Relu(%\"getitem_174\")\n",
       "            1567 |  # node_conv2d_63\n",
       "                    %\"conv2d_63\"<FLOAT,[batch,128,64,64]>  ::Conv(%\"relu_2\", %\"model.decoder.blocks.1.conv2.0.weight\"{...}, %\"val_2843\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1568 |  # node__native_batch_norm_legit_no_training_37__0\n",
       "                    %\"getitem_177\"<FLOAT,[batch,128,64,64]>  ::BatchNormalization(%\"conv2d_63\", %\"model.decoder.blocks.1.conv2.1.weight\"{...}, %\"model.decoder.blocks.1.conv2.1.bias\"{...}, %\"model.decoder.blocks.1.conv2.1.running_mean\"{...}, %\"model.decoder.blocks.1.conv2.1.running_var\"{...}) {momentum=0.9, epsilon=1e-05}\n",
       "            1569 |  # node_relu_3\n",
       "                    %\"relu_3\"<FLOAT,[batch,128,64,64]>  ::Relu(%\"getitem_177\")\n",
       "            1570 |  # node_Shape_2725\n",
       "                    %\"val_2857\"<INT64,[2]>  ::Shape(%\"relu_3\") {end=2, start=0}\n",
       "            1571 |  # node_Concat_2728\n",
       "                    %\"val_2860\"<INT64,[4]>  ::Concat(%\"val_2857\", %\"val_2859\"{[128, 128]}) {axis=0}\n",
       "            1572 |  # node_upsample_nearest2d_2\n",
       "                    %\"upsample_nearest2d_2\"<FLOAT,[batch,128,128,128]>  ::Resize(%\"relu_3\", None, None, %\"val_2860\") {extrapolation_value=0.0, keep_aspect_ratio_policy='stretch', antialias=0, nearest_mode='floor', exclude_outside=0, cubic_coeff_a=-0.75, coordinate_transformation_mode='asymmetric', mode='nearest'}\n",
       "            1573 |  # node_cat_3\n",
       "                    %\"cat_3\"<FLOAT,[batch,192,128,128]>  ::Concat(%\"upsample_nearest2d_2\", %\"permute_11\") {axis=1}\n",
       "            1574 |  # node_conv2d_64\n",
       "                    %\"conv2d_64\"<FLOAT,[batch,64,128,128]>  ::Conv(%\"cat_3\", %\"model.decoder.blocks.2.conv1.0.weight\"{...}, %\"val_2865\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1575 |  # node__native_batch_norm_legit_no_training_38__0\n",
       "                    %\"getitem_180\"<FLOAT,[batch,64,128,128]>  ::BatchNormalization(%\"conv2d_64\", %\"model.decoder.blocks.2.conv1.1.weight\"{...}, %\"model.decoder.blocks.2.conv1.1.bias\"{...}, %\"model.decoder.blocks.2.conv1.1.running_mean\"{...}, %\"model.decoder.blocks.2.conv1.1.running_var\"{...}) {momentum=0.9, epsilon=1e-05}\n",
       "            1576 |  # node_relu_4\n",
       "                    %\"relu_4\"<FLOAT,[batch,64,128,128]>  ::Relu(%\"getitem_180\")\n",
       "            1577 |  # node_conv2d_65\n",
       "                    %\"conv2d_65\"<FLOAT,[batch,64,128,128]>  ::Conv(%\"relu_4\", %\"model.decoder.blocks.2.conv2.0.weight\"{...}, %\"val_2865\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1578 |  # node__native_batch_norm_legit_no_training_39__0\n",
       "                    %\"getitem_183\"<FLOAT,[batch,64,128,128]>  ::BatchNormalization(%\"conv2d_65\", %\"model.decoder.blocks.2.conv2.1.weight\"{...}, %\"model.decoder.blocks.2.conv2.1.bias\"{...}, %\"model.decoder.blocks.2.conv2.1.running_mean\"{...}, %\"model.decoder.blocks.2.conv2.1.running_var\"{...}) {momentum=0.9, epsilon=1e-05}\n",
       "            1579 |  # node_relu_5\n",
       "                    %\"relu_5\"<FLOAT,[batch,64,128,128]>  ::Relu(%\"getitem_183\")\n",
       "            1580 |  # node_Shape_2747\n",
       "                    %\"val_2879\"<INT64,[2]>  ::Shape(%\"relu_5\") {end=2, start=0}\n",
       "            1581 |  # node_Concat_2750\n",
       "                    %\"val_2882\"<INT64,[4]>  ::Concat(%\"val_2879\", %\"val_2881\"{[256, 256]}) {axis=0}\n",
       "            1582 |  # node_upsample_nearest2d_3\n",
       "                    %\"upsample_nearest2d_3\"<FLOAT,[batch,64,256,256]>  ::Resize(%\"relu_5\", None, None, %\"val_2882\") {extrapolation_value=0.0, keep_aspect_ratio_policy='stretch', antialias=0, nearest_mode='floor', exclude_outside=0, cubic_coeff_a=-0.75, coordinate_transformation_mode='asymmetric', mode='nearest'}\n",
       "            1583 |  # node_cat_4\n",
       "                    %\"cat_4\"<FLOAT,[batch,128,256,256]>  ::Concat(%\"upsample_nearest2d_3\", %\"conv2d_1\") {axis=1}\n",
       "            1584 |  # node_conv2d_66\n",
       "                    %\"conv2d_66\"<FLOAT,[batch,32,256,256]>  ::Conv(%\"cat_4\", %\"model.decoder.blocks.3.conv1.0.weight\"{...}, %\"val_2887\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1585 |  # node__native_batch_norm_legit_no_training_40__0\n",
       "                    %\"getitem_186\"<FLOAT,[batch,32,256,256]>  ::BatchNormalization(%\"conv2d_66\", %\"model.decoder.blocks.3.conv1.1.weight\"{...}, %\"model.decoder.blocks.3.conv1.1.bias\"{...}, %\"model.decoder.blocks.3.conv1.1.running_mean\"{...}, %\"model.decoder.blocks.3.conv1.1.running_var\"{...}) {momentum=0.9, epsilon=1e-05}\n",
       "            1586 |  # node_relu_6\n",
       "                    %\"relu_6\"<FLOAT,[batch,32,256,256]>  ::Relu(%\"getitem_186\")\n",
       "            1587 |  # node_conv2d_67\n",
       "                    %\"conv2d_67\"<FLOAT,[batch,32,256,256]>  ::Conv(%\"relu_6\", %\"model.decoder.blocks.3.conv2.0.weight\"{...}, %\"val_2887\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1588 |  # node__native_batch_norm_legit_no_training_41__0\n",
       "                    %\"getitem_189\"<FLOAT,[batch,32,256,256]>  ::BatchNormalization(%\"conv2d_67\", %\"model.decoder.blocks.3.conv2.1.weight\"{...}, %\"model.decoder.blocks.3.conv2.1.bias\"{...}, %\"model.decoder.blocks.3.conv2.1.running_mean\"{...}, %\"model.decoder.blocks.3.conv2.1.running_var\"{...}) {momentum=0.9, epsilon=1e-05}\n",
       "            1589 |  # node_relu_7\n",
       "                    %\"relu_7\"<FLOAT,[batch,32,256,256]>  ::Relu(%\"getitem_189\")\n",
       "            1590 |  # node_Shape_2769\n",
       "                    %\"val_2901\"<INT64,[2]>  ::Shape(%\"relu_7\") {end=2, start=0}\n",
       "            1591 |  # node_Concat_2772\n",
       "                    %\"val_2904\"<INT64,[4]>  ::Concat(%\"val_2901\", %\"val_2903\"{[512, 512]}) {axis=0}\n",
       "            1592 |  # node_upsample_nearest2d_4\n",
       "                    %\"upsample_nearest2d_4\"<FLOAT,[batch,32,512,512]>  ::Resize(%\"relu_7\", None, None, %\"val_2904\") {extrapolation_value=0.0, keep_aspect_ratio_policy='stretch', antialias=0, nearest_mode='floor', exclude_outside=0, cubic_coeff_a=-0.75, coordinate_transformation_mode='asymmetric', mode='nearest'}\n",
       "            1593 |  # node_conv2d_68\n",
       "                    %\"conv2d_68\"<FLOAT,[batch,16,512,512]>  ::Conv(%\"upsample_nearest2d_4\", %\"model.decoder.blocks.4.conv1.0.weight\"{...}, %\"val_2909\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1594 |  # node__native_batch_norm_legit_no_training_42__0\n",
       "                    %\"getitem_192\"<FLOAT,[batch,16,512,512]>  ::BatchNormalization(%\"conv2d_68\", %\"model.decoder.blocks.4.conv1.1.weight\"{...}, %\"model.decoder.blocks.4.conv1.1.bias\"{...}, %\"model.decoder.blocks.4.conv1.1.running_mean\"{...}, %\"model.decoder.blocks.4.conv1.1.running_var\"{...}) {momentum=0.9, epsilon=1e-05}\n",
       "            1595 |  # node_relu_8\n",
       "                    %\"relu_8\"<FLOAT,[batch,16,512,512]>  ::Relu(%\"getitem_192\")\n",
       "            1596 |  # node_conv2d_69\n",
       "                    %\"conv2d_69\"<FLOAT,[batch,16,512,512]>  ::Conv(%\"relu_8\", %\"model.decoder.blocks.4.conv2.0.weight\"{...}, %\"val_2909\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            1597 |  # node__native_batch_norm_legit_no_training_43__0\n",
       "                    %\"getitem_195\"<FLOAT,[batch,16,512,512]>  ::BatchNormalization(%\"conv2d_69\", %\"model.decoder.blocks.4.conv2.1.weight\"{...}, %\"model.decoder.blocks.4.conv2.1.bias\"{...}, %\"model.decoder.blocks.4.conv2.1.running_mean\"{...}, %\"model.decoder.blocks.4.conv2.1.running_var\"{...}) {momentum=0.9, epsilon=1e-05}\n",
       "            1598 |  # node_relu_9\n",
       "                    %\"relu_9\"<FLOAT,[batch,16,512,512]>  ::Relu(%\"getitem_195\")\n",
       "            1599 |  # node_conv2d_70\n",
       "                    %\"output\"<FLOAT,[batch,1,512,512]>  ::Conv(%\"relu_9\", %\"model.segmentation_head.0.weight\"{...}, %\"model.segmentation_head.0.bias\"{[-0.13056014478206635]}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            return %\"output\"<FLOAT,[batch,1,512,512]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_model_encoder_model_stem_conv1_weight: \"f32[64, 10, 3, 3]\", p_model_encoder_model_stem_conv1_bias: \"f32[64]\", p_model_encoder_model_stem_norm1_weight: \"f32[64]\", p_model_encoder_model_stem_norm1_bias: \"f32[64]\", p_model_encoder_model_stem_conv2_weight: \"f32[64, 64, 3, 3]\", p_model_encoder_model_stem_conv2_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_conv_pre_norm_weight: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_conv_pre_norm_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_conv_conv1_1x1_weight: \"f32[256, 64, 1, 1]\", p_model_encoder_model_stages_0_blocks_0_conv_norm1_weight: \"f32[256]\", p_model_encoder_model_stages_0_blocks_0_conv_norm1_bias: \"f32[256]\", p_model_encoder_model_stages_0_blocks_0_conv_conv2_kxk_weight: \"f32[256, 1, 3, 3]\", p_model_encoder_model_stages_0_blocks_0_conv_norm2_weight: \"f32[256]\", p_model_encoder_model_stages_0_blocks_0_conv_norm2_bias: \"f32[256]\", p_model_encoder_model_stages_0_blocks_0_conv_se_fc1_weight: \"f32[16, 256, 1, 1]\", p_model_encoder_model_stages_0_blocks_0_conv_se_fc1_bias: \"f32[16]\", p_model_encoder_model_stages_0_blocks_0_conv_se_fc2_weight: \"f32[256, 16, 1, 1]\", p_model_encoder_model_stages_0_blocks_0_conv_se_fc2_bias: \"f32[256]\", p_model_encoder_model_stages_0_blocks_0_conv_conv3_1x1_weight: \"f32[64, 256, 1, 1]\", p_model_encoder_model_stages_0_blocks_0_conv_conv3_1x1_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_attn_block_norm1_weight: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_attn_block_norm1_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_attn_block_attn_qkv_weight: \"f32[192, 64]\", p_model_encoder_model_stages_0_blocks_0_attn_block_attn_qkv_bias: \"f32[192]\", p_model_encoder_model_stages_0_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table: \"f32[2, 31, 31]\", p_model_encoder_model_stages_0_blocks_0_attn_block_attn_proj_weight: \"f32[64, 64]\", p_model_encoder_model_stages_0_blocks_0_attn_block_attn_proj_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_attn_block_norm2_weight: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_attn_block_norm2_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc1_weight: \"f32[256, 64]\", p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc1_bias: \"f32[256]\", p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc2_weight: \"f32[64, 256]\", p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc2_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_attn_grid_norm1_weight: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_attn_grid_norm1_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_qkv_weight: \"f32[192, 64]\", p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_qkv_bias: \"f32[192]\", p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table: \"f32[2, 31, 31]\", p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_proj_weight: \"f32[64, 64]\", p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_proj_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_attn_grid_norm2_weight: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_attn_grid_norm2_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc1_weight: \"f32[256, 64]\", p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc1_bias: \"f32[256]\", p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc2_weight: \"f32[64, 256]\", p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc2_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_conv_pre_norm_weight: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_conv_pre_norm_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_conv_conv1_1x1_weight: \"f32[256, 64, 1, 1]\", p_model_encoder_model_stages_0_blocks_1_conv_norm1_weight: \"f32[256]\", p_model_encoder_model_stages_0_blocks_1_conv_norm1_bias: \"f32[256]\", p_model_encoder_model_stages_0_blocks_1_conv_conv2_kxk_weight: \"f32[256, 1, 3, 3]\", p_model_encoder_model_stages_0_blocks_1_conv_norm2_weight: \"f32[256]\", p_model_encoder_model_stages_0_blocks_1_conv_norm2_bias: \"f32[256]\", p_model_encoder_model_stages_0_blocks_1_conv_se_fc1_weight: \"f32[16, 256, 1, 1]\", p_model_encoder_model_stages_0_blocks_1_conv_se_fc1_bias: \"f32[16]\", p_model_encoder_model_stages_0_blocks_1_conv_se_fc2_weight: \"f32[256, 16, 1, 1]\", p_model_encoder_model_stages_0_blocks_1_conv_se_fc2_bias: \"f32[256]\", p_model_encoder_model_stages_0_blocks_1_conv_conv3_1x1_weight: \"f32[64, 256, 1, 1]\", p_model_encoder_model_stages_0_blocks_1_conv_conv3_1x1_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_attn_block_norm1_weight: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_attn_block_norm1_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_attn_block_attn_qkv_weight: \"f32[192, 64]\", p_model_encoder_model_stages_0_blocks_1_attn_block_attn_qkv_bias: \"f32[192]\", p_model_encoder_model_stages_0_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table: \"f32[2, 31, 31]\", p_model_encoder_model_stages_0_blocks_1_attn_block_attn_proj_weight: \"f32[64, 64]\", p_model_encoder_model_stages_0_blocks_1_attn_block_attn_proj_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_attn_block_norm2_weight: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_attn_block_norm2_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc1_weight: \"f32[256, 64]\", p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc1_bias: \"f32[256]\", p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc2_weight: \"f32[64, 256]\", p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc2_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_attn_grid_norm1_weight: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_attn_grid_norm1_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_qkv_weight: \"f32[192, 64]\", p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_qkv_bias: \"f32[192]\", p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table: \"f32[2, 31, 31]\", p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_proj_weight: \"f32[64, 64]\", p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_proj_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_attn_grid_norm2_weight: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_attn_grid_norm2_bias: \"f32[64]\", p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc1_weight: \"f32[256, 64]\", p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc1_bias: \"f32[256]\", p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc2_weight: \"f32[64, 256]\", p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc2_bias: \"f32[64]\", p_model_encoder_model_stages_1_blocks_0_conv_shortcut_expand_weight: \"f32[128, 64, 1, 1]\", p_model_encoder_model_stages_1_blocks_0_conv_shortcut_expand_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_0_conv_pre_norm_weight: \"f32[64]\", p_model_encoder_model_stages_1_blocks_0_conv_pre_norm_bias: \"f32[64]\", p_model_encoder_model_stages_1_blocks_0_conv_conv1_1x1_weight: \"f32[512, 64, 1, 1]\", p_model_encoder_model_stages_1_blocks_0_conv_norm1_weight: \"f32[512]\", p_model_encoder_model_stages_1_blocks_0_conv_norm1_bias: \"f32[512]\", p_model_encoder_model_stages_1_blocks_0_conv_conv2_kxk_weight: \"f32[512, 1, 3, 3]\", p_model_encoder_model_stages_1_blocks_0_conv_norm2_weight: \"f32[512]\", p_model_encoder_model_stages_1_blocks_0_conv_norm2_bias: \"f32[512]\", p_model_encoder_model_stages_1_blocks_0_conv_se_fc1_weight: \"f32[32, 512, 1, 1]\", p_model_encoder_model_stages_1_blocks_0_conv_se_fc1_bias: \"f32[32]\", p_model_encoder_model_stages_1_blocks_0_conv_se_fc2_weight: \"f32[512, 32, 1, 1]\", p_model_encoder_model_stages_1_blocks_0_conv_se_fc2_bias: \"f32[512]\", p_model_encoder_model_stages_1_blocks_0_conv_conv3_1x1_weight: \"f32[128, 512, 1, 1]\", p_model_encoder_model_stages_1_blocks_0_conv_conv3_1x1_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_0_attn_block_norm1_weight: \"f32[128]\", p_model_encoder_model_stages_1_blocks_0_attn_block_norm1_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_0_attn_block_attn_qkv_weight: \"f32[384, 128]\", p_model_encoder_model_stages_1_blocks_0_attn_block_attn_qkv_bias: \"f32[384]\", p_model_encoder_model_stages_1_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table: \"f32[4, 31, 31]\", p_model_encoder_model_stages_1_blocks_0_attn_block_attn_proj_weight: \"f32[128, 128]\", p_model_encoder_model_stages_1_blocks_0_attn_block_attn_proj_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_0_attn_block_norm2_weight: \"f32[128]\", p_model_encoder_model_stages_1_blocks_0_attn_block_norm2_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc1_weight: \"f32[512, 128]\", p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc1_bias: \"f32[512]\", p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc2_weight: \"f32[128, 512]\", p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc2_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_0_attn_grid_norm1_weight: \"f32[128]\", p_model_encoder_model_stages_1_blocks_0_attn_grid_norm1_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_qkv_weight: \"f32[384, 128]\", p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_qkv_bias: \"f32[384]\", p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table: \"f32[4, 31, 31]\", p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_proj_weight: \"f32[128, 128]\", p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_proj_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_0_attn_grid_norm2_weight: \"f32[128]\", p_model_encoder_model_stages_1_blocks_0_attn_grid_norm2_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc1_weight: \"f32[512, 128]\", p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc1_bias: \"f32[512]\", p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc2_weight: \"f32[128, 512]\", p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc2_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_conv_pre_norm_weight: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_conv_pre_norm_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_conv_conv1_1x1_weight: \"f32[512, 128, 1, 1]\", p_model_encoder_model_stages_1_blocks_1_conv_norm1_weight: \"f32[512]\", p_model_encoder_model_stages_1_blocks_1_conv_norm1_bias: \"f32[512]\", p_model_encoder_model_stages_1_blocks_1_conv_conv2_kxk_weight: \"f32[512, 1, 3, 3]\", p_model_encoder_model_stages_1_blocks_1_conv_norm2_weight: \"f32[512]\", p_model_encoder_model_stages_1_blocks_1_conv_norm2_bias: \"f32[512]\", p_model_encoder_model_stages_1_blocks_1_conv_se_fc1_weight: \"f32[32, 512, 1, 1]\", p_model_encoder_model_stages_1_blocks_1_conv_se_fc1_bias: \"f32[32]\", p_model_encoder_model_stages_1_blocks_1_conv_se_fc2_weight: \"f32[512, 32, 1, 1]\", p_model_encoder_model_stages_1_blocks_1_conv_se_fc2_bias: \"f32[512]\", p_model_encoder_model_stages_1_blocks_1_conv_conv3_1x1_weight: \"f32[128, 512, 1, 1]\", p_model_encoder_model_stages_1_blocks_1_conv_conv3_1x1_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_attn_block_norm1_weight: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_attn_block_norm1_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_attn_block_attn_qkv_weight: \"f32[384, 128]\", p_model_encoder_model_stages_1_blocks_1_attn_block_attn_qkv_bias: \"f32[384]\", p_model_encoder_model_stages_1_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table: \"f32[4, 31, 31]\", p_model_encoder_model_stages_1_blocks_1_attn_block_attn_proj_weight: \"f32[128, 128]\", p_model_encoder_model_stages_1_blocks_1_attn_block_attn_proj_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_attn_block_norm2_weight: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_attn_block_norm2_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc1_weight: \"f32[512, 128]\", p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc1_bias: \"f32[512]\", p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc2_weight: \"f32[128, 512]\", p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc2_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_attn_grid_norm1_weight: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_attn_grid_norm1_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_qkv_weight: \"f32[384, 128]\", p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_qkv_bias: \"f32[384]\", p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table: \"f32[4, 31, 31]\", p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_proj_weight: \"f32[128, 128]\", p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_proj_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_attn_grid_norm2_weight: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_attn_grid_norm2_bias: \"f32[128]\", p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc1_weight: \"f32[512, 128]\", p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc1_bias: \"f32[512]\", p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc2_weight: \"f32[128, 512]\", p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc2_bias: \"f32[128]\", p_model_encoder_model_stages_2_blocks_0_conv_shortcut_expand_weight: \"f32[256, 128, 1, 1]\", p_model_encoder_model_stages_2_blocks_0_conv_shortcut_expand_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_0_conv_pre_norm_weight: \"f32[128]\", p_model_encoder_model_stages_2_blocks_0_conv_pre_norm_bias: \"f32[128]\", p_model_encoder_model_stages_2_blocks_0_conv_conv1_1x1_weight: \"f32[1024, 128, 1, 1]\", p_model_encoder_model_stages_2_blocks_0_conv_norm1_weight: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_0_conv_norm1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_0_conv_conv2_kxk_weight: \"f32[1024, 1, 3, 3]\", p_model_encoder_model_stages_2_blocks_0_conv_norm2_weight: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_0_conv_norm2_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_0_conv_se_fc1_weight: \"f32[64, 1024, 1, 1]\", p_model_encoder_model_stages_2_blocks_0_conv_se_fc1_bias: \"f32[64]\", p_model_encoder_model_stages_2_blocks_0_conv_se_fc2_weight: \"f32[1024, 64, 1, 1]\", p_model_encoder_model_stages_2_blocks_0_conv_se_fc2_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_0_conv_conv3_1x1_weight: \"f32[256, 1024, 1, 1]\", p_model_encoder_model_stages_2_blocks_0_conv_conv3_1x1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_0_attn_block_norm1_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_0_attn_block_norm1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_0_attn_block_attn_qkv_weight: \"f32[768, 256]\", p_model_encoder_model_stages_2_blocks_0_attn_block_attn_qkv_bias: \"f32[768]\", p_model_encoder_model_stages_2_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table: \"f32[8, 31, 31]\", p_model_encoder_model_stages_2_blocks_0_attn_block_attn_proj_weight: \"f32[256, 256]\", p_model_encoder_model_stages_2_blocks_0_attn_block_attn_proj_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_0_attn_block_norm2_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_0_attn_block_norm2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc1_weight: \"f32[1024, 256]\", p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc2_weight: \"f32[256, 1024]\", p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_0_attn_grid_norm1_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_0_attn_grid_norm1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_qkv_weight: \"f32[768, 256]\", p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_qkv_bias: \"f32[768]\", p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table: \"f32[8, 31, 31]\", p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_proj_weight: \"f32[256, 256]\", p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_proj_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_0_attn_grid_norm2_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_0_attn_grid_norm2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc1_weight: \"f32[1024, 256]\", p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc2_weight: \"f32[256, 1024]\", p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_conv_pre_norm_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_conv_pre_norm_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_conv_conv1_1x1_weight: \"f32[1024, 256, 1, 1]\", p_model_encoder_model_stages_2_blocks_1_conv_norm1_weight: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_1_conv_norm1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_1_conv_conv2_kxk_weight: \"f32[1024, 1, 3, 3]\", p_model_encoder_model_stages_2_blocks_1_conv_norm2_weight: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_1_conv_norm2_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_1_conv_se_fc1_weight: \"f32[64, 1024, 1, 1]\", p_model_encoder_model_stages_2_blocks_1_conv_se_fc1_bias: \"f32[64]\", p_model_encoder_model_stages_2_blocks_1_conv_se_fc2_weight: \"f32[1024, 64, 1, 1]\", p_model_encoder_model_stages_2_blocks_1_conv_se_fc2_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_1_conv_conv3_1x1_weight: \"f32[256, 1024, 1, 1]\", p_model_encoder_model_stages_2_blocks_1_conv_conv3_1x1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_attn_block_norm1_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_attn_block_norm1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_attn_block_attn_qkv_weight: \"f32[768, 256]\", p_model_encoder_model_stages_2_blocks_1_attn_block_attn_qkv_bias: \"f32[768]\", p_model_encoder_model_stages_2_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table: \"f32[8, 31, 31]\", p_model_encoder_model_stages_2_blocks_1_attn_block_attn_proj_weight: \"f32[256, 256]\", p_model_encoder_model_stages_2_blocks_1_attn_block_attn_proj_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_attn_block_norm2_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_attn_block_norm2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc1_weight: \"f32[1024, 256]\", p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc2_weight: \"f32[256, 1024]\", p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_attn_grid_norm1_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_attn_grid_norm1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_qkv_weight: \"f32[768, 256]\", p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_qkv_bias: \"f32[768]\", p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table: \"f32[8, 31, 31]\", p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_proj_weight: \"f32[256, 256]\", p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_proj_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_attn_grid_norm2_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_attn_grid_norm2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc1_weight: \"f32[1024, 256]\", p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc2_weight: \"f32[256, 1024]\", p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_conv_pre_norm_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_conv_pre_norm_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_conv_conv1_1x1_weight: \"f32[1024, 256, 1, 1]\", p_model_encoder_model_stages_2_blocks_2_conv_norm1_weight: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_2_conv_norm1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_2_conv_conv2_kxk_weight: \"f32[1024, 1, 3, 3]\", p_model_encoder_model_stages_2_blocks_2_conv_norm2_weight: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_2_conv_norm2_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_2_conv_se_fc1_weight: \"f32[64, 1024, 1, 1]\", p_model_encoder_model_stages_2_blocks_2_conv_se_fc1_bias: \"f32[64]\", p_model_encoder_model_stages_2_blocks_2_conv_se_fc2_weight: \"f32[1024, 64, 1, 1]\", p_model_encoder_model_stages_2_blocks_2_conv_se_fc2_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_2_conv_conv3_1x1_weight: \"f32[256, 1024, 1, 1]\", p_model_encoder_model_stages_2_blocks_2_conv_conv3_1x1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_attn_block_norm1_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_attn_block_norm1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_attn_block_attn_qkv_weight: \"f32[768, 256]\", p_model_encoder_model_stages_2_blocks_2_attn_block_attn_qkv_bias: \"f32[768]\", p_model_encoder_model_stages_2_blocks_2_attn_block_attn_rel_pos_relative_position_bias_table: \"f32[8, 31, 31]\", p_model_encoder_model_stages_2_blocks_2_attn_block_attn_proj_weight: \"f32[256, 256]\", p_model_encoder_model_stages_2_blocks_2_attn_block_attn_proj_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_attn_block_norm2_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_attn_block_norm2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc1_weight: \"f32[1024, 256]\", p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc2_weight: \"f32[256, 1024]\", p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_attn_grid_norm1_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_attn_grid_norm1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_qkv_weight: \"f32[768, 256]\", p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_qkv_bias: \"f32[768]\", p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_rel_pos_relative_position_bias_table: \"f32[8, 31, 31]\", p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_proj_weight: \"f32[256, 256]\", p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_proj_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_attn_grid_norm2_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_attn_grid_norm2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc1_weight: \"f32[1024, 256]\", p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc2_weight: \"f32[256, 1024]\", p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_conv_pre_norm_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_conv_pre_norm_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_conv_conv1_1x1_weight: \"f32[1024, 256, 1, 1]\", p_model_encoder_model_stages_2_blocks_3_conv_norm1_weight: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_3_conv_norm1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_3_conv_conv2_kxk_weight: \"f32[1024, 1, 3, 3]\", p_model_encoder_model_stages_2_blocks_3_conv_norm2_weight: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_3_conv_norm2_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_3_conv_se_fc1_weight: \"f32[64, 1024, 1, 1]\", p_model_encoder_model_stages_2_blocks_3_conv_se_fc1_bias: \"f32[64]\", p_model_encoder_model_stages_2_blocks_3_conv_se_fc2_weight: \"f32[1024, 64, 1, 1]\", p_model_encoder_model_stages_2_blocks_3_conv_se_fc2_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_3_conv_conv3_1x1_weight: \"f32[256, 1024, 1, 1]\", p_model_encoder_model_stages_2_blocks_3_conv_conv3_1x1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_attn_block_norm1_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_attn_block_norm1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_attn_block_attn_qkv_weight: \"f32[768, 256]\", p_model_encoder_model_stages_2_blocks_3_attn_block_attn_qkv_bias: \"f32[768]\", p_model_encoder_model_stages_2_blocks_3_attn_block_attn_rel_pos_relative_position_bias_table: \"f32[8, 31, 31]\", p_model_encoder_model_stages_2_blocks_3_attn_block_attn_proj_weight: \"f32[256, 256]\", p_model_encoder_model_stages_2_blocks_3_attn_block_attn_proj_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_attn_block_norm2_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_attn_block_norm2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc1_weight: \"f32[1024, 256]\", p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc2_weight: \"f32[256, 1024]\", p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_attn_grid_norm1_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_attn_grid_norm1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_qkv_weight: \"f32[768, 256]\", p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_qkv_bias: \"f32[768]\", p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_rel_pos_relative_position_bias_table: \"f32[8, 31, 31]\", p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_proj_weight: \"f32[256, 256]\", p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_proj_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_attn_grid_norm2_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_attn_grid_norm2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc1_weight: \"f32[1024, 256]\", p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc2_weight: \"f32[256, 1024]\", p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_conv_pre_norm_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_conv_pre_norm_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_conv_conv1_1x1_weight: \"f32[1024, 256, 1, 1]\", p_model_encoder_model_stages_2_blocks_4_conv_norm1_weight: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_4_conv_norm1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_4_conv_conv2_kxk_weight: \"f32[1024, 1, 3, 3]\", p_model_encoder_model_stages_2_blocks_4_conv_norm2_weight: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_4_conv_norm2_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_4_conv_se_fc1_weight: \"f32[64, 1024, 1, 1]\", p_model_encoder_model_stages_2_blocks_4_conv_se_fc1_bias: \"f32[64]\", p_model_encoder_model_stages_2_blocks_4_conv_se_fc2_weight: \"f32[1024, 64, 1, 1]\", p_model_encoder_model_stages_2_blocks_4_conv_se_fc2_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_4_conv_conv3_1x1_weight: \"f32[256, 1024, 1, 1]\", p_model_encoder_model_stages_2_blocks_4_conv_conv3_1x1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_attn_block_norm1_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_attn_block_norm1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_attn_block_attn_qkv_weight: \"f32[768, 256]\", p_model_encoder_model_stages_2_blocks_4_attn_block_attn_qkv_bias: \"f32[768]\", p_model_encoder_model_stages_2_blocks_4_attn_block_attn_rel_pos_relative_position_bias_table: \"f32[8, 31, 31]\", p_model_encoder_model_stages_2_blocks_4_attn_block_attn_proj_weight: \"f32[256, 256]\", p_model_encoder_model_stages_2_blocks_4_attn_block_attn_proj_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_attn_block_norm2_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_attn_block_norm2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc1_weight: \"f32[1024, 256]\", p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc2_weight: \"f32[256, 1024]\", p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_attn_grid_norm1_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_attn_grid_norm1_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_qkv_weight: \"f32[768, 256]\", p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_qkv_bias: \"f32[768]\", p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_rel_pos_relative_position_bias_table: \"f32[8, 31, 31]\", p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_proj_weight: \"f32[256, 256]\", p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_proj_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_attn_grid_norm2_weight: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_attn_grid_norm2_bias: \"f32[256]\", p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc1_weight: \"f32[1024, 256]\", p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc1_bias: \"f32[1024]\", p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc2_weight: \"f32[256, 1024]\", p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc2_bias: \"f32[256]\", p_model_encoder_model_stages_3_blocks_0_conv_shortcut_expand_weight: \"f32[512, 256, 1, 1]\", p_model_encoder_model_stages_3_blocks_0_conv_shortcut_expand_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_0_conv_pre_norm_weight: \"f32[256]\", p_model_encoder_model_stages_3_blocks_0_conv_pre_norm_bias: \"f32[256]\", p_model_encoder_model_stages_3_blocks_0_conv_conv1_1x1_weight: \"f32[2048, 256, 1, 1]\", p_model_encoder_model_stages_3_blocks_0_conv_norm1_weight: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_0_conv_norm1_bias: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_0_conv_conv2_kxk_weight: \"f32[2048, 1, 3, 3]\", p_model_encoder_model_stages_3_blocks_0_conv_norm2_weight: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_0_conv_norm2_bias: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_0_conv_se_fc1_weight: \"f32[128, 2048, 1, 1]\", p_model_encoder_model_stages_3_blocks_0_conv_se_fc1_bias: \"f32[128]\", p_model_encoder_model_stages_3_blocks_0_conv_se_fc2_weight: \"f32[2048, 128, 1, 1]\", p_model_encoder_model_stages_3_blocks_0_conv_se_fc2_bias: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_0_conv_conv3_1x1_weight: \"f32[512, 2048, 1, 1]\", p_model_encoder_model_stages_3_blocks_0_conv_conv3_1x1_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_0_attn_block_norm1_weight: \"f32[512]\", p_model_encoder_model_stages_3_blocks_0_attn_block_norm1_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_0_attn_block_attn_qkv_weight: \"f32[1536, 512]\", p_model_encoder_model_stages_3_blocks_0_attn_block_attn_qkv_bias: \"f32[1536]\", p_model_encoder_model_stages_3_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table: \"f32[16, 31, 31]\", p_model_encoder_model_stages_3_blocks_0_attn_block_attn_proj_weight: \"f32[512, 512]\", p_model_encoder_model_stages_3_blocks_0_attn_block_attn_proj_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_0_attn_block_norm2_weight: \"f32[512]\", p_model_encoder_model_stages_3_blocks_0_attn_block_norm2_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc1_weight: \"f32[2048, 512]\", p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc1_bias: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc2_weight: \"f32[512, 2048]\", p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc2_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_0_attn_grid_norm1_weight: \"f32[512]\", p_model_encoder_model_stages_3_blocks_0_attn_grid_norm1_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_qkv_weight: \"f32[1536, 512]\", p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_qkv_bias: \"f32[1536]\", p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table: \"f32[16, 31, 31]\", p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_proj_weight: \"f32[512, 512]\", p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_proj_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_0_attn_grid_norm2_weight: \"f32[512]\", p_model_encoder_model_stages_3_blocks_0_attn_grid_norm2_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc1_weight: \"f32[2048, 512]\", p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc1_bias: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc2_weight: \"f32[512, 2048]\", p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc2_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_conv_pre_norm_weight: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_conv_pre_norm_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_conv_conv1_1x1_weight: \"f32[2048, 512, 1, 1]\", p_model_encoder_model_stages_3_blocks_1_conv_norm1_weight: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_1_conv_norm1_bias: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_1_conv_conv2_kxk_weight: \"f32[2048, 1, 3, 3]\", p_model_encoder_model_stages_3_blocks_1_conv_norm2_weight: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_1_conv_norm2_bias: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_1_conv_se_fc1_weight: \"f32[128, 2048, 1, 1]\", p_model_encoder_model_stages_3_blocks_1_conv_se_fc1_bias: \"f32[128]\", p_model_encoder_model_stages_3_blocks_1_conv_se_fc2_weight: \"f32[2048, 128, 1, 1]\", p_model_encoder_model_stages_3_blocks_1_conv_se_fc2_bias: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_1_conv_conv3_1x1_weight: \"f32[512, 2048, 1, 1]\", p_model_encoder_model_stages_3_blocks_1_conv_conv3_1x1_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_attn_block_norm1_weight: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_attn_block_norm1_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_attn_block_attn_qkv_weight: \"f32[1536, 512]\", p_model_encoder_model_stages_3_blocks_1_attn_block_attn_qkv_bias: \"f32[1536]\", p_model_encoder_model_stages_3_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table: \"f32[16, 31, 31]\", p_model_encoder_model_stages_3_blocks_1_attn_block_attn_proj_weight: \"f32[512, 512]\", p_model_encoder_model_stages_3_blocks_1_attn_block_attn_proj_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_attn_block_norm2_weight: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_attn_block_norm2_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc1_weight: \"f32[2048, 512]\", p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc1_bias: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc2_weight: \"f32[512, 2048]\", p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc2_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_attn_grid_norm1_weight: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_attn_grid_norm1_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_qkv_weight: \"f32[1536, 512]\", p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_qkv_bias: \"f32[1536]\", p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table: \"f32[16, 31, 31]\", p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_proj_weight: \"f32[512, 512]\", p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_proj_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_attn_grid_norm2_weight: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_attn_grid_norm2_bias: \"f32[512]\", p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc1_weight: \"f32[2048, 512]\", p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc1_bias: \"f32[2048]\", p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc2_weight: \"f32[512, 2048]\", p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc2_bias: \"f32[512]\", p_model_decoder_blocks_0_conv1_0_weight: \"f32[256, 768, 3, 3]\", p_model_decoder_blocks_0_conv1_1_weight: \"f32[256]\", p_model_decoder_blocks_0_conv1_1_bias: \"f32[256]\", p_model_decoder_blocks_0_conv2_0_weight: \"f32[256, 256, 3, 3]\", p_model_decoder_blocks_0_conv2_1_weight: \"f32[256]\", p_model_decoder_blocks_0_conv2_1_bias: \"f32[256]\", p_model_decoder_blocks_1_conv1_0_weight: \"f32[128, 384, 3, 3]\", p_model_decoder_blocks_1_conv1_1_weight: \"f32[128]\", p_model_decoder_blocks_1_conv1_1_bias: \"f32[128]\", p_model_decoder_blocks_1_conv2_0_weight: \"f32[128, 128, 3, 3]\", p_model_decoder_blocks_1_conv2_1_weight: \"f32[128]\", p_model_decoder_blocks_1_conv2_1_bias: \"f32[128]\", p_model_decoder_blocks_2_conv1_0_weight: \"f32[64, 192, 3, 3]\", p_model_decoder_blocks_2_conv1_1_weight: \"f32[64]\", p_model_decoder_blocks_2_conv1_1_bias: \"f32[64]\", p_model_decoder_blocks_2_conv2_0_weight: \"f32[64, 64, 3, 3]\", p_model_decoder_blocks_2_conv2_1_weight: \"f32[64]\", p_model_decoder_blocks_2_conv2_1_bias: \"f32[64]\", p_model_decoder_blocks_3_conv1_0_weight: \"f32[32, 128, 3, 3]\", p_model_decoder_blocks_3_conv1_1_weight: \"f32[32]\", p_model_decoder_blocks_3_conv1_1_bias: \"f32[32]\", p_model_decoder_blocks_3_conv2_0_weight: \"f32[32, 32, 3, 3]\", p_model_decoder_blocks_3_conv2_1_weight: \"f32[32]\", p_model_decoder_blocks_3_conv2_1_bias: \"f32[32]\", p_model_decoder_blocks_4_conv1_0_weight: \"f32[16, 32, 3, 3]\", p_model_decoder_blocks_4_conv1_1_weight: \"f32[16]\", p_model_decoder_blocks_4_conv1_1_bias: \"f32[16]\", p_model_decoder_blocks_4_conv2_0_weight: \"f32[16, 16, 3, 3]\", p_model_decoder_blocks_4_conv2_1_weight: \"f32[16]\", p_model_decoder_blocks_4_conv2_1_bias: \"f32[16]\", p_model_segmentation_head_0_weight: \"f32[1, 16, 3, 3]\", p_model_segmentation_head_0_bias: \"f32[1]\", b_per_channel_mean: \"f32[1, 10, 1, 1]\", b_per_channel_std: \"f32[1, 10, 1, 1]\", b_model_encoder_model_stem_norm1_running_mean: \"f32[64]\", b_model_encoder_model_stem_norm1_running_var: \"f32[64]\", b_model_encoder_model_stem_norm1_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_0_blocks_0_conv_pre_norm_running_mean: \"f32[64]\", b_model_encoder_model_stages_0_blocks_0_conv_pre_norm_running_var: \"f32[64]\", b_model_encoder_model_stages_0_blocks_0_conv_pre_norm_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_0_blocks_0_conv_norm1_running_mean: \"f32[256]\", b_model_encoder_model_stages_0_blocks_0_conv_norm1_running_var: \"f32[256]\", b_model_encoder_model_stages_0_blocks_0_conv_norm1_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_0_blocks_0_conv_norm2_running_mean: \"f32[256]\", b_model_encoder_model_stages_0_blocks_0_conv_norm2_running_var: \"f32[256]\", b_model_encoder_model_stages_0_blocks_0_conv_norm2_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_0_blocks_0_attn_block_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_0_blocks_0_attn_block_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_0_blocks_0_attn_grid_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_0_blocks_0_attn_grid_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_0_blocks_1_conv_pre_norm_running_mean: \"f32[64]\", b_model_encoder_model_stages_0_blocks_1_conv_pre_norm_running_var: \"f32[64]\", b_model_encoder_model_stages_0_blocks_1_conv_pre_norm_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_0_blocks_1_conv_norm1_running_mean: \"f32[256]\", b_model_encoder_model_stages_0_blocks_1_conv_norm1_running_var: \"f32[256]\", b_model_encoder_model_stages_0_blocks_1_conv_norm1_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_0_blocks_1_conv_norm2_running_mean: \"f32[256]\", b_model_encoder_model_stages_0_blocks_1_conv_norm2_running_var: \"f32[256]\", b_model_encoder_model_stages_0_blocks_1_conv_norm2_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_0_blocks_1_attn_block_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_0_blocks_1_attn_block_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_0_blocks_1_attn_grid_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_0_blocks_1_attn_grid_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_1_blocks_0_conv_pre_norm_running_mean: \"f32[64]\", b_model_encoder_model_stages_1_blocks_0_conv_pre_norm_running_var: \"f32[64]\", b_model_encoder_model_stages_1_blocks_0_conv_pre_norm_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_1_blocks_0_conv_norm1_running_mean: \"f32[512]\", b_model_encoder_model_stages_1_blocks_0_conv_norm1_running_var: \"f32[512]\", b_model_encoder_model_stages_1_blocks_0_conv_norm1_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_1_blocks_0_conv_norm2_running_mean: \"f32[512]\", b_model_encoder_model_stages_1_blocks_0_conv_norm2_running_var: \"f32[512]\", b_model_encoder_model_stages_1_blocks_0_conv_norm2_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_1_blocks_0_attn_block_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_1_blocks_0_attn_block_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_1_blocks_0_attn_grid_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_1_blocks_0_attn_grid_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_1_blocks_1_conv_pre_norm_running_mean: \"f32[128]\", b_model_encoder_model_stages_1_blocks_1_conv_pre_norm_running_var: \"f32[128]\", b_model_encoder_model_stages_1_blocks_1_conv_pre_norm_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_1_blocks_1_conv_norm1_running_mean: \"f32[512]\", b_model_encoder_model_stages_1_blocks_1_conv_norm1_running_var: \"f32[512]\", b_model_encoder_model_stages_1_blocks_1_conv_norm1_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_1_blocks_1_conv_norm2_running_mean: \"f32[512]\", b_model_encoder_model_stages_1_blocks_1_conv_norm2_running_var: \"f32[512]\", b_model_encoder_model_stages_1_blocks_1_conv_norm2_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_1_blocks_1_attn_block_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_1_blocks_1_attn_block_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_1_blocks_1_attn_grid_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_1_blocks_1_attn_grid_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_0_conv_pre_norm_running_mean: \"f32[128]\", b_model_encoder_model_stages_2_blocks_0_conv_pre_norm_running_var: \"f32[128]\", b_model_encoder_model_stages_2_blocks_0_conv_pre_norm_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_0_conv_norm1_running_mean: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_0_conv_norm1_running_var: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_0_conv_norm1_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_0_conv_norm2_running_mean: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_0_conv_norm2_running_var: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_0_conv_norm2_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_0_attn_block_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_0_attn_block_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_0_attn_grid_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_0_attn_grid_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_1_conv_pre_norm_running_mean: \"f32[256]\", b_model_encoder_model_stages_2_blocks_1_conv_pre_norm_running_var: \"f32[256]\", b_model_encoder_model_stages_2_blocks_1_conv_pre_norm_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_1_conv_norm1_running_mean: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_1_conv_norm1_running_var: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_1_conv_norm1_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_1_conv_norm2_running_mean: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_1_conv_norm2_running_var: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_1_conv_norm2_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_1_attn_block_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_1_attn_block_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_1_attn_grid_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_1_attn_grid_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_2_conv_pre_norm_running_mean: \"f32[256]\", b_model_encoder_model_stages_2_blocks_2_conv_pre_norm_running_var: \"f32[256]\", b_model_encoder_model_stages_2_blocks_2_conv_pre_norm_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_2_conv_norm1_running_mean: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_2_conv_norm1_running_var: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_2_conv_norm1_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_2_conv_norm2_running_mean: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_2_conv_norm2_running_var: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_2_conv_norm2_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_2_attn_block_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_2_attn_block_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_2_attn_grid_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_2_attn_grid_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_3_conv_pre_norm_running_mean: \"f32[256]\", b_model_encoder_model_stages_2_blocks_3_conv_pre_norm_running_var: \"f32[256]\", b_model_encoder_model_stages_2_blocks_3_conv_pre_norm_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_3_conv_norm1_running_mean: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_3_conv_norm1_running_var: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_3_conv_norm1_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_3_conv_norm2_running_mean: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_3_conv_norm2_running_var: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_3_conv_norm2_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_3_attn_block_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_3_attn_block_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_3_attn_grid_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_3_attn_grid_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_4_conv_pre_norm_running_mean: \"f32[256]\", b_model_encoder_model_stages_2_blocks_4_conv_pre_norm_running_var: \"f32[256]\", b_model_encoder_model_stages_2_blocks_4_conv_pre_norm_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_4_conv_norm1_running_mean: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_4_conv_norm1_running_var: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_4_conv_norm1_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_4_conv_norm2_running_mean: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_4_conv_norm2_running_var: \"f32[1024]\", b_model_encoder_model_stages_2_blocks_4_conv_norm2_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_2_blocks_4_attn_block_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_4_attn_block_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_4_attn_grid_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_2_blocks_4_attn_grid_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_3_blocks_0_conv_pre_norm_running_mean: \"f32[256]\", b_model_encoder_model_stages_3_blocks_0_conv_pre_norm_running_var: \"f32[256]\", b_model_encoder_model_stages_3_blocks_0_conv_pre_norm_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_3_blocks_0_conv_norm1_running_mean: \"f32[2048]\", b_model_encoder_model_stages_3_blocks_0_conv_norm1_running_var: \"f32[2048]\", b_model_encoder_model_stages_3_blocks_0_conv_norm1_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_3_blocks_0_conv_norm2_running_mean: \"f32[2048]\", b_model_encoder_model_stages_3_blocks_0_conv_norm2_running_var: \"f32[2048]\", b_model_encoder_model_stages_3_blocks_0_conv_norm2_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_3_blocks_0_attn_block_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_3_blocks_0_attn_block_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_3_blocks_0_attn_grid_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_3_blocks_0_attn_grid_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_3_blocks_1_conv_pre_norm_running_mean: \"f32[512]\", b_model_encoder_model_stages_3_blocks_1_conv_pre_norm_running_var: \"f32[512]\", b_model_encoder_model_stages_3_blocks_1_conv_pre_norm_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_3_blocks_1_conv_norm1_running_mean: \"f32[2048]\", b_model_encoder_model_stages_3_blocks_1_conv_norm1_running_var: \"f32[2048]\", b_model_encoder_model_stages_3_blocks_1_conv_norm1_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_3_blocks_1_conv_norm2_running_mean: \"f32[2048]\", b_model_encoder_model_stages_3_blocks_1_conv_norm2_running_var: \"f32[2048]\", b_model_encoder_model_stages_3_blocks_1_conv_norm2_num_batches_tracked: \"i64[]\", b_model_encoder_model_stages_3_blocks_1_attn_block_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_3_blocks_1_attn_block_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_3_blocks_1_attn_grid_attn_rel_pos_height_lookup: \"f32[16, 16, 31]\", b_model_encoder_model_stages_3_blocks_1_attn_grid_attn_rel_pos_width_lookup: \"f32[16, 16, 31]\", b_model_decoder_blocks_0_conv1_1_running_mean: \"f32[256]\", b_model_decoder_blocks_0_conv1_1_running_var: \"f32[256]\", b_model_decoder_blocks_0_conv1_1_num_batches_tracked: \"i64[]\", b_model_decoder_blocks_0_conv2_1_running_mean: \"f32[256]\", b_model_decoder_blocks_0_conv2_1_running_var: \"f32[256]\", b_model_decoder_blocks_0_conv2_1_num_batches_tracked: \"i64[]\", b_model_decoder_blocks_1_conv1_1_running_mean: \"f32[128]\", b_model_decoder_blocks_1_conv1_1_running_var: \"f32[128]\", b_model_decoder_blocks_1_conv1_1_num_batches_tracked: \"i64[]\", b_model_decoder_blocks_1_conv2_1_running_mean: \"f32[128]\", b_model_decoder_blocks_1_conv2_1_running_var: \"f32[128]\", b_model_decoder_blocks_1_conv2_1_num_batches_tracked: \"i64[]\", b_model_decoder_blocks_2_conv1_1_running_mean: \"f32[64]\", b_model_decoder_blocks_2_conv1_1_running_var: \"f32[64]\", b_model_decoder_blocks_2_conv1_1_num_batches_tracked: \"i64[]\", b_model_decoder_blocks_2_conv2_1_running_mean: \"f32[64]\", b_model_decoder_blocks_2_conv2_1_running_var: \"f32[64]\", b_model_decoder_blocks_2_conv2_1_num_batches_tracked: \"i64[]\", b_model_decoder_blocks_3_conv1_1_running_mean: \"f32[32]\", b_model_decoder_blocks_3_conv1_1_running_var: \"f32[32]\", b_model_decoder_blocks_3_conv1_1_num_batches_tracked: \"i64[]\", b_model_decoder_blocks_3_conv2_1_running_mean: \"f32[32]\", b_model_decoder_blocks_3_conv2_1_running_var: \"f32[32]\", b_model_decoder_blocks_3_conv2_1_num_batches_tracked: \"i64[]\", b_model_decoder_blocks_4_conv1_1_running_mean: \"f32[16]\", b_model_decoder_blocks_4_conv1_1_running_var: \"f32[16]\", b_model_decoder_blocks_4_conv1_1_num_batches_tracked: \"i64[]\", b_model_decoder_blocks_4_conv2_1_running_mean: \"f32[16]\", b_model_decoder_blocks_4_conv2_1_running_var: \"f32[16]\", b_model_decoder_blocks_4_conv2_1_num_batches_tracked: \"i64[]\", x: \"f32[s77, 5, 512, 512]\"):\n",
       "                     # \n",
       "                    sym_size_int_37: \"Sym(s77)\" = torch.ops.aten.sym_size.int(x, 0)\n",
       "            \n",
       "                     # File: /var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_18517/148043875.py:51 in forward, code: blue = x.select(1, 0).unsqueeze(1)\n",
       "                    select: \"f32[s77, 512, 512]\" = torch.ops.aten.select.int(x, 1, 0)\n",
       "                    unsqueeze: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.unsqueeze.default(select, 1);  select = None\n",
       "            \n",
       "                     # File: /var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_18517/148043875.py:52 in forward, code: green = x.select(1, 1).unsqueeze(1)\n",
       "                    select_1: \"f32[s77, 512, 512]\" = torch.ops.aten.select.int(x, 1, 1)\n",
       "                    unsqueeze_1: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.unsqueeze.default(select_1, 1);  select_1 = None\n",
       "            \n",
       "                     # File: /var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_18517/148043875.py:53 in forward, code: red = x.select(1, 2).unsqueeze(1)\n",
       "                    select_2: \"f32[s77, 512, 512]\" = torch.ops.aten.select.int(x, 1, 2)\n",
       "                    unsqueeze_2: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.unsqueeze.default(select_2, 1);  select_2 = None\n",
       "            \n",
       "                     # File: /var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_18517/148043875.py:54 in forward, code: nir = x.select(1, 3).unsqueeze(1)\n",
       "                    select_3: \"f32[s77, 512, 512]\" = torch.ops.aten.select.int(x, 1, 3)\n",
       "                    unsqueeze_3: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.unsqueeze.default(select_3, 1);  select_3 = None\n",
       "            \n",
       "                     # File: /var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_18517/148043875.py:55 in forward, code: re = x.select(1, 4).unsqueeze(1)\n",
       "                    select_4: \"f32[s77, 512, 512]\" = torch.ops.aten.select.int(x, 1, 4);  x = None\n",
       "                    unsqueeze_4: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.unsqueeze.default(select_4, 1);  select_4 = None\n",
       "            \n",
       "                     # File: /var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_18517/148043875.py:58 in forward, code: ndvi = self.normalized_index(nir, red)\n",
       "                    sub_10: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.sub.Tensor(unsqueeze_3, unsqueeze_2)\n",
       "                    add_50: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.add.Tensor(unsqueeze_3, unsqueeze_2)\n",
       "                    add_56: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.add.Tensor(add_50, 1e-10);  add_50 = None\n",
       "                    div: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.div.Tensor(sub_10, add_56);  sub_10 = add_56 = None\n",
       "            \n",
       "                     # File: /var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_18517/148043875.py:59 in forward, code: gndvi = self.normalized_index(nir, green)\n",
       "                    sub_15: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.sub.Tensor(unsqueeze_3, unsqueeze_1)\n",
       "                    add_72: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.add.Tensor(unsqueeze_3, unsqueeze_1)\n",
       "                    add_78: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.add.Tensor(add_72, 1e-10);  add_72 = None\n",
       "                    div_1: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.div.Tensor(sub_15, add_78);  sub_15 = add_78 = None\n",
       "            \n",
       "                     # File: /var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_18517/148043875.py:60 in forward, code: ndvi_re = self.normalized_index(re, red)\n",
       "                    sub_20: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.sub.Tensor(unsqueeze_4, unsqueeze_2)\n",
       "                    add_94: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.add.Tensor(unsqueeze_4, unsqueeze_2)\n",
       "                    add_100: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.add.Tensor(add_94, 1e-10);  add_94 = None\n",
       "                    div_2: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.div.Tensor(sub_20, add_100);  sub_20 = add_100 = None\n",
       "            \n",
       "                     # File: /var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_18517/148043875.py:63 in forward, code: ndwi = self.normalized_index(green, nir)\n",
       "                    sub_25: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.sub.Tensor(unsqueeze_1, unsqueeze_3)\n",
       "                    add_116: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.add.Tensor(unsqueeze_1, unsqueeze_3)\n",
       "                    add_122: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.add.Tensor(add_116, 1e-10);  add_116 = None\n",
       "                    div_3: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.div.Tensor(sub_25, add_122);  sub_25 = add_122 = None\n",
       "            \n",
       "                     # File: /var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_18517/148043875.py:64 in forward, code: chl_green = (nir / (green + EPS)) - 1  # Chlorophyll Index Green\n",
       "                    add_133: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.add.Tensor(unsqueeze_1, 1e-10)\n",
       "                    div_4: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.div.Tensor(unsqueeze_3, add_133);  add_133 = None\n",
       "                    scalar_tensor_default: \"f32[]\" = torch.ops.aten.scalar_tensor.default(1, dtype = torch.float32)\n",
       "                    sub_32: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.sub.Tensor(div_4, scalar_tensor_default);  div_4 = scalar_tensor_default = None\n",
       "            \n",
       "                     # File: /var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_18517/148043875.py:67 in forward, code: x_aug = torch.cat([blue, green, red, nir, re, ndvi, ndwi, gndvi, chl_green, ndvi_re], dim=1)\n",
       "                    cat: \"f32[s77, 10, 512, 512]\" = torch.ops.aten.cat.default([unsqueeze, unsqueeze_1, unsqueeze_2, unsqueeze_3, unsqueeze_4, div, div_3, div_1, sub_32, div_2], 1);  unsqueeze = unsqueeze_1 = unsqueeze_2 = unsqueeze_3 = unsqueeze_4 = div = div_3 = div_1 = sub_32 = div_2 = None\n",
       "            \n",
       "                     # File: /var/folders/65/rp91cc952vq9zbnz_h9j_f6h0000gp/T/ipykernel_18517/148043875.py:69 in forward, code: x_aug_normalized = (x_aug - self.per_channel_mean) / self.per_channel_std\n",
       "                    sub_35: \"f32[s77, 10, 512, 512]\" = torch.ops.aten.sub.Tensor(cat, b_per_channel_mean);  cat = b_per_channel_mean = None\n",
       "                    div_5: \"f32[s77, 10, 512, 512]\" = torch.ops.aten.div.Tensor(sub_35, b_per_channel_std);  sub_35 = b_per_channel_std = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/conv2d_same.py:63 in forward, code: return conv2d_same(\n",
       "                    pad: \"f32[s77, 10, 513, 513]\" = torch.ops.aten.pad.default(div_5, [0, 1, 0, 1], 'constant', 0.0);  div_5 = None\n",
       "                    conv2d: \"f32[s77, 64, 256, 256]\" = torch.ops.aten.conv2d.default(pad, p_model_encoder_model_stem_conv1_weight, p_model_encoder_model_stem_conv1_bias, [2, 2]);  pad = p_model_encoder_model_stem_conv1_weight = p_model_encoder_model_stem_conv1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d, p_model_encoder_model_stem_norm1_weight, p_model_encoder_model_stem_norm1_bias, b_model_encoder_model_stem_norm1_running_mean, b_model_encoder_model_stem_norm1_running_var, 0.1, 0.001);  conv2d = p_model_encoder_model_stem_norm1_weight = p_model_encoder_model_stem_norm1_bias = b_model_encoder_model_stem_norm1_running_mean = b_model_encoder_model_stem_norm1_running_var = None\n",
       "                    getitem: \"f32[s77, 64, 256, 256]\" = _native_batch_norm_legit_no_training[0];  _native_batch_norm_legit_no_training = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu: \"f32[s77, 64, 256, 256]\" = torch.ops.aten.gelu.default(getitem, approximate = 'tanh');  getitem = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_1: \"f32[s77, 64, 256, 256]\" = torch.ops.aten.conv2d.default(gelu, p_model_encoder_model_stem_conv2_weight, p_model_encoder_model_stem_conv2_bias, [1, 1], [1, 1]);  gelu = p_model_encoder_model_stem_conv2_weight = p_model_encoder_model_stem_conv2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/pool2d_same.py:45 in forward, code: x = pad_same(x, self.kernel_size, self.stride)\n",
       "                    pad_1: \"f32[s77, 64, 256, 256]\" = torch.ops.aten.pad.default(conv2d_1, [0, 0, 0, 0], 'constant', 0.0)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/pool2d_same.py:46 in forward, code: return F.avg_pool2d(\n",
       "                    avg_pool2d: \"f32[s77, 64, 128, 128]\" = torch.ops.aten.avg_pool2d.default(pad_1, [2, 2], [2, 2]);  pad_1 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_1 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_1, p_model_encoder_model_stages_0_blocks_0_conv_pre_norm_weight, p_model_encoder_model_stages_0_blocks_0_conv_pre_norm_bias, b_model_encoder_model_stages_0_blocks_0_conv_pre_norm_running_mean, b_model_encoder_model_stages_0_blocks_0_conv_pre_norm_running_var, 0.1, 0.001);  p_model_encoder_model_stages_0_blocks_0_conv_pre_norm_weight = p_model_encoder_model_stages_0_blocks_0_conv_pre_norm_bias = b_model_encoder_model_stages_0_blocks_0_conv_pre_norm_running_mean = b_model_encoder_model_stages_0_blocks_0_conv_pre_norm_running_var = None\n",
       "                    getitem_3: \"f32[s77, 64, 256, 256]\" = _native_batch_norm_legit_no_training_1[0];  _native_batch_norm_legit_no_training_1 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_2: \"f32[s77, 256, 256, 256]\" = torch.ops.aten.conv2d.default(getitem_3, p_model_encoder_model_stages_0_blocks_0_conv_conv1_1x1_weight);  getitem_3 = p_model_encoder_model_stages_0_blocks_0_conv_conv1_1x1_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_2 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_2, p_model_encoder_model_stages_0_blocks_0_conv_norm1_weight, p_model_encoder_model_stages_0_blocks_0_conv_norm1_bias, b_model_encoder_model_stages_0_blocks_0_conv_norm1_running_mean, b_model_encoder_model_stages_0_blocks_0_conv_norm1_running_var, 0.1, 0.001);  conv2d_2 = p_model_encoder_model_stages_0_blocks_0_conv_norm1_weight = p_model_encoder_model_stages_0_blocks_0_conv_norm1_bias = b_model_encoder_model_stages_0_blocks_0_conv_norm1_running_mean = b_model_encoder_model_stages_0_blocks_0_conv_norm1_running_var = None\n",
       "                    getitem_6: \"f32[s77, 256, 256, 256]\" = _native_batch_norm_legit_no_training_2[0];  _native_batch_norm_legit_no_training_2 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_1: \"f32[s77, 256, 256, 256]\" = torch.ops.aten.gelu.default(getitem_6, approximate = 'tanh');  getitem_6 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/conv2d_same.py:63 in forward, code: return conv2d_same(\n",
       "                    pad_2: \"f32[s77, 256, 257, 257]\" = torch.ops.aten.pad.default(gelu_1, [0, 1, 0, 1], 'constant', 0.0);  gelu_1 = None\n",
       "                    conv2d_3: \"f32[s77, 256, 128, 128]\" = torch.ops.aten.conv2d.default(pad_2, p_model_encoder_model_stages_0_blocks_0_conv_conv2_kxk_weight, None, [2, 2], [0, 0], [1, 1], 256);  pad_2 = p_model_encoder_model_stages_0_blocks_0_conv_conv2_kxk_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_3 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_3, p_model_encoder_model_stages_0_blocks_0_conv_norm2_weight, p_model_encoder_model_stages_0_blocks_0_conv_norm2_bias, b_model_encoder_model_stages_0_blocks_0_conv_norm2_running_mean, b_model_encoder_model_stages_0_blocks_0_conv_norm2_running_var, 0.1, 0.001);  conv2d_3 = p_model_encoder_model_stages_0_blocks_0_conv_norm2_weight = p_model_encoder_model_stages_0_blocks_0_conv_norm2_bias = b_model_encoder_model_stages_0_blocks_0_conv_norm2_running_mean = b_model_encoder_model_stages_0_blocks_0_conv_norm2_running_var = None\n",
       "                    getitem_9: \"f32[s77, 256, 128, 128]\" = _native_batch_norm_legit_no_training_3[0];  _native_batch_norm_legit_no_training_3 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_2: \"f32[s77, 256, 128, 128]\" = torch.ops.aten.gelu.default(getitem_9, approximate = 'tanh');  getitem_9 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:56 in forward, code: x_se = x.mean((2, 3), keepdim=True)\n",
       "                    mean: \"f32[s77, 256, 1, 1]\" = torch.ops.aten.mean.dim(gelu_2, [2, 3], True)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_4: \"f32[s77, 16, 1, 1]\" = torch.ops.aten.conv2d.default(mean, p_model_encoder_model_stages_0_blocks_0_conv_se_fc1_weight, p_model_encoder_model_stages_0_blocks_0_conv_se_fc1_bias);  mean = p_model_encoder_model_stages_0_blocks_0_conv_se_fc1_weight = p_model_encoder_model_stages_0_blocks_0_conv_se_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:473 in forward, code: return F.silu(input, inplace=self.inplace)\n",
       "                    silu: \"f32[s77, 16, 1, 1]\" = torch.ops.aten.silu.default(conv2d_4);  conv2d_4 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_5: \"f32[s77, 256, 1, 1]\" = torch.ops.aten.conv2d.default(silu, p_model_encoder_model_stages_0_blocks_0_conv_se_fc2_weight, p_model_encoder_model_stages_0_blocks_0_conv_se_fc2_bias);  silu = p_model_encoder_model_stages_0_blocks_0_conv_se_fc2_weight = p_model_encoder_model_stages_0_blocks_0_conv_se_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:57 in forward, code: return x.sigmoid_() if self.inplace else x.sigmoid()\n",
       "                    sigmoid: \"f32[s77, 256, 1, 1]\" = torch.ops.aten.sigmoid.default(conv2d_5);  conv2d_5 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:63 in forward, code: return x * self.gate(x_se)\n",
       "                    mul_126: \"f32[s77, 256, 128, 128]\" = torch.ops.aten.mul.Tensor(gelu_2, sigmoid);  gelu_2 = sigmoid = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_6: \"f32[s77, 64, 128, 128]\" = torch.ops.aten.conv2d.default(mul_126, p_model_encoder_model_stages_0_blocks_0_conv_conv3_1x1_weight, p_model_encoder_model_stages_0_blocks_0_conv_conv3_1x1_bias);  mul_126 = p_model_encoder_model_stages_0_blocks_0_conv_conv3_1x1_weight = p_model_encoder_model_stages_0_blocks_0_conv_conv3_1x1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:635 in forward, code: x = self.drop_path(x) + shortcut\n",
       "                    add_280: \"f32[s77, 64, 128, 128]\" = torch.ops.aten.add.Tensor(conv2d_6, avg_pool2d);  conv2d_6 = avg_pool2d = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1117 in forward, code: x = x.permute(0, 2, 3, 1)  # to NHWC (channels-last)\n",
       "                    permute: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.permute.default(add_280, [0, 2, 3, 1]);  add_280 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.layer_norm.default(permute, [64], p_model_encoder_model_stages_0_blocks_0_attn_block_norm1_weight, p_model_encoder_model_stages_0_blocks_0_attn_block_norm1_bias);  p_model_encoder_model_stages_0_blocks_0_attn_block_norm1_weight = p_model_encoder_model_stages_0_blocks_0_attn_block_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view: \"f32[s77, 8, 16, 8, 16, 64]\" = torch.ops.aten.view.default(layer_norm, [sym_size_int_37, 8, 16, 8, 16, 64]);  layer_norm = None\n",
       "                    permute_1: \"f32[s77, 8, 8, 16, 16, 64]\" = torch.ops.aten.permute.default(view, [0, 1, 3, 2, 4, 5]);  view = None\n",
       "                    clone: \"f32[s77, 8, 8, 16, 16, 64]\" = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
       "                    view_1: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten.view.default(clone, [-1, 16, 16, 64]);  clone = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear: \"f32[64*s77, 16, 16, 192]\" = torch.ops.aten.linear.default(view_1, p_model_encoder_model_stages_0_blocks_0_attn_block_attn_qkv_weight, p_model_encoder_model_stages_0_blocks_0_attn_block_attn_qkv_bias);  view_1 = p_model_encoder_model_stages_0_blocks_0_attn_block_attn_qkv_weight = p_model_encoder_model_stages_0_blocks_0_attn_block_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    mul_153: \"Sym(64*s77)\" = 64 * sym_size_int_37\n",
       "                    view_2: \"f32[64*s77, 256, 3, 2, 32]\" = torch.ops.aten.view.default(linear, [mul_153, -1, 3, 2, 32]);  linear = None\n",
       "                    transpose: \"f32[64*s77, 2, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_2, 1, 3);  view_2 = None\n",
       "                    unbind = torch.ops.aten.unbind.int(transpose, 2);  transpose = None\n",
       "                    getitem_12: \"f32[64*s77, 2, 256, 32]\" = unbind[0]\n",
       "                    getitem_13: \"f32[64*s77, 2, 256, 32]\" = unbind[1]\n",
       "                    getitem_14: \"f32[64*s77, 2, 256, 32]\" = unbind[2];  unbind = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum: \"f32[2, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_0_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_0_blocks_0_attn_block_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_0_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_0_blocks_0_attn_block_attn_rel_pos_height_lookup = None\n",
       "                    einsum_1: \"f32[2, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum, b_model_encoder_model_stages_0_blocks_0_attn_block_attn_rel_pos_width_lookup]);  einsum = b_model_encoder_model_stages_0_blocks_0_attn_block_attn_rel_pos_width_lookup = None\n",
       "                    clone_1: \"f32[2, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_1, memory_format = torch.contiguous_format);  einsum_1 = None\n",
       "                    _unsafe_view: \"f32[2, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_1, [2, 256, 256]);  clone_1 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention: \"f32[64*s77, 2, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_12, getitem_13, getitem_14, _unsafe_view);  getitem_12 = getitem_13 = getitem_14 = _unsafe_view = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_1: \"f32[64*s77, 256, 2, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None\n",
       "                    clone_2: \"f32[64*s77, 256, 2, 32]\" = torch.ops.aten.clone.default(transpose_1, memory_format = torch.contiguous_format);  transpose_1 = None\n",
       "                    _unsafe_view_1: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten._unsafe_view.default(clone_2, [mul_153, 16, 16, 64]);  clone_2 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_1: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten.linear.default(_unsafe_view_1, p_model_encoder_model_stages_0_blocks_0_attn_block_attn_proj_weight, p_model_encoder_model_stages_0_blocks_0_attn_block_attn_proj_bias);  _unsafe_view_1 = p_model_encoder_model_stages_0_blocks_0_attn_block_attn_proj_weight = p_model_encoder_model_stages_0_blocks_0_attn_block_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_3: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten.clone.default(linear_1);  linear_1 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_3: \"f32[s77, 8, 8, 16, 16, 64]\" = torch.ops.aten.view.default(clone_3, [-1, 8, 8, 16, 16, 64]);  clone_3 = None\n",
       "                    permute_2: \"f32[s77, 8, 16, 8, 16, 64]\" = torch.ops.aten.permute.default(view_3, [0, 1, 3, 2, 4, 5]);  view_3 = None\n",
       "                    clone_4: \"f32[s77, 8, 16, 8, 16, 64]\" = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
       "                    view_4: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.view.default(clone_4, [-1, 128, 128, 64]);  clone_4 = None\n",
       "                    add_410: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.add.Tensor(permute, view_4);  permute = view_4 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_1: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.layer_norm.default(add_410, [64], p_model_encoder_model_stages_0_blocks_0_attn_block_norm2_weight, p_model_encoder_model_stages_0_blocks_0_attn_block_norm2_bias);  p_model_encoder_model_stages_0_blocks_0_attn_block_norm2_weight = p_model_encoder_model_stages_0_blocks_0_attn_block_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_2: \"f32[s77, 128, 128, 256]\" = torch.ops.aten.linear.default(layer_norm_1, p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc1_weight, p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc1_bias);  layer_norm_1 = p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc1_weight = p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_3: \"f32[s77, 128, 128, 256]\" = torch.ops.aten.gelu.default(linear_2, approximate = 'tanh');  linear_2 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_5: \"f32[s77, 128, 128, 256]\" = torch.ops.aten.clone.default(gelu_3);  gelu_3 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_3: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.linear.default(clone_5, p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc2_weight, p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc2_bias);  clone_5 = p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc2_weight = p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_6: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.clone.default(linear_3);  linear_3 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_446: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.add.Tensor(add_410, clone_6);  add_410 = clone_6 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_2: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.layer_norm.default(add_446, [64], p_model_encoder_model_stages_0_blocks_0_attn_grid_norm1_weight, p_model_encoder_model_stages_0_blocks_0_attn_grid_norm1_bias);  p_model_encoder_model_stages_0_blocks_0_attn_grid_norm1_weight = p_model_encoder_model_stages_0_blocks_0_attn_grid_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_5: \"f32[s77, 16, 8, 16, 8, 64]\" = torch.ops.aten.view.default(layer_norm_2, [sym_size_int_37, 16, 8, 16, 8, 64]);  layer_norm_2 = None\n",
       "                    permute_3: \"f32[s77, 8, 8, 16, 16, 64]\" = torch.ops.aten.permute.default(view_5, [0, 2, 4, 1, 3, 5]);  view_5 = None\n",
       "                    clone_7: \"f32[s77, 8, 8, 16, 16, 64]\" = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
       "                    view_6: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten.view.default(clone_7, [-1, 16, 16, 64]);  clone_7 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_4: \"f32[64*s77, 16, 16, 192]\" = torch.ops.aten.linear.default(view_6, p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_qkv_weight, p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_qkv_bias);  view_6 = p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_qkv_weight = p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_7: \"f32[64*s77, 256, 3, 2, 32]\" = torch.ops.aten.view.default(linear_4, [mul_153, -1, 3, 2, 32]);  linear_4 = None\n",
       "                    transpose_2: \"f32[64*s77, 2, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_7, 1, 3);  view_7 = None\n",
       "                    unbind_1 = torch.ops.aten.unbind.int(transpose_2, 2);  transpose_2 = None\n",
       "                    getitem_15: \"f32[64*s77, 2, 256, 32]\" = unbind_1[0]\n",
       "                    getitem_16: \"f32[64*s77, 2, 256, 32]\" = unbind_1[1]\n",
       "                    getitem_17: \"f32[64*s77, 2, 256, 32]\" = unbind_1[2];  unbind_1 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_2: \"f32[2, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_0_blocks_0_attn_grid_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_0_blocks_0_attn_grid_attn_rel_pos_height_lookup = None\n",
       "                    einsum_3: \"f32[2, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_2, b_model_encoder_model_stages_0_blocks_0_attn_grid_attn_rel_pos_width_lookup]);  einsum_2 = b_model_encoder_model_stages_0_blocks_0_attn_grid_attn_rel_pos_width_lookup = None\n",
       "                    clone_8: \"f32[2, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_3, memory_format = torch.contiguous_format);  einsum_3 = None\n",
       "                    _unsafe_view_2: \"f32[2, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_8, [2, 256, 256]);  clone_8 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_1: \"f32[64*s77, 2, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_15, getitem_16, getitem_17, _unsafe_view_2);  getitem_15 = getitem_16 = getitem_17 = _unsafe_view_2 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_3: \"f32[64*s77, 256, 2, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None\n",
       "                    clone_9: \"f32[64*s77, 256, 2, 32]\" = torch.ops.aten.clone.default(transpose_3, memory_format = torch.contiguous_format);  transpose_3 = None\n",
       "                    _unsafe_view_3: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten._unsafe_view.default(clone_9, [mul_153, 16, 16, 64]);  clone_9 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_5: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten.linear.default(_unsafe_view_3, p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_proj_weight, p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_proj_bias);  _unsafe_view_3 = p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_proj_weight = p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_10: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten.clone.default(linear_5);  linear_5 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_8: \"f32[s77, 8, 8, 16, 16, 64]\" = torch.ops.aten.view.default(clone_10, [-1, 8, 8, 16, 16, 64]);  clone_10 = None\n",
       "                    permute_4: \"f32[s77, 16, 8, 16, 8, 64]\" = torch.ops.aten.permute.default(view_8, [0, 3, 1, 4, 2, 5]);  view_8 = None\n",
       "                    clone_11: \"f32[s77, 16, 8, 16, 8, 64]\" = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
       "                    view_9: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.view.default(clone_11, [-1, 128, 128, 64]);  clone_11 = None\n",
       "                    add_571: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.add.Tensor(add_446, view_9);  add_446 = view_9 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_3: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.layer_norm.default(add_571, [64], p_model_encoder_model_stages_0_blocks_0_attn_grid_norm2_weight, p_model_encoder_model_stages_0_blocks_0_attn_grid_norm2_bias);  p_model_encoder_model_stages_0_blocks_0_attn_grid_norm2_weight = p_model_encoder_model_stages_0_blocks_0_attn_grid_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_6: \"f32[s77, 128, 128, 256]\" = torch.ops.aten.linear.default(layer_norm_3, p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc1_weight, p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc1_bias);  layer_norm_3 = p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc1_weight = p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_4: \"f32[s77, 128, 128, 256]\" = torch.ops.aten.gelu.default(linear_6, approximate = 'tanh');  linear_6 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_12: \"f32[s77, 128, 128, 256]\" = torch.ops.aten.clone.default(gelu_4);  gelu_4 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_7: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.linear.default(clone_12, p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc2_weight, p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc2_bias);  clone_12 = p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc2_weight = p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_13: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.clone.default(linear_7);  linear_7 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_607: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.add.Tensor(add_571, clone_13);  add_571 = clone_13 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1122 in forward, code: x = x.permute(0, 3, 1, 2)  # back to NCHW\n",
       "                    permute_5: \"f32[s77, 64, 128, 128]\" = torch.ops.aten.permute.default(add_607, [0, 3, 1, 2]);  add_607 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_4 = torch.ops.aten._native_batch_norm_legit_no_training.default(permute_5, p_model_encoder_model_stages_0_blocks_1_conv_pre_norm_weight, p_model_encoder_model_stages_0_blocks_1_conv_pre_norm_bias, b_model_encoder_model_stages_0_blocks_1_conv_pre_norm_running_mean, b_model_encoder_model_stages_0_blocks_1_conv_pre_norm_running_var, 0.1, 0.001);  p_model_encoder_model_stages_0_blocks_1_conv_pre_norm_weight = p_model_encoder_model_stages_0_blocks_1_conv_pre_norm_bias = b_model_encoder_model_stages_0_blocks_1_conv_pre_norm_running_mean = b_model_encoder_model_stages_0_blocks_1_conv_pre_norm_running_var = None\n",
       "                    getitem_18: \"f32[s77, 64, 128, 128]\" = _native_batch_norm_legit_no_training_4[0];  _native_batch_norm_legit_no_training_4 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_7: \"f32[s77, 256, 128, 128]\" = torch.ops.aten.conv2d.default(getitem_18, p_model_encoder_model_stages_0_blocks_1_conv_conv1_1x1_weight);  getitem_18 = p_model_encoder_model_stages_0_blocks_1_conv_conv1_1x1_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_5 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_7, p_model_encoder_model_stages_0_blocks_1_conv_norm1_weight, p_model_encoder_model_stages_0_blocks_1_conv_norm1_bias, b_model_encoder_model_stages_0_blocks_1_conv_norm1_running_mean, b_model_encoder_model_stages_0_blocks_1_conv_norm1_running_var, 0.1, 0.001);  conv2d_7 = p_model_encoder_model_stages_0_blocks_1_conv_norm1_weight = p_model_encoder_model_stages_0_blocks_1_conv_norm1_bias = b_model_encoder_model_stages_0_blocks_1_conv_norm1_running_mean = b_model_encoder_model_stages_0_blocks_1_conv_norm1_running_var = None\n",
       "                    getitem_21: \"f32[s77, 256, 128, 128]\" = _native_batch_norm_legit_no_training_5[0];  _native_batch_norm_legit_no_training_5 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_5: \"f32[s77, 256, 128, 128]\" = torch.ops.aten.gelu.default(getitem_21, approximate = 'tanh');  getitem_21 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_8: \"f32[s77, 256, 128, 128]\" = torch.ops.aten.conv2d.default(gelu_5, p_model_encoder_model_stages_0_blocks_1_conv_conv2_kxk_weight, None, [1, 1], [1, 1], [1, 1], 256);  gelu_5 = p_model_encoder_model_stages_0_blocks_1_conv_conv2_kxk_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_6 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_8, p_model_encoder_model_stages_0_blocks_1_conv_norm2_weight, p_model_encoder_model_stages_0_blocks_1_conv_norm2_bias, b_model_encoder_model_stages_0_blocks_1_conv_norm2_running_mean, b_model_encoder_model_stages_0_blocks_1_conv_norm2_running_var, 0.1, 0.001);  conv2d_8 = p_model_encoder_model_stages_0_blocks_1_conv_norm2_weight = p_model_encoder_model_stages_0_blocks_1_conv_norm2_bias = b_model_encoder_model_stages_0_blocks_1_conv_norm2_running_mean = b_model_encoder_model_stages_0_blocks_1_conv_norm2_running_var = None\n",
       "                    getitem_24: \"f32[s77, 256, 128, 128]\" = _native_batch_norm_legit_no_training_6[0];  _native_batch_norm_legit_no_training_6 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_6: \"f32[s77, 256, 128, 128]\" = torch.ops.aten.gelu.default(getitem_24, approximate = 'tanh');  getitem_24 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:56 in forward, code: x_se = x.mean((2, 3), keepdim=True)\n",
       "                    mean_1: \"f32[s77, 256, 1, 1]\" = torch.ops.aten.mean.dim(gelu_6, [2, 3], True)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_9: \"f32[s77, 16, 1, 1]\" = torch.ops.aten.conv2d.default(mean_1, p_model_encoder_model_stages_0_blocks_1_conv_se_fc1_weight, p_model_encoder_model_stages_0_blocks_1_conv_se_fc1_bias);  mean_1 = p_model_encoder_model_stages_0_blocks_1_conv_se_fc1_weight = p_model_encoder_model_stages_0_blocks_1_conv_se_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:473 in forward, code: return F.silu(input, inplace=self.inplace)\n",
       "                    silu_1: \"f32[s77, 16, 1, 1]\" = torch.ops.aten.silu.default(conv2d_9);  conv2d_9 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_10: \"f32[s77, 256, 1, 1]\" = torch.ops.aten.conv2d.default(silu_1, p_model_encoder_model_stages_0_blocks_1_conv_se_fc2_weight, p_model_encoder_model_stages_0_blocks_1_conv_se_fc2_bias);  silu_1 = p_model_encoder_model_stages_0_blocks_1_conv_se_fc2_weight = p_model_encoder_model_stages_0_blocks_1_conv_se_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:57 in forward, code: return x.sigmoid_() if self.inplace else x.sigmoid()\n",
       "                    sigmoid_1: \"f32[s77, 256, 1, 1]\" = torch.ops.aten.sigmoid.default(conv2d_10);  conv2d_10 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:63 in forward, code: return x * self.gate(x_se)\n",
       "                    mul_371: \"f32[s77, 256, 128, 128]\" = torch.ops.aten.mul.Tensor(gelu_6, sigmoid_1);  gelu_6 = sigmoid_1 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_11: \"f32[s77, 64, 128, 128]\" = torch.ops.aten.conv2d.default(mul_371, p_model_encoder_model_stages_0_blocks_1_conv_conv3_1x1_weight, p_model_encoder_model_stages_0_blocks_1_conv_conv3_1x1_bias);  mul_371 = p_model_encoder_model_stages_0_blocks_1_conv_conv3_1x1_weight = p_model_encoder_model_stages_0_blocks_1_conv_conv3_1x1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:635 in forward, code: x = self.drop_path(x) + shortcut\n",
       "                    add_694: \"f32[s77, 64, 128, 128]\" = torch.ops.aten.add.Tensor(conv2d_11, permute_5);  conv2d_11 = permute_5 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1117 in forward, code: x = x.permute(0, 2, 3, 1)  # to NHWC (channels-last)\n",
       "                    permute_6: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.permute.default(add_694, [0, 2, 3, 1]);  add_694 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_4: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.layer_norm.default(permute_6, [64], p_model_encoder_model_stages_0_blocks_1_attn_block_norm1_weight, p_model_encoder_model_stages_0_blocks_1_attn_block_norm1_bias);  p_model_encoder_model_stages_0_blocks_1_attn_block_norm1_weight = p_model_encoder_model_stages_0_blocks_1_attn_block_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_10: \"f32[s77, 8, 16, 8, 16, 64]\" = torch.ops.aten.view.default(layer_norm_4, [sym_size_int_37, 8, 16, 8, 16, 64]);  layer_norm_4 = None\n",
       "                    permute_7: \"f32[s77, 8, 8, 16, 16, 64]\" = torch.ops.aten.permute.default(view_10, [0, 1, 3, 2, 4, 5]);  view_10 = None\n",
       "                    clone_14: \"f32[s77, 8, 8, 16, 16, 64]\" = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
       "                    view_11: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten.view.default(clone_14, [-1, 16, 16, 64]);  clone_14 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_8: \"f32[64*s77, 16, 16, 192]\" = torch.ops.aten.linear.default(view_11, p_model_encoder_model_stages_0_blocks_1_attn_block_attn_qkv_weight, p_model_encoder_model_stages_0_blocks_1_attn_block_attn_qkv_bias);  view_11 = p_model_encoder_model_stages_0_blocks_1_attn_block_attn_qkv_weight = p_model_encoder_model_stages_0_blocks_1_attn_block_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_12: \"f32[64*s77, 256, 3, 2, 32]\" = torch.ops.aten.view.default(linear_8, [mul_153, -1, 3, 2, 32]);  linear_8 = None\n",
       "                    transpose_4: \"f32[64*s77, 2, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_12, 1, 3);  view_12 = None\n",
       "                    unbind_2 = torch.ops.aten.unbind.int(transpose_4, 2);  transpose_4 = None\n",
       "                    getitem_27: \"f32[64*s77, 2, 256, 32]\" = unbind_2[0]\n",
       "                    getitem_28: \"f32[64*s77, 2, 256, 32]\" = unbind_2[1]\n",
       "                    getitem_29: \"f32[64*s77, 2, 256, 32]\" = unbind_2[2];  unbind_2 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_4: \"f32[2, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_0_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_0_blocks_1_attn_block_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_0_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_0_blocks_1_attn_block_attn_rel_pos_height_lookup = None\n",
       "                    einsum_5: \"f32[2, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_4, b_model_encoder_model_stages_0_blocks_1_attn_block_attn_rel_pos_width_lookup]);  einsum_4 = b_model_encoder_model_stages_0_blocks_1_attn_block_attn_rel_pos_width_lookup = None\n",
       "                    clone_15: \"f32[2, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_5, memory_format = torch.contiguous_format);  einsum_5 = None\n",
       "                    _unsafe_view_4: \"f32[2, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_15, [2, 256, 256]);  clone_15 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_2: \"f32[64*s77, 2, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_27, getitem_28, getitem_29, _unsafe_view_4);  getitem_27 = getitem_28 = getitem_29 = _unsafe_view_4 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_5: \"f32[64*s77, 256, 2, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_2, 1, 2);  scaled_dot_product_attention_2 = None\n",
       "                    clone_16: \"f32[64*s77, 256, 2, 32]\" = torch.ops.aten.clone.default(transpose_5, memory_format = torch.contiguous_format);  transpose_5 = None\n",
       "                    _unsafe_view_5: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten._unsafe_view.default(clone_16, [mul_153, 16, 16, 64]);  clone_16 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_9: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten.linear.default(_unsafe_view_5, p_model_encoder_model_stages_0_blocks_1_attn_block_attn_proj_weight, p_model_encoder_model_stages_0_blocks_1_attn_block_attn_proj_bias);  _unsafe_view_5 = p_model_encoder_model_stages_0_blocks_1_attn_block_attn_proj_weight = p_model_encoder_model_stages_0_blocks_1_attn_block_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_17: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten.clone.default(linear_9);  linear_9 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_13: \"f32[s77, 8, 8, 16, 16, 64]\" = torch.ops.aten.view.default(clone_17, [-1, 8, 8, 16, 16, 64]);  clone_17 = None\n",
       "                    permute_8: \"f32[s77, 8, 16, 8, 16, 64]\" = torch.ops.aten.permute.default(view_13, [0, 1, 3, 2, 4, 5]);  view_13 = None\n",
       "                    clone_18: \"f32[s77, 8, 16, 8, 16, 64]\" = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
       "                    view_14: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.view.default(clone_18, [-1, 128, 128, 64]);  clone_18 = None\n",
       "                    add_824: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.add.Tensor(permute_6, view_14);  permute_6 = view_14 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_5: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.layer_norm.default(add_824, [64], p_model_encoder_model_stages_0_blocks_1_attn_block_norm2_weight, p_model_encoder_model_stages_0_blocks_1_attn_block_norm2_bias);  p_model_encoder_model_stages_0_blocks_1_attn_block_norm2_weight = p_model_encoder_model_stages_0_blocks_1_attn_block_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_10: \"f32[s77, 128, 128, 256]\" = torch.ops.aten.linear.default(layer_norm_5, p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc1_weight, p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc1_bias);  layer_norm_5 = p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc1_weight = p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_7: \"f32[s77, 128, 128, 256]\" = torch.ops.aten.gelu.default(linear_10, approximate = 'tanh');  linear_10 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_19: \"f32[s77, 128, 128, 256]\" = torch.ops.aten.clone.default(gelu_7);  gelu_7 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_11: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.linear.default(clone_19, p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc2_weight, p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc2_bias);  clone_19 = p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc2_weight = p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_20: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.clone.default(linear_11);  linear_11 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_860: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.add.Tensor(add_824, clone_20);  add_824 = clone_20 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_6: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.layer_norm.default(add_860, [64], p_model_encoder_model_stages_0_blocks_1_attn_grid_norm1_weight, p_model_encoder_model_stages_0_blocks_1_attn_grid_norm1_bias);  p_model_encoder_model_stages_0_blocks_1_attn_grid_norm1_weight = p_model_encoder_model_stages_0_blocks_1_attn_grid_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_15: \"f32[s77, 16, 8, 16, 8, 64]\" = torch.ops.aten.view.default(layer_norm_6, [sym_size_int_37, 16, 8, 16, 8, 64]);  layer_norm_6 = None\n",
       "                    permute_9: \"f32[s77, 8, 8, 16, 16, 64]\" = torch.ops.aten.permute.default(view_15, [0, 2, 4, 1, 3, 5]);  view_15 = None\n",
       "                    clone_21: \"f32[s77, 8, 8, 16, 16, 64]\" = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
       "                    view_16: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten.view.default(clone_21, [-1, 16, 16, 64]);  clone_21 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_12: \"f32[64*s77, 16, 16, 192]\" = torch.ops.aten.linear.default(view_16, p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_qkv_weight, p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_qkv_bias);  view_16 = p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_qkv_weight = p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_17: \"f32[64*s77, 256, 3, 2, 32]\" = torch.ops.aten.view.default(linear_12, [mul_153, -1, 3, 2, 32]);  linear_12 = None\n",
       "                    transpose_6: \"f32[64*s77, 2, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_17, 1, 3);  view_17 = None\n",
       "                    unbind_3 = torch.ops.aten.unbind.int(transpose_6, 2);  transpose_6 = None\n",
       "                    getitem_30: \"f32[64*s77, 2, 256, 32]\" = unbind_3[0]\n",
       "                    getitem_31: \"f32[64*s77, 2, 256, 32]\" = unbind_3[1]\n",
       "                    getitem_32: \"f32[64*s77, 2, 256, 32]\" = unbind_3[2];  unbind_3 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_6: \"f32[2, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_0_blocks_1_attn_grid_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_0_blocks_1_attn_grid_attn_rel_pos_height_lookup = None\n",
       "                    einsum_7: \"f32[2, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_6, b_model_encoder_model_stages_0_blocks_1_attn_grid_attn_rel_pos_width_lookup]);  einsum_6 = b_model_encoder_model_stages_0_blocks_1_attn_grid_attn_rel_pos_width_lookup = None\n",
       "                    clone_22: \"f32[2, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_7, memory_format = torch.contiguous_format);  einsum_7 = None\n",
       "                    _unsafe_view_6: \"f32[2, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_22, [2, 256, 256]);  clone_22 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_3: \"f32[64*s77, 2, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_30, getitem_31, getitem_32, _unsafe_view_6);  getitem_30 = getitem_31 = getitem_32 = _unsafe_view_6 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_7: \"f32[64*s77, 256, 2, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_3, 1, 2);  scaled_dot_product_attention_3 = None\n",
       "                    clone_23: \"f32[64*s77, 256, 2, 32]\" = torch.ops.aten.clone.default(transpose_7, memory_format = torch.contiguous_format);  transpose_7 = None\n",
       "                    _unsafe_view_7: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten._unsafe_view.default(clone_23, [mul_153, 16, 16, 64]);  clone_23 = mul_153 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_13: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten.linear.default(_unsafe_view_7, p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_proj_weight, p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_proj_bias);  _unsafe_view_7 = p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_proj_weight = p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_24: \"f32[64*s77, 16, 16, 64]\" = torch.ops.aten.clone.default(linear_13);  linear_13 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_18: \"f32[s77, 8, 8, 16, 16, 64]\" = torch.ops.aten.view.default(clone_24, [-1, 8, 8, 16, 16, 64]);  clone_24 = None\n",
       "                    permute_10: \"f32[s77, 16, 8, 16, 8, 64]\" = torch.ops.aten.permute.default(view_18, [0, 3, 1, 4, 2, 5]);  view_18 = None\n",
       "                    clone_25: \"f32[s77, 16, 8, 16, 8, 64]\" = torch.ops.aten.clone.default(permute_10, memory_format = torch.contiguous_format);  permute_10 = None\n",
       "                    view_19: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.view.default(clone_25, [-1, 128, 128, 64]);  clone_25 = None\n",
       "                    add_985: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.add.Tensor(add_860, view_19);  add_860 = view_19 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_7: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.layer_norm.default(add_985, [64], p_model_encoder_model_stages_0_blocks_1_attn_grid_norm2_weight, p_model_encoder_model_stages_0_blocks_1_attn_grid_norm2_bias);  p_model_encoder_model_stages_0_blocks_1_attn_grid_norm2_weight = p_model_encoder_model_stages_0_blocks_1_attn_grid_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_14: \"f32[s77, 128, 128, 256]\" = torch.ops.aten.linear.default(layer_norm_7, p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc1_weight, p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc1_bias);  layer_norm_7 = p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc1_weight = p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_8: \"f32[s77, 128, 128, 256]\" = torch.ops.aten.gelu.default(linear_14, approximate = 'tanh');  linear_14 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_26: \"f32[s77, 128, 128, 256]\" = torch.ops.aten.clone.default(gelu_8);  gelu_8 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_15: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.linear.default(clone_26, p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc2_weight, p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc2_bias);  clone_26 = p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc2_weight = p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_27: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.clone.default(linear_15);  linear_15 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1021: \"f32[s77, 128, 128, 64]\" = torch.ops.aten.add.Tensor(add_985, clone_27);  add_985 = clone_27 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1122 in forward, code: x = x.permute(0, 3, 1, 2)  # back to NCHW\n",
       "                    permute_11: \"f32[s77, 64, 128, 128]\" = torch.ops.aten.permute.default(add_1021, [0, 3, 1, 2]);  add_1021 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/pool2d_same.py:45 in forward, code: x = pad_same(x, self.kernel_size, self.stride)\n",
       "                    pad_3: \"f32[s77, 64, 128, 128]\" = torch.ops.aten.pad.default(permute_11, [0, 0, 0, 0], 'constant', 0.0)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/pool2d_same.py:46 in forward, code: return F.avg_pool2d(\n",
       "                    avg_pool2d_1: \"f32[s77, 64, 64, 64]\" = torch.ops.aten.avg_pool2d.default(pad_3, [2, 2], [2, 2]);  pad_3 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_12: \"f32[s77, 128, 64, 64]\" = torch.ops.aten.conv2d.default(avg_pool2d_1, p_model_encoder_model_stages_1_blocks_0_conv_shortcut_expand_weight, p_model_encoder_model_stages_1_blocks_0_conv_shortcut_expand_bias);  avg_pool2d_1 = p_model_encoder_model_stages_1_blocks_0_conv_shortcut_expand_weight = p_model_encoder_model_stages_1_blocks_0_conv_shortcut_expand_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_7 = torch.ops.aten._native_batch_norm_legit_no_training.default(permute_11, p_model_encoder_model_stages_1_blocks_0_conv_pre_norm_weight, p_model_encoder_model_stages_1_blocks_0_conv_pre_norm_bias, b_model_encoder_model_stages_1_blocks_0_conv_pre_norm_running_mean, b_model_encoder_model_stages_1_blocks_0_conv_pre_norm_running_var, 0.1, 0.001);  p_model_encoder_model_stages_1_blocks_0_conv_pre_norm_weight = p_model_encoder_model_stages_1_blocks_0_conv_pre_norm_bias = b_model_encoder_model_stages_1_blocks_0_conv_pre_norm_running_mean = b_model_encoder_model_stages_1_blocks_0_conv_pre_norm_running_var = None\n",
       "                    getitem_33: \"f32[s77, 64, 128, 128]\" = _native_batch_norm_legit_no_training_7[0];  _native_batch_norm_legit_no_training_7 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_13: \"f32[s77, 512, 128, 128]\" = torch.ops.aten.conv2d.default(getitem_33, p_model_encoder_model_stages_1_blocks_0_conv_conv1_1x1_weight);  getitem_33 = p_model_encoder_model_stages_1_blocks_0_conv_conv1_1x1_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_8 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_13, p_model_encoder_model_stages_1_blocks_0_conv_norm1_weight, p_model_encoder_model_stages_1_blocks_0_conv_norm1_bias, b_model_encoder_model_stages_1_blocks_0_conv_norm1_running_mean, b_model_encoder_model_stages_1_blocks_0_conv_norm1_running_var, 0.1, 0.001);  conv2d_13 = p_model_encoder_model_stages_1_blocks_0_conv_norm1_weight = p_model_encoder_model_stages_1_blocks_0_conv_norm1_bias = b_model_encoder_model_stages_1_blocks_0_conv_norm1_running_mean = b_model_encoder_model_stages_1_blocks_0_conv_norm1_running_var = None\n",
       "                    getitem_36: \"f32[s77, 512, 128, 128]\" = _native_batch_norm_legit_no_training_8[0];  _native_batch_norm_legit_no_training_8 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_9: \"f32[s77, 512, 128, 128]\" = torch.ops.aten.gelu.default(getitem_36, approximate = 'tanh');  getitem_36 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/conv2d_same.py:63 in forward, code: return conv2d_same(\n",
       "                    pad_4: \"f32[s77, 512, 129, 129]\" = torch.ops.aten.pad.default(gelu_9, [0, 1, 0, 1], 'constant', 0.0);  gelu_9 = None\n",
       "                    conv2d_14: \"f32[s77, 512, 64, 64]\" = torch.ops.aten.conv2d.default(pad_4, p_model_encoder_model_stages_1_blocks_0_conv_conv2_kxk_weight, None, [2, 2], [0, 0], [1, 1], 512);  pad_4 = p_model_encoder_model_stages_1_blocks_0_conv_conv2_kxk_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_9 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_14, p_model_encoder_model_stages_1_blocks_0_conv_norm2_weight, p_model_encoder_model_stages_1_blocks_0_conv_norm2_bias, b_model_encoder_model_stages_1_blocks_0_conv_norm2_running_mean, b_model_encoder_model_stages_1_blocks_0_conv_norm2_running_var, 0.1, 0.001);  conv2d_14 = p_model_encoder_model_stages_1_blocks_0_conv_norm2_weight = p_model_encoder_model_stages_1_blocks_0_conv_norm2_bias = b_model_encoder_model_stages_1_blocks_0_conv_norm2_running_mean = b_model_encoder_model_stages_1_blocks_0_conv_norm2_running_var = None\n",
       "                    getitem_39: \"f32[s77, 512, 64, 64]\" = _native_batch_norm_legit_no_training_9[0];  _native_batch_norm_legit_no_training_9 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_10: \"f32[s77, 512, 64, 64]\" = torch.ops.aten.gelu.default(getitem_39, approximate = 'tanh');  getitem_39 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:56 in forward, code: x_se = x.mean((2, 3), keepdim=True)\n",
       "                    mean_2: \"f32[s77, 512, 1, 1]\" = torch.ops.aten.mean.dim(gelu_10, [2, 3], True)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_15: \"f32[s77, 32, 1, 1]\" = torch.ops.aten.conv2d.default(mean_2, p_model_encoder_model_stages_1_blocks_0_conv_se_fc1_weight, p_model_encoder_model_stages_1_blocks_0_conv_se_fc1_bias);  mean_2 = p_model_encoder_model_stages_1_blocks_0_conv_se_fc1_weight = p_model_encoder_model_stages_1_blocks_0_conv_se_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:473 in forward, code: return F.silu(input, inplace=self.inplace)\n",
       "                    silu_2: \"f32[s77, 32, 1, 1]\" = torch.ops.aten.silu.default(conv2d_15);  conv2d_15 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_16: \"f32[s77, 512, 1, 1]\" = torch.ops.aten.conv2d.default(silu_2, p_model_encoder_model_stages_1_blocks_0_conv_se_fc2_weight, p_model_encoder_model_stages_1_blocks_0_conv_se_fc2_bias);  silu_2 = p_model_encoder_model_stages_1_blocks_0_conv_se_fc2_weight = p_model_encoder_model_stages_1_blocks_0_conv_se_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:57 in forward, code: return x.sigmoid_() if self.inplace else x.sigmoid()\n",
       "                    sigmoid_2: \"f32[s77, 512, 1, 1]\" = torch.ops.aten.sigmoid.default(conv2d_16);  conv2d_16 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:63 in forward, code: return x * self.gate(x_se)\n",
       "                    mul_623: \"f32[s77, 512, 64, 64]\" = torch.ops.aten.mul.Tensor(gelu_10, sigmoid_2);  gelu_10 = sigmoid_2 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_17: \"f32[s77, 128, 64, 64]\" = torch.ops.aten.conv2d.default(mul_623, p_model_encoder_model_stages_1_blocks_0_conv_conv3_1x1_weight, p_model_encoder_model_stages_1_blocks_0_conv_conv3_1x1_bias);  mul_623 = p_model_encoder_model_stages_1_blocks_0_conv_conv3_1x1_weight = p_model_encoder_model_stages_1_blocks_0_conv_conv3_1x1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:635 in forward, code: x = self.drop_path(x) + shortcut\n",
       "                    add_1128: \"f32[s77, 128, 64, 64]\" = torch.ops.aten.add.Tensor(conv2d_17, conv2d_12);  conv2d_17 = conv2d_12 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1117 in forward, code: x = x.permute(0, 2, 3, 1)  # to NHWC (channels-last)\n",
       "                    permute_12: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.permute.default(add_1128, [0, 2, 3, 1]);  add_1128 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_8: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.layer_norm.default(permute_12, [128], p_model_encoder_model_stages_1_blocks_0_attn_block_norm1_weight, p_model_encoder_model_stages_1_blocks_0_attn_block_norm1_bias);  p_model_encoder_model_stages_1_blocks_0_attn_block_norm1_weight = p_model_encoder_model_stages_1_blocks_0_attn_block_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_20: \"f32[s77, 4, 16, 4, 16, 128]\" = torch.ops.aten.view.default(layer_norm_8, [sym_size_int_37, 4, 16, 4, 16, 128]);  layer_norm_8 = None\n",
       "                    permute_13: \"f32[s77, 4, 4, 16, 16, 128]\" = torch.ops.aten.permute.default(view_20, [0, 1, 3, 2, 4, 5]);  view_20 = None\n",
       "                    clone_28: \"f32[s77, 4, 4, 16, 16, 128]\" = torch.ops.aten.clone.default(permute_13, memory_format = torch.contiguous_format);  permute_13 = None\n",
       "                    view_21: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten.view.default(clone_28, [-1, 16, 16, 128]);  clone_28 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_16: \"f32[16*s77, 16, 16, 384]\" = torch.ops.aten.linear.default(view_21, p_model_encoder_model_stages_1_blocks_0_attn_block_attn_qkv_weight, p_model_encoder_model_stages_1_blocks_0_attn_block_attn_qkv_bias);  view_21 = p_model_encoder_model_stages_1_blocks_0_attn_block_attn_qkv_weight = p_model_encoder_model_stages_1_blocks_0_attn_block_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    mul_650: \"Sym(16*s77)\" = 16 * sym_size_int_37\n",
       "                    view_22: \"f32[16*s77, 256, 3, 4, 32]\" = torch.ops.aten.view.default(linear_16, [mul_650, -1, 3, 4, 32]);  linear_16 = None\n",
       "                    transpose_8: \"f32[16*s77, 4, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_22, 1, 3);  view_22 = None\n",
       "                    unbind_4 = torch.ops.aten.unbind.int(transpose_8, 2);  transpose_8 = None\n",
       "                    getitem_42: \"f32[16*s77, 4, 256, 32]\" = unbind_4[0]\n",
       "                    getitem_43: \"f32[16*s77, 4, 256, 32]\" = unbind_4[1]\n",
       "                    getitem_44: \"f32[16*s77, 4, 256, 32]\" = unbind_4[2];  unbind_4 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_8: \"f32[4, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_1_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_1_blocks_0_attn_block_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_1_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_1_blocks_0_attn_block_attn_rel_pos_height_lookup = None\n",
       "                    einsum_9: \"f32[4, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_8, b_model_encoder_model_stages_1_blocks_0_attn_block_attn_rel_pos_width_lookup]);  einsum_8 = b_model_encoder_model_stages_1_blocks_0_attn_block_attn_rel_pos_width_lookup = None\n",
       "                    clone_29: \"f32[4, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_9, memory_format = torch.contiguous_format);  einsum_9 = None\n",
       "                    _unsafe_view_8: \"f32[4, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_29, [4, 256, 256]);  clone_29 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_4: \"f32[16*s77, 4, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_42, getitem_43, getitem_44, _unsafe_view_8);  getitem_42 = getitem_43 = getitem_44 = _unsafe_view_8 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_9: \"f32[16*s77, 256, 4, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_4, 1, 2);  scaled_dot_product_attention_4 = None\n",
       "                    clone_30: \"f32[16*s77, 256, 4, 32]\" = torch.ops.aten.clone.default(transpose_9, memory_format = torch.contiguous_format);  transpose_9 = None\n",
       "                    _unsafe_view_9: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten._unsafe_view.default(clone_30, [mul_650, 16, 16, 128]);  clone_30 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_17: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten.linear.default(_unsafe_view_9, p_model_encoder_model_stages_1_blocks_0_attn_block_attn_proj_weight, p_model_encoder_model_stages_1_blocks_0_attn_block_attn_proj_bias);  _unsafe_view_9 = p_model_encoder_model_stages_1_blocks_0_attn_block_attn_proj_weight = p_model_encoder_model_stages_1_blocks_0_attn_block_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_31: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten.clone.default(linear_17);  linear_17 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_23: \"f32[s77, 4, 4, 16, 16, 128]\" = torch.ops.aten.view.default(clone_31, [-1, 4, 4, 16, 16, 128]);  clone_31 = None\n",
       "                    permute_14: \"f32[s77, 4, 16, 4, 16, 128]\" = torch.ops.aten.permute.default(view_23, [0, 1, 3, 2, 4, 5]);  view_23 = None\n",
       "                    clone_32: \"f32[s77, 4, 16, 4, 16, 128]\" = torch.ops.aten.clone.default(permute_14, memory_format = torch.contiguous_format);  permute_14 = None\n",
       "                    view_24: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.view.default(clone_32, [-1, 64, 64, 128]);  clone_32 = None\n",
       "                    add_1258: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.add.Tensor(permute_12, view_24);  permute_12 = view_24 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_9: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.layer_norm.default(add_1258, [128], p_model_encoder_model_stages_1_blocks_0_attn_block_norm2_weight, p_model_encoder_model_stages_1_blocks_0_attn_block_norm2_bias);  p_model_encoder_model_stages_1_blocks_0_attn_block_norm2_weight = p_model_encoder_model_stages_1_blocks_0_attn_block_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_18: \"f32[s77, 64, 64, 512]\" = torch.ops.aten.linear.default(layer_norm_9, p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc1_weight, p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc1_bias);  layer_norm_9 = p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc1_weight = p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_11: \"f32[s77, 64, 64, 512]\" = torch.ops.aten.gelu.default(linear_18, approximate = 'tanh');  linear_18 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_33: \"f32[s77, 64, 64, 512]\" = torch.ops.aten.clone.default(gelu_11);  gelu_11 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_19: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.linear.default(clone_33, p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc2_weight, p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc2_bias);  clone_33 = p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc2_weight = p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_34: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.clone.default(linear_19);  linear_19 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1294: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.add.Tensor(add_1258, clone_34);  add_1258 = clone_34 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_10: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.layer_norm.default(add_1294, [128], p_model_encoder_model_stages_1_blocks_0_attn_grid_norm1_weight, p_model_encoder_model_stages_1_blocks_0_attn_grid_norm1_bias);  p_model_encoder_model_stages_1_blocks_0_attn_grid_norm1_weight = p_model_encoder_model_stages_1_blocks_0_attn_grid_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_25: \"f32[s77, 16, 4, 16, 4, 128]\" = torch.ops.aten.view.default(layer_norm_10, [sym_size_int_37, 16, 4, 16, 4, 128]);  layer_norm_10 = None\n",
       "                    permute_15: \"f32[s77, 4, 4, 16, 16, 128]\" = torch.ops.aten.permute.default(view_25, [0, 2, 4, 1, 3, 5]);  view_25 = None\n",
       "                    clone_35: \"f32[s77, 4, 4, 16, 16, 128]\" = torch.ops.aten.clone.default(permute_15, memory_format = torch.contiguous_format);  permute_15 = None\n",
       "                    view_26: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten.view.default(clone_35, [-1, 16, 16, 128]);  clone_35 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_20: \"f32[16*s77, 16, 16, 384]\" = torch.ops.aten.linear.default(view_26, p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_qkv_weight, p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_qkv_bias);  view_26 = p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_qkv_weight = p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_27: \"f32[16*s77, 256, 3, 4, 32]\" = torch.ops.aten.view.default(linear_20, [mul_650, -1, 3, 4, 32]);  linear_20 = None\n",
       "                    transpose_10: \"f32[16*s77, 4, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_27, 1, 3);  view_27 = None\n",
       "                    unbind_5 = torch.ops.aten.unbind.int(transpose_10, 2);  transpose_10 = None\n",
       "                    getitem_45: \"f32[16*s77, 4, 256, 32]\" = unbind_5[0]\n",
       "                    getitem_46: \"f32[16*s77, 4, 256, 32]\" = unbind_5[1]\n",
       "                    getitem_47: \"f32[16*s77, 4, 256, 32]\" = unbind_5[2];  unbind_5 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_10: \"f32[4, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_1_blocks_0_attn_grid_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_1_blocks_0_attn_grid_attn_rel_pos_height_lookup = None\n",
       "                    einsum_11: \"f32[4, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_10, b_model_encoder_model_stages_1_blocks_0_attn_grid_attn_rel_pos_width_lookup]);  einsum_10 = b_model_encoder_model_stages_1_blocks_0_attn_grid_attn_rel_pos_width_lookup = None\n",
       "                    clone_36: \"f32[4, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_11, memory_format = torch.contiguous_format);  einsum_11 = None\n",
       "                    _unsafe_view_10: \"f32[4, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_36, [4, 256, 256]);  clone_36 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_5: \"f32[16*s77, 4, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_45, getitem_46, getitem_47, _unsafe_view_10);  getitem_45 = getitem_46 = getitem_47 = _unsafe_view_10 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_11: \"f32[16*s77, 256, 4, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_5, 1, 2);  scaled_dot_product_attention_5 = None\n",
       "                    clone_37: \"f32[16*s77, 256, 4, 32]\" = torch.ops.aten.clone.default(transpose_11, memory_format = torch.contiguous_format);  transpose_11 = None\n",
       "                    _unsafe_view_11: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten._unsafe_view.default(clone_37, [mul_650, 16, 16, 128]);  clone_37 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_21: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten.linear.default(_unsafe_view_11, p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_proj_weight, p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_proj_bias);  _unsafe_view_11 = p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_proj_weight = p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_38: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten.clone.default(linear_21);  linear_21 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_28: \"f32[s77, 4, 4, 16, 16, 128]\" = torch.ops.aten.view.default(clone_38, [-1, 4, 4, 16, 16, 128]);  clone_38 = None\n",
       "                    permute_16: \"f32[s77, 16, 4, 16, 4, 128]\" = torch.ops.aten.permute.default(view_28, [0, 3, 1, 4, 2, 5]);  view_28 = None\n",
       "                    clone_39: \"f32[s77, 16, 4, 16, 4, 128]\" = torch.ops.aten.clone.default(permute_16, memory_format = torch.contiguous_format);  permute_16 = None\n",
       "                    view_29: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.view.default(clone_39, [-1, 64, 64, 128]);  clone_39 = None\n",
       "                    add_1419: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.add.Tensor(add_1294, view_29);  add_1294 = view_29 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_11: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.layer_norm.default(add_1419, [128], p_model_encoder_model_stages_1_blocks_0_attn_grid_norm2_weight, p_model_encoder_model_stages_1_blocks_0_attn_grid_norm2_bias);  p_model_encoder_model_stages_1_blocks_0_attn_grid_norm2_weight = p_model_encoder_model_stages_1_blocks_0_attn_grid_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_22: \"f32[s77, 64, 64, 512]\" = torch.ops.aten.linear.default(layer_norm_11, p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc1_weight, p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc1_bias);  layer_norm_11 = p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc1_weight = p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_12: \"f32[s77, 64, 64, 512]\" = torch.ops.aten.gelu.default(linear_22, approximate = 'tanh');  linear_22 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_40: \"f32[s77, 64, 64, 512]\" = torch.ops.aten.clone.default(gelu_12);  gelu_12 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_23: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.linear.default(clone_40, p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc2_weight, p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc2_bias);  clone_40 = p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc2_weight = p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_41: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.clone.default(linear_23);  linear_23 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1455: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.add.Tensor(add_1419, clone_41);  add_1419 = clone_41 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1122 in forward, code: x = x.permute(0, 3, 1, 2)  # back to NCHW\n",
       "                    permute_17: \"f32[s77, 128, 64, 64]\" = torch.ops.aten.permute.default(add_1455, [0, 3, 1, 2]);  add_1455 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_10 = torch.ops.aten._native_batch_norm_legit_no_training.default(permute_17, p_model_encoder_model_stages_1_blocks_1_conv_pre_norm_weight, p_model_encoder_model_stages_1_blocks_1_conv_pre_norm_bias, b_model_encoder_model_stages_1_blocks_1_conv_pre_norm_running_mean, b_model_encoder_model_stages_1_blocks_1_conv_pre_norm_running_var, 0.1, 0.001);  p_model_encoder_model_stages_1_blocks_1_conv_pre_norm_weight = p_model_encoder_model_stages_1_blocks_1_conv_pre_norm_bias = b_model_encoder_model_stages_1_blocks_1_conv_pre_norm_running_mean = b_model_encoder_model_stages_1_blocks_1_conv_pre_norm_running_var = None\n",
       "                    getitem_48: \"f32[s77, 128, 64, 64]\" = _native_batch_norm_legit_no_training_10[0];  _native_batch_norm_legit_no_training_10 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_18: \"f32[s77, 512, 64, 64]\" = torch.ops.aten.conv2d.default(getitem_48, p_model_encoder_model_stages_1_blocks_1_conv_conv1_1x1_weight);  getitem_48 = p_model_encoder_model_stages_1_blocks_1_conv_conv1_1x1_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_11 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_18, p_model_encoder_model_stages_1_blocks_1_conv_norm1_weight, p_model_encoder_model_stages_1_blocks_1_conv_norm1_bias, b_model_encoder_model_stages_1_blocks_1_conv_norm1_running_mean, b_model_encoder_model_stages_1_blocks_1_conv_norm1_running_var, 0.1, 0.001);  conv2d_18 = p_model_encoder_model_stages_1_blocks_1_conv_norm1_weight = p_model_encoder_model_stages_1_blocks_1_conv_norm1_bias = b_model_encoder_model_stages_1_blocks_1_conv_norm1_running_mean = b_model_encoder_model_stages_1_blocks_1_conv_norm1_running_var = None\n",
       "                    getitem_51: \"f32[s77, 512, 64, 64]\" = _native_batch_norm_legit_no_training_11[0];  _native_batch_norm_legit_no_training_11 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_13: \"f32[s77, 512, 64, 64]\" = torch.ops.aten.gelu.default(getitem_51, approximate = 'tanh');  getitem_51 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_19: \"f32[s77, 512, 64, 64]\" = torch.ops.aten.conv2d.default(gelu_13, p_model_encoder_model_stages_1_blocks_1_conv_conv2_kxk_weight, None, [1, 1], [1, 1], [1, 1], 512);  gelu_13 = p_model_encoder_model_stages_1_blocks_1_conv_conv2_kxk_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_12 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_19, p_model_encoder_model_stages_1_blocks_1_conv_norm2_weight, p_model_encoder_model_stages_1_blocks_1_conv_norm2_bias, b_model_encoder_model_stages_1_blocks_1_conv_norm2_running_mean, b_model_encoder_model_stages_1_blocks_1_conv_norm2_running_var, 0.1, 0.001);  conv2d_19 = p_model_encoder_model_stages_1_blocks_1_conv_norm2_weight = p_model_encoder_model_stages_1_blocks_1_conv_norm2_bias = b_model_encoder_model_stages_1_blocks_1_conv_norm2_running_mean = b_model_encoder_model_stages_1_blocks_1_conv_norm2_running_var = None\n",
       "                    getitem_54: \"f32[s77, 512, 64, 64]\" = _native_batch_norm_legit_no_training_12[0];  _native_batch_norm_legit_no_training_12 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_14: \"f32[s77, 512, 64, 64]\" = torch.ops.aten.gelu.default(getitem_54, approximate = 'tanh');  getitem_54 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:56 in forward, code: x_se = x.mean((2, 3), keepdim=True)\n",
       "                    mean_3: \"f32[s77, 512, 1, 1]\" = torch.ops.aten.mean.dim(gelu_14, [2, 3], True)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_20: \"f32[s77, 32, 1, 1]\" = torch.ops.aten.conv2d.default(mean_3, p_model_encoder_model_stages_1_blocks_1_conv_se_fc1_weight, p_model_encoder_model_stages_1_blocks_1_conv_se_fc1_bias);  mean_3 = p_model_encoder_model_stages_1_blocks_1_conv_se_fc1_weight = p_model_encoder_model_stages_1_blocks_1_conv_se_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:473 in forward, code: return F.silu(input, inplace=self.inplace)\n",
       "                    silu_3: \"f32[s77, 32, 1, 1]\" = torch.ops.aten.silu.default(conv2d_20);  conv2d_20 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_21: \"f32[s77, 512, 1, 1]\" = torch.ops.aten.conv2d.default(silu_3, p_model_encoder_model_stages_1_blocks_1_conv_se_fc2_weight, p_model_encoder_model_stages_1_blocks_1_conv_se_fc2_bias);  silu_3 = p_model_encoder_model_stages_1_blocks_1_conv_se_fc2_weight = p_model_encoder_model_stages_1_blocks_1_conv_se_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:57 in forward, code: return x.sigmoid_() if self.inplace else x.sigmoid()\n",
       "                    sigmoid_3: \"f32[s77, 512, 1, 1]\" = torch.ops.aten.sigmoid.default(conv2d_21);  conv2d_21 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:63 in forward, code: return x * self.gate(x_se)\n",
       "                    mul_868: \"f32[s77, 512, 64, 64]\" = torch.ops.aten.mul.Tensor(gelu_14, sigmoid_3);  gelu_14 = sigmoid_3 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_22: \"f32[s77, 128, 64, 64]\" = torch.ops.aten.conv2d.default(mul_868, p_model_encoder_model_stages_1_blocks_1_conv_conv3_1x1_weight, p_model_encoder_model_stages_1_blocks_1_conv_conv3_1x1_bias);  mul_868 = p_model_encoder_model_stages_1_blocks_1_conv_conv3_1x1_weight = p_model_encoder_model_stages_1_blocks_1_conv_conv3_1x1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:635 in forward, code: x = self.drop_path(x) + shortcut\n",
       "                    add_1542: \"f32[s77, 128, 64, 64]\" = torch.ops.aten.add.Tensor(conv2d_22, permute_17);  conv2d_22 = permute_17 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1117 in forward, code: x = x.permute(0, 2, 3, 1)  # to NHWC (channels-last)\n",
       "                    permute_18: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.permute.default(add_1542, [0, 2, 3, 1]);  add_1542 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_12: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.layer_norm.default(permute_18, [128], p_model_encoder_model_stages_1_blocks_1_attn_block_norm1_weight, p_model_encoder_model_stages_1_blocks_1_attn_block_norm1_bias);  p_model_encoder_model_stages_1_blocks_1_attn_block_norm1_weight = p_model_encoder_model_stages_1_blocks_1_attn_block_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_30: \"f32[s77, 4, 16, 4, 16, 128]\" = torch.ops.aten.view.default(layer_norm_12, [sym_size_int_37, 4, 16, 4, 16, 128]);  layer_norm_12 = None\n",
       "                    permute_19: \"f32[s77, 4, 4, 16, 16, 128]\" = torch.ops.aten.permute.default(view_30, [0, 1, 3, 2, 4, 5]);  view_30 = None\n",
       "                    clone_42: \"f32[s77, 4, 4, 16, 16, 128]\" = torch.ops.aten.clone.default(permute_19, memory_format = torch.contiguous_format);  permute_19 = None\n",
       "                    view_31: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten.view.default(clone_42, [-1, 16, 16, 128]);  clone_42 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_24: \"f32[16*s77, 16, 16, 384]\" = torch.ops.aten.linear.default(view_31, p_model_encoder_model_stages_1_blocks_1_attn_block_attn_qkv_weight, p_model_encoder_model_stages_1_blocks_1_attn_block_attn_qkv_bias);  view_31 = p_model_encoder_model_stages_1_blocks_1_attn_block_attn_qkv_weight = p_model_encoder_model_stages_1_blocks_1_attn_block_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_32: \"f32[16*s77, 256, 3, 4, 32]\" = torch.ops.aten.view.default(linear_24, [mul_650, -1, 3, 4, 32]);  linear_24 = None\n",
       "                    transpose_12: \"f32[16*s77, 4, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_32, 1, 3);  view_32 = None\n",
       "                    unbind_6 = torch.ops.aten.unbind.int(transpose_12, 2);  transpose_12 = None\n",
       "                    getitem_57: \"f32[16*s77, 4, 256, 32]\" = unbind_6[0]\n",
       "                    getitem_58: \"f32[16*s77, 4, 256, 32]\" = unbind_6[1]\n",
       "                    getitem_59: \"f32[16*s77, 4, 256, 32]\" = unbind_6[2];  unbind_6 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_12: \"f32[4, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_1_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_1_blocks_1_attn_block_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_1_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_1_blocks_1_attn_block_attn_rel_pos_height_lookup = None\n",
       "                    einsum_13: \"f32[4, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_12, b_model_encoder_model_stages_1_blocks_1_attn_block_attn_rel_pos_width_lookup]);  einsum_12 = b_model_encoder_model_stages_1_blocks_1_attn_block_attn_rel_pos_width_lookup = None\n",
       "                    clone_43: \"f32[4, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_13, memory_format = torch.contiguous_format);  einsum_13 = None\n",
       "                    _unsafe_view_12: \"f32[4, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_43, [4, 256, 256]);  clone_43 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_6: \"f32[16*s77, 4, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_57, getitem_58, getitem_59, _unsafe_view_12);  getitem_57 = getitem_58 = getitem_59 = _unsafe_view_12 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_13: \"f32[16*s77, 256, 4, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_6, 1, 2);  scaled_dot_product_attention_6 = None\n",
       "                    clone_44: \"f32[16*s77, 256, 4, 32]\" = torch.ops.aten.clone.default(transpose_13, memory_format = torch.contiguous_format);  transpose_13 = None\n",
       "                    _unsafe_view_13: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten._unsafe_view.default(clone_44, [mul_650, 16, 16, 128]);  clone_44 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_25: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten.linear.default(_unsafe_view_13, p_model_encoder_model_stages_1_blocks_1_attn_block_attn_proj_weight, p_model_encoder_model_stages_1_blocks_1_attn_block_attn_proj_bias);  _unsafe_view_13 = p_model_encoder_model_stages_1_blocks_1_attn_block_attn_proj_weight = p_model_encoder_model_stages_1_blocks_1_attn_block_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_45: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten.clone.default(linear_25);  linear_25 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_33: \"f32[s77, 4, 4, 16, 16, 128]\" = torch.ops.aten.view.default(clone_45, [-1, 4, 4, 16, 16, 128]);  clone_45 = None\n",
       "                    permute_20: \"f32[s77, 4, 16, 4, 16, 128]\" = torch.ops.aten.permute.default(view_33, [0, 1, 3, 2, 4, 5]);  view_33 = None\n",
       "                    clone_46: \"f32[s77, 4, 16, 4, 16, 128]\" = torch.ops.aten.clone.default(permute_20, memory_format = torch.contiguous_format);  permute_20 = None\n",
       "                    view_34: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.view.default(clone_46, [-1, 64, 64, 128]);  clone_46 = None\n",
       "                    add_1672: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.add.Tensor(permute_18, view_34);  permute_18 = view_34 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_13: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.layer_norm.default(add_1672, [128], p_model_encoder_model_stages_1_blocks_1_attn_block_norm2_weight, p_model_encoder_model_stages_1_blocks_1_attn_block_norm2_bias);  p_model_encoder_model_stages_1_blocks_1_attn_block_norm2_weight = p_model_encoder_model_stages_1_blocks_1_attn_block_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_26: \"f32[s77, 64, 64, 512]\" = torch.ops.aten.linear.default(layer_norm_13, p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc1_weight, p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc1_bias);  layer_norm_13 = p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc1_weight = p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_15: \"f32[s77, 64, 64, 512]\" = torch.ops.aten.gelu.default(linear_26, approximate = 'tanh');  linear_26 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_47: \"f32[s77, 64, 64, 512]\" = torch.ops.aten.clone.default(gelu_15);  gelu_15 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_27: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.linear.default(clone_47, p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc2_weight, p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc2_bias);  clone_47 = p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc2_weight = p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_48: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.clone.default(linear_27);  linear_27 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1708: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.add.Tensor(add_1672, clone_48);  add_1672 = clone_48 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_14: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.layer_norm.default(add_1708, [128], p_model_encoder_model_stages_1_blocks_1_attn_grid_norm1_weight, p_model_encoder_model_stages_1_blocks_1_attn_grid_norm1_bias);  p_model_encoder_model_stages_1_blocks_1_attn_grid_norm1_weight = p_model_encoder_model_stages_1_blocks_1_attn_grid_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_35: \"f32[s77, 16, 4, 16, 4, 128]\" = torch.ops.aten.view.default(layer_norm_14, [sym_size_int_37, 16, 4, 16, 4, 128]);  layer_norm_14 = None\n",
       "                    permute_21: \"f32[s77, 4, 4, 16, 16, 128]\" = torch.ops.aten.permute.default(view_35, [0, 2, 4, 1, 3, 5]);  view_35 = None\n",
       "                    clone_49: \"f32[s77, 4, 4, 16, 16, 128]\" = torch.ops.aten.clone.default(permute_21, memory_format = torch.contiguous_format);  permute_21 = None\n",
       "                    view_36: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten.view.default(clone_49, [-1, 16, 16, 128]);  clone_49 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_28: \"f32[16*s77, 16, 16, 384]\" = torch.ops.aten.linear.default(view_36, p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_qkv_weight, p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_qkv_bias);  view_36 = p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_qkv_weight = p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_37: \"f32[16*s77, 256, 3, 4, 32]\" = torch.ops.aten.view.default(linear_28, [mul_650, -1, 3, 4, 32]);  linear_28 = None\n",
       "                    transpose_14: \"f32[16*s77, 4, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_37, 1, 3);  view_37 = None\n",
       "                    unbind_7 = torch.ops.aten.unbind.int(transpose_14, 2);  transpose_14 = None\n",
       "                    getitem_60: \"f32[16*s77, 4, 256, 32]\" = unbind_7[0]\n",
       "                    getitem_61: \"f32[16*s77, 4, 256, 32]\" = unbind_7[1]\n",
       "                    getitem_62: \"f32[16*s77, 4, 256, 32]\" = unbind_7[2];  unbind_7 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_14: \"f32[4, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_1_blocks_1_attn_grid_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_1_blocks_1_attn_grid_attn_rel_pos_height_lookup = None\n",
       "                    einsum_15: \"f32[4, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_14, b_model_encoder_model_stages_1_blocks_1_attn_grid_attn_rel_pos_width_lookup]);  einsum_14 = b_model_encoder_model_stages_1_blocks_1_attn_grid_attn_rel_pos_width_lookup = None\n",
       "                    clone_50: \"f32[4, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_15, memory_format = torch.contiguous_format);  einsum_15 = None\n",
       "                    _unsafe_view_14: \"f32[4, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_50, [4, 256, 256]);  clone_50 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_7: \"f32[16*s77, 4, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_60, getitem_61, getitem_62, _unsafe_view_14);  getitem_60 = getitem_61 = getitem_62 = _unsafe_view_14 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_15: \"f32[16*s77, 256, 4, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_7, 1, 2);  scaled_dot_product_attention_7 = None\n",
       "                    clone_51: \"f32[16*s77, 256, 4, 32]\" = torch.ops.aten.clone.default(transpose_15, memory_format = torch.contiguous_format);  transpose_15 = None\n",
       "                    _unsafe_view_15: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten._unsafe_view.default(clone_51, [mul_650, 16, 16, 128]);  clone_51 = mul_650 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_29: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten.linear.default(_unsafe_view_15, p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_proj_weight, p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_proj_bias);  _unsafe_view_15 = p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_proj_weight = p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_52: \"f32[16*s77, 16, 16, 128]\" = torch.ops.aten.clone.default(linear_29);  linear_29 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_38: \"f32[s77, 4, 4, 16, 16, 128]\" = torch.ops.aten.view.default(clone_52, [-1, 4, 4, 16, 16, 128]);  clone_52 = None\n",
       "                    permute_22: \"f32[s77, 16, 4, 16, 4, 128]\" = torch.ops.aten.permute.default(view_38, [0, 3, 1, 4, 2, 5]);  view_38 = None\n",
       "                    clone_53: \"f32[s77, 16, 4, 16, 4, 128]\" = torch.ops.aten.clone.default(permute_22, memory_format = torch.contiguous_format);  permute_22 = None\n",
       "                    view_39: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.view.default(clone_53, [-1, 64, 64, 128]);  clone_53 = None\n",
       "                    add_1833: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.add.Tensor(add_1708, view_39);  add_1708 = view_39 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_15: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.layer_norm.default(add_1833, [128], p_model_encoder_model_stages_1_blocks_1_attn_grid_norm2_weight, p_model_encoder_model_stages_1_blocks_1_attn_grid_norm2_bias);  p_model_encoder_model_stages_1_blocks_1_attn_grid_norm2_weight = p_model_encoder_model_stages_1_blocks_1_attn_grid_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_30: \"f32[s77, 64, 64, 512]\" = torch.ops.aten.linear.default(layer_norm_15, p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc1_weight, p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc1_bias);  layer_norm_15 = p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc1_weight = p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_16: \"f32[s77, 64, 64, 512]\" = torch.ops.aten.gelu.default(linear_30, approximate = 'tanh');  linear_30 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_54: \"f32[s77, 64, 64, 512]\" = torch.ops.aten.clone.default(gelu_16);  gelu_16 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_31: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.linear.default(clone_54, p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc2_weight, p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc2_bias);  clone_54 = p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc2_weight = p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_55: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.clone.default(linear_31);  linear_31 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_1869: \"f32[s77, 64, 64, 128]\" = torch.ops.aten.add.Tensor(add_1833, clone_55);  add_1833 = clone_55 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1122 in forward, code: x = x.permute(0, 3, 1, 2)  # back to NCHW\n",
       "                    permute_23: \"f32[s77, 128, 64, 64]\" = torch.ops.aten.permute.default(add_1869, [0, 3, 1, 2]);  add_1869 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/pool2d_same.py:45 in forward, code: x = pad_same(x, self.kernel_size, self.stride)\n",
       "                    pad_5: \"f32[s77, 128, 64, 64]\" = torch.ops.aten.pad.default(permute_23, [0, 0, 0, 0], 'constant', 0.0)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/pool2d_same.py:46 in forward, code: return F.avg_pool2d(\n",
       "                    avg_pool2d_2: \"f32[s77, 128, 32, 32]\" = torch.ops.aten.avg_pool2d.default(pad_5, [2, 2], [2, 2]);  pad_5 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_23: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.conv2d.default(avg_pool2d_2, p_model_encoder_model_stages_2_blocks_0_conv_shortcut_expand_weight, p_model_encoder_model_stages_2_blocks_0_conv_shortcut_expand_bias);  avg_pool2d_2 = p_model_encoder_model_stages_2_blocks_0_conv_shortcut_expand_weight = p_model_encoder_model_stages_2_blocks_0_conv_shortcut_expand_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_13 = torch.ops.aten._native_batch_norm_legit_no_training.default(permute_23, p_model_encoder_model_stages_2_blocks_0_conv_pre_norm_weight, p_model_encoder_model_stages_2_blocks_0_conv_pre_norm_bias, b_model_encoder_model_stages_2_blocks_0_conv_pre_norm_running_mean, b_model_encoder_model_stages_2_blocks_0_conv_pre_norm_running_var, 0.1, 0.001);  p_model_encoder_model_stages_2_blocks_0_conv_pre_norm_weight = p_model_encoder_model_stages_2_blocks_0_conv_pre_norm_bias = b_model_encoder_model_stages_2_blocks_0_conv_pre_norm_running_mean = b_model_encoder_model_stages_2_blocks_0_conv_pre_norm_running_var = None\n",
       "                    getitem_63: \"f32[s77, 128, 64, 64]\" = _native_batch_norm_legit_no_training_13[0];  _native_batch_norm_legit_no_training_13 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_24: \"f32[s77, 1024, 64, 64]\" = torch.ops.aten.conv2d.default(getitem_63, p_model_encoder_model_stages_2_blocks_0_conv_conv1_1x1_weight);  getitem_63 = p_model_encoder_model_stages_2_blocks_0_conv_conv1_1x1_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_14 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_24, p_model_encoder_model_stages_2_blocks_0_conv_norm1_weight, p_model_encoder_model_stages_2_blocks_0_conv_norm1_bias, b_model_encoder_model_stages_2_blocks_0_conv_norm1_running_mean, b_model_encoder_model_stages_2_blocks_0_conv_norm1_running_var, 0.1, 0.001);  conv2d_24 = p_model_encoder_model_stages_2_blocks_0_conv_norm1_weight = p_model_encoder_model_stages_2_blocks_0_conv_norm1_bias = b_model_encoder_model_stages_2_blocks_0_conv_norm1_running_mean = b_model_encoder_model_stages_2_blocks_0_conv_norm1_running_var = None\n",
       "                    getitem_66: \"f32[s77, 1024, 64, 64]\" = _native_batch_norm_legit_no_training_14[0];  _native_batch_norm_legit_no_training_14 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_17: \"f32[s77, 1024, 64, 64]\" = torch.ops.aten.gelu.default(getitem_66, approximate = 'tanh');  getitem_66 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/conv2d_same.py:63 in forward, code: return conv2d_same(\n",
       "                    pad_6: \"f32[s77, 1024, 65, 65]\" = torch.ops.aten.pad.default(gelu_17, [0, 1, 0, 1], 'constant', 0.0);  gelu_17 = None\n",
       "                    conv2d_25: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.conv2d.default(pad_6, p_model_encoder_model_stages_2_blocks_0_conv_conv2_kxk_weight, None, [2, 2], [0, 0], [1, 1], 1024);  pad_6 = p_model_encoder_model_stages_2_blocks_0_conv_conv2_kxk_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_15 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_25, p_model_encoder_model_stages_2_blocks_0_conv_norm2_weight, p_model_encoder_model_stages_2_blocks_0_conv_norm2_bias, b_model_encoder_model_stages_2_blocks_0_conv_norm2_running_mean, b_model_encoder_model_stages_2_blocks_0_conv_norm2_running_var, 0.1, 0.001);  conv2d_25 = p_model_encoder_model_stages_2_blocks_0_conv_norm2_weight = p_model_encoder_model_stages_2_blocks_0_conv_norm2_bias = b_model_encoder_model_stages_2_blocks_0_conv_norm2_running_mean = b_model_encoder_model_stages_2_blocks_0_conv_norm2_running_var = None\n",
       "                    getitem_69: \"f32[s77, 1024, 32, 32]\" = _native_batch_norm_legit_no_training_15[0];  _native_batch_norm_legit_no_training_15 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_18: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.gelu.default(getitem_69, approximate = 'tanh');  getitem_69 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:56 in forward, code: x_se = x.mean((2, 3), keepdim=True)\n",
       "                    mean_4: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.mean.dim(gelu_18, [2, 3], True)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_26: \"f32[s77, 64, 1, 1]\" = torch.ops.aten.conv2d.default(mean_4, p_model_encoder_model_stages_2_blocks_0_conv_se_fc1_weight, p_model_encoder_model_stages_2_blocks_0_conv_se_fc1_bias);  mean_4 = p_model_encoder_model_stages_2_blocks_0_conv_se_fc1_weight = p_model_encoder_model_stages_2_blocks_0_conv_se_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:473 in forward, code: return F.silu(input, inplace=self.inplace)\n",
       "                    silu_4: \"f32[s77, 64, 1, 1]\" = torch.ops.aten.silu.default(conv2d_26);  conv2d_26 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_27: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.conv2d.default(silu_4, p_model_encoder_model_stages_2_blocks_0_conv_se_fc2_weight, p_model_encoder_model_stages_2_blocks_0_conv_se_fc2_bias);  silu_4 = p_model_encoder_model_stages_2_blocks_0_conv_se_fc2_weight = p_model_encoder_model_stages_2_blocks_0_conv_se_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:57 in forward, code: return x.sigmoid_() if self.inplace else x.sigmoid()\n",
       "                    sigmoid_4: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.sigmoid.default(conv2d_27);  conv2d_27 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:63 in forward, code: return x * self.gate(x_se)\n",
       "                    mul_1120: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.mul.Tensor(gelu_18, sigmoid_4);  gelu_18 = sigmoid_4 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_28: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.conv2d.default(mul_1120, p_model_encoder_model_stages_2_blocks_0_conv_conv3_1x1_weight, p_model_encoder_model_stages_2_blocks_0_conv_conv3_1x1_bias);  mul_1120 = p_model_encoder_model_stages_2_blocks_0_conv_conv3_1x1_weight = p_model_encoder_model_stages_2_blocks_0_conv_conv3_1x1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:635 in forward, code: x = self.drop_path(x) + shortcut\n",
       "                    add_1976: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_28, conv2d_23);  conv2d_28 = conv2d_23 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1117 in forward, code: x = x.permute(0, 2, 3, 1)  # to NHWC (channels-last)\n",
       "                    permute_24: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.permute.default(add_1976, [0, 2, 3, 1]);  add_1976 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_16: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(permute_24, [256], p_model_encoder_model_stages_2_blocks_0_attn_block_norm1_weight, p_model_encoder_model_stages_2_blocks_0_attn_block_norm1_bias);  p_model_encoder_model_stages_2_blocks_0_attn_block_norm1_weight = p_model_encoder_model_stages_2_blocks_0_attn_block_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_40: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.view.default(layer_norm_16, [sym_size_int_37, 2, 16, 2, 16, 256]);  layer_norm_16 = None\n",
       "                    permute_25: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.permute.default(view_40, [0, 1, 3, 2, 4, 5]);  view_40 = None\n",
       "                    clone_56: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.clone.default(permute_25, memory_format = torch.contiguous_format);  permute_25 = None\n",
       "                    view_41: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.view.default(clone_56, [-1, 16, 16, 256]);  clone_56 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_32: \"f32[4*s77, 16, 16, 768]\" = torch.ops.aten.linear.default(view_41, p_model_encoder_model_stages_2_blocks_0_attn_block_attn_qkv_weight, p_model_encoder_model_stages_2_blocks_0_attn_block_attn_qkv_bias);  view_41 = p_model_encoder_model_stages_2_blocks_0_attn_block_attn_qkv_weight = p_model_encoder_model_stages_2_blocks_0_attn_block_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    mul_1147: \"Sym(4*s77)\" = 4 * sym_size_int_37\n",
       "                    view_42: \"f32[4*s77, 256, 3, 8, 32]\" = torch.ops.aten.view.default(linear_32, [mul_1147, -1, 3, 8, 32]);  linear_32 = None\n",
       "                    transpose_16: \"f32[4*s77, 8, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_42, 1, 3);  view_42 = None\n",
       "                    unbind_8 = torch.ops.aten.unbind.int(transpose_16, 2);  transpose_16 = None\n",
       "                    getitem_72: \"f32[4*s77, 8, 256, 32]\" = unbind_8[0]\n",
       "                    getitem_73: \"f32[4*s77, 8, 256, 32]\" = unbind_8[1]\n",
       "                    getitem_74: \"f32[4*s77, 8, 256, 32]\" = unbind_8[2];  unbind_8 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_16: \"f32[8, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_2_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_2_blocks_0_attn_block_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_2_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_2_blocks_0_attn_block_attn_rel_pos_height_lookup = None\n",
       "                    einsum_17: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_16, b_model_encoder_model_stages_2_blocks_0_attn_block_attn_rel_pos_width_lookup]);  einsum_16 = b_model_encoder_model_stages_2_blocks_0_attn_block_attn_rel_pos_width_lookup = None\n",
       "                    clone_57: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_17, memory_format = torch.contiguous_format);  einsum_17 = None\n",
       "                    _unsafe_view_16: \"f32[8, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_57, [8, 256, 256]);  clone_57 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_8: \"f32[4*s77, 8, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_72, getitem_73, getitem_74, _unsafe_view_16);  getitem_72 = getitem_73 = getitem_74 = _unsafe_view_16 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_17: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_8, 1, 2);  scaled_dot_product_attention_8 = None\n",
       "                    clone_58: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.clone.default(transpose_17, memory_format = torch.contiguous_format);  transpose_17 = None\n",
       "                    _unsafe_view_17: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten._unsafe_view.default(clone_58, [mul_1147, 16, 16, 256]);  clone_58 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_33: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.linear.default(_unsafe_view_17, p_model_encoder_model_stages_2_blocks_0_attn_block_attn_proj_weight, p_model_encoder_model_stages_2_blocks_0_attn_block_attn_proj_bias);  _unsafe_view_17 = p_model_encoder_model_stages_2_blocks_0_attn_block_attn_proj_weight = p_model_encoder_model_stages_2_blocks_0_attn_block_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_59: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.clone.default(linear_33);  linear_33 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_43: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.view.default(clone_59, [-1, 2, 2, 16, 16, 256]);  clone_59 = None\n",
       "                    permute_26: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.permute.default(view_43, [0, 1, 3, 2, 4, 5]);  view_43 = None\n",
       "                    clone_60: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.clone.default(permute_26, memory_format = torch.contiguous_format);  permute_26 = None\n",
       "                    view_44: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.view.default(clone_60, [-1, 32, 32, 256]);  clone_60 = None\n",
       "                    add_2106: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(permute_24, view_44);  permute_24 = view_44 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_17: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_2106, [256], p_model_encoder_model_stages_2_blocks_0_attn_block_norm2_weight, p_model_encoder_model_stages_2_blocks_0_attn_block_norm2_bias);  p_model_encoder_model_stages_2_blocks_0_attn_block_norm2_weight = p_model_encoder_model_stages_2_blocks_0_attn_block_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_34: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.linear.default(layer_norm_17, p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc1_weight, p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc1_bias);  layer_norm_17 = p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc1_weight = p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_19: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.gelu.default(linear_34, approximate = 'tanh');  linear_34 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_61: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.clone.default(gelu_19);  gelu_19 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_35: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.linear.default(clone_61, p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc2_weight, p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc2_bias);  clone_61 = p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc2_weight = p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_62: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.clone.default(linear_35);  linear_35 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_2142: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_2106, clone_62);  add_2106 = clone_62 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_18: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_2142, [256], p_model_encoder_model_stages_2_blocks_0_attn_grid_norm1_weight, p_model_encoder_model_stages_2_blocks_0_attn_grid_norm1_bias);  p_model_encoder_model_stages_2_blocks_0_attn_grid_norm1_weight = p_model_encoder_model_stages_2_blocks_0_attn_grid_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_45: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.view.default(layer_norm_18, [sym_size_int_37, 16, 2, 16, 2, 256]);  layer_norm_18 = None\n",
       "                    permute_27: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.permute.default(view_45, [0, 2, 4, 1, 3, 5]);  view_45 = None\n",
       "                    clone_63: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.clone.default(permute_27, memory_format = torch.contiguous_format);  permute_27 = None\n",
       "                    view_46: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.view.default(clone_63, [-1, 16, 16, 256]);  clone_63 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_36: \"f32[4*s77, 16, 16, 768]\" = torch.ops.aten.linear.default(view_46, p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_qkv_weight, p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_qkv_bias);  view_46 = p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_qkv_weight = p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_47: \"f32[4*s77, 256, 3, 8, 32]\" = torch.ops.aten.view.default(linear_36, [mul_1147, -1, 3, 8, 32]);  linear_36 = None\n",
       "                    transpose_18: \"f32[4*s77, 8, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_47, 1, 3);  view_47 = None\n",
       "                    unbind_9 = torch.ops.aten.unbind.int(transpose_18, 2);  transpose_18 = None\n",
       "                    getitem_75: \"f32[4*s77, 8, 256, 32]\" = unbind_9[0]\n",
       "                    getitem_76: \"f32[4*s77, 8, 256, 32]\" = unbind_9[1]\n",
       "                    getitem_77: \"f32[4*s77, 8, 256, 32]\" = unbind_9[2];  unbind_9 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_18: \"f32[8, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_2_blocks_0_attn_grid_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_2_blocks_0_attn_grid_attn_rel_pos_height_lookup = None\n",
       "                    einsum_19: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_18, b_model_encoder_model_stages_2_blocks_0_attn_grid_attn_rel_pos_width_lookup]);  einsum_18 = b_model_encoder_model_stages_2_blocks_0_attn_grid_attn_rel_pos_width_lookup = None\n",
       "                    clone_64: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_19, memory_format = torch.contiguous_format);  einsum_19 = None\n",
       "                    _unsafe_view_18: \"f32[8, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_64, [8, 256, 256]);  clone_64 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_9: \"f32[4*s77, 8, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_75, getitem_76, getitem_77, _unsafe_view_18);  getitem_75 = getitem_76 = getitem_77 = _unsafe_view_18 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_19: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_9, 1, 2);  scaled_dot_product_attention_9 = None\n",
       "                    clone_65: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.clone.default(transpose_19, memory_format = torch.contiguous_format);  transpose_19 = None\n",
       "                    _unsafe_view_19: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten._unsafe_view.default(clone_65, [mul_1147, 16, 16, 256]);  clone_65 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_37: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.linear.default(_unsafe_view_19, p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_proj_weight, p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_proj_bias);  _unsafe_view_19 = p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_proj_weight = p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_66: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.clone.default(linear_37);  linear_37 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_48: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.view.default(clone_66, [-1, 2, 2, 16, 16, 256]);  clone_66 = None\n",
       "                    permute_28: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.permute.default(view_48, [0, 3, 1, 4, 2, 5]);  view_48 = None\n",
       "                    clone_67: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.clone.default(permute_28, memory_format = torch.contiguous_format);  permute_28 = None\n",
       "                    view_49: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.view.default(clone_67, [-1, 32, 32, 256]);  clone_67 = None\n",
       "                    add_2267: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_2142, view_49);  add_2142 = view_49 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_19: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_2267, [256], p_model_encoder_model_stages_2_blocks_0_attn_grid_norm2_weight, p_model_encoder_model_stages_2_blocks_0_attn_grid_norm2_bias);  p_model_encoder_model_stages_2_blocks_0_attn_grid_norm2_weight = p_model_encoder_model_stages_2_blocks_0_attn_grid_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_38: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.linear.default(layer_norm_19, p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc1_weight, p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc1_bias);  layer_norm_19 = p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc1_weight = p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_20: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.gelu.default(linear_38, approximate = 'tanh');  linear_38 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_68: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.clone.default(gelu_20);  gelu_20 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_39: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.linear.default(clone_68, p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc2_weight, p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc2_bias);  clone_68 = p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc2_weight = p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_69: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.clone.default(linear_39);  linear_39 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_2303: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_2267, clone_69);  add_2267 = clone_69 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1122 in forward, code: x = x.permute(0, 3, 1, 2)  # back to NCHW\n",
       "                    permute_29: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.permute.default(add_2303, [0, 3, 1, 2]);  add_2303 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_16 = torch.ops.aten._native_batch_norm_legit_no_training.default(permute_29, p_model_encoder_model_stages_2_blocks_1_conv_pre_norm_weight, p_model_encoder_model_stages_2_blocks_1_conv_pre_norm_bias, b_model_encoder_model_stages_2_blocks_1_conv_pre_norm_running_mean, b_model_encoder_model_stages_2_blocks_1_conv_pre_norm_running_var, 0.1, 0.001);  p_model_encoder_model_stages_2_blocks_1_conv_pre_norm_weight = p_model_encoder_model_stages_2_blocks_1_conv_pre_norm_bias = b_model_encoder_model_stages_2_blocks_1_conv_pre_norm_running_mean = b_model_encoder_model_stages_2_blocks_1_conv_pre_norm_running_var = None\n",
       "                    getitem_78: \"f32[s77, 256, 32, 32]\" = _native_batch_norm_legit_no_training_16[0];  _native_batch_norm_legit_no_training_16 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_29: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.conv2d.default(getitem_78, p_model_encoder_model_stages_2_blocks_1_conv_conv1_1x1_weight);  getitem_78 = p_model_encoder_model_stages_2_blocks_1_conv_conv1_1x1_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_17 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_29, p_model_encoder_model_stages_2_blocks_1_conv_norm1_weight, p_model_encoder_model_stages_2_blocks_1_conv_norm1_bias, b_model_encoder_model_stages_2_blocks_1_conv_norm1_running_mean, b_model_encoder_model_stages_2_blocks_1_conv_norm1_running_var, 0.1, 0.001);  conv2d_29 = p_model_encoder_model_stages_2_blocks_1_conv_norm1_weight = p_model_encoder_model_stages_2_blocks_1_conv_norm1_bias = b_model_encoder_model_stages_2_blocks_1_conv_norm1_running_mean = b_model_encoder_model_stages_2_blocks_1_conv_norm1_running_var = None\n",
       "                    getitem_81: \"f32[s77, 1024, 32, 32]\" = _native_batch_norm_legit_no_training_17[0];  _native_batch_norm_legit_no_training_17 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_21: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.gelu.default(getitem_81, approximate = 'tanh');  getitem_81 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_30: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.conv2d.default(gelu_21, p_model_encoder_model_stages_2_blocks_1_conv_conv2_kxk_weight, None, [1, 1], [1, 1], [1, 1], 1024);  gelu_21 = p_model_encoder_model_stages_2_blocks_1_conv_conv2_kxk_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_18 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_30, p_model_encoder_model_stages_2_blocks_1_conv_norm2_weight, p_model_encoder_model_stages_2_blocks_1_conv_norm2_bias, b_model_encoder_model_stages_2_blocks_1_conv_norm2_running_mean, b_model_encoder_model_stages_2_blocks_1_conv_norm2_running_var, 0.1, 0.001);  conv2d_30 = p_model_encoder_model_stages_2_blocks_1_conv_norm2_weight = p_model_encoder_model_stages_2_blocks_1_conv_norm2_bias = b_model_encoder_model_stages_2_blocks_1_conv_norm2_running_mean = b_model_encoder_model_stages_2_blocks_1_conv_norm2_running_var = None\n",
       "                    getitem_84: \"f32[s77, 1024, 32, 32]\" = _native_batch_norm_legit_no_training_18[0];  _native_batch_norm_legit_no_training_18 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_22: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.gelu.default(getitem_84, approximate = 'tanh');  getitem_84 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:56 in forward, code: x_se = x.mean((2, 3), keepdim=True)\n",
       "                    mean_5: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.mean.dim(gelu_22, [2, 3], True)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_31: \"f32[s77, 64, 1, 1]\" = torch.ops.aten.conv2d.default(mean_5, p_model_encoder_model_stages_2_blocks_1_conv_se_fc1_weight, p_model_encoder_model_stages_2_blocks_1_conv_se_fc1_bias);  mean_5 = p_model_encoder_model_stages_2_blocks_1_conv_se_fc1_weight = p_model_encoder_model_stages_2_blocks_1_conv_se_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:473 in forward, code: return F.silu(input, inplace=self.inplace)\n",
       "                    silu_5: \"f32[s77, 64, 1, 1]\" = torch.ops.aten.silu.default(conv2d_31);  conv2d_31 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_32: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.conv2d.default(silu_5, p_model_encoder_model_stages_2_blocks_1_conv_se_fc2_weight, p_model_encoder_model_stages_2_blocks_1_conv_se_fc2_bias);  silu_5 = p_model_encoder_model_stages_2_blocks_1_conv_se_fc2_weight = p_model_encoder_model_stages_2_blocks_1_conv_se_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:57 in forward, code: return x.sigmoid_() if self.inplace else x.sigmoid()\n",
       "                    sigmoid_5: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.sigmoid.default(conv2d_32);  conv2d_32 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:63 in forward, code: return x * self.gate(x_se)\n",
       "                    mul_1365: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.mul.Tensor(gelu_22, sigmoid_5);  gelu_22 = sigmoid_5 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_33: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.conv2d.default(mul_1365, p_model_encoder_model_stages_2_blocks_1_conv_conv3_1x1_weight, p_model_encoder_model_stages_2_blocks_1_conv_conv3_1x1_bias);  mul_1365 = p_model_encoder_model_stages_2_blocks_1_conv_conv3_1x1_weight = p_model_encoder_model_stages_2_blocks_1_conv_conv3_1x1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:635 in forward, code: x = self.drop_path(x) + shortcut\n",
       "                    add_2390: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_33, permute_29);  conv2d_33 = permute_29 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1117 in forward, code: x = x.permute(0, 2, 3, 1)  # to NHWC (channels-last)\n",
       "                    permute_30: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.permute.default(add_2390, [0, 2, 3, 1]);  add_2390 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_20: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(permute_30, [256], p_model_encoder_model_stages_2_blocks_1_attn_block_norm1_weight, p_model_encoder_model_stages_2_blocks_1_attn_block_norm1_bias);  p_model_encoder_model_stages_2_blocks_1_attn_block_norm1_weight = p_model_encoder_model_stages_2_blocks_1_attn_block_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_50: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.view.default(layer_norm_20, [sym_size_int_37, 2, 16, 2, 16, 256]);  layer_norm_20 = None\n",
       "                    permute_31: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.permute.default(view_50, [0, 1, 3, 2, 4, 5]);  view_50 = None\n",
       "                    clone_70: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.clone.default(permute_31, memory_format = torch.contiguous_format);  permute_31 = None\n",
       "                    view_51: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.view.default(clone_70, [-1, 16, 16, 256]);  clone_70 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_40: \"f32[4*s77, 16, 16, 768]\" = torch.ops.aten.linear.default(view_51, p_model_encoder_model_stages_2_blocks_1_attn_block_attn_qkv_weight, p_model_encoder_model_stages_2_blocks_1_attn_block_attn_qkv_bias);  view_51 = p_model_encoder_model_stages_2_blocks_1_attn_block_attn_qkv_weight = p_model_encoder_model_stages_2_blocks_1_attn_block_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_52: \"f32[4*s77, 256, 3, 8, 32]\" = torch.ops.aten.view.default(linear_40, [mul_1147, -1, 3, 8, 32]);  linear_40 = None\n",
       "                    transpose_20: \"f32[4*s77, 8, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_52, 1, 3);  view_52 = None\n",
       "                    unbind_10 = torch.ops.aten.unbind.int(transpose_20, 2);  transpose_20 = None\n",
       "                    getitem_87: \"f32[4*s77, 8, 256, 32]\" = unbind_10[0]\n",
       "                    getitem_88: \"f32[4*s77, 8, 256, 32]\" = unbind_10[1]\n",
       "                    getitem_89: \"f32[4*s77, 8, 256, 32]\" = unbind_10[2];  unbind_10 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_20: \"f32[8, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_2_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_2_blocks_1_attn_block_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_2_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_2_blocks_1_attn_block_attn_rel_pos_height_lookup = None\n",
       "                    einsum_21: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_20, b_model_encoder_model_stages_2_blocks_1_attn_block_attn_rel_pos_width_lookup]);  einsum_20 = b_model_encoder_model_stages_2_blocks_1_attn_block_attn_rel_pos_width_lookup = None\n",
       "                    clone_71: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_21, memory_format = torch.contiguous_format);  einsum_21 = None\n",
       "                    _unsafe_view_20: \"f32[8, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_71, [8, 256, 256]);  clone_71 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_10: \"f32[4*s77, 8, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_87, getitem_88, getitem_89, _unsafe_view_20);  getitem_87 = getitem_88 = getitem_89 = _unsafe_view_20 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_21: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_10, 1, 2);  scaled_dot_product_attention_10 = None\n",
       "                    clone_72: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.clone.default(transpose_21, memory_format = torch.contiguous_format);  transpose_21 = None\n",
       "                    _unsafe_view_21: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten._unsafe_view.default(clone_72, [mul_1147, 16, 16, 256]);  clone_72 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_41: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.linear.default(_unsafe_view_21, p_model_encoder_model_stages_2_blocks_1_attn_block_attn_proj_weight, p_model_encoder_model_stages_2_blocks_1_attn_block_attn_proj_bias);  _unsafe_view_21 = p_model_encoder_model_stages_2_blocks_1_attn_block_attn_proj_weight = p_model_encoder_model_stages_2_blocks_1_attn_block_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_73: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.clone.default(linear_41);  linear_41 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_53: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.view.default(clone_73, [-1, 2, 2, 16, 16, 256]);  clone_73 = None\n",
       "                    permute_32: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.permute.default(view_53, [0, 1, 3, 2, 4, 5]);  view_53 = None\n",
       "                    clone_74: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.clone.default(permute_32, memory_format = torch.contiguous_format);  permute_32 = None\n",
       "                    view_54: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.view.default(clone_74, [-1, 32, 32, 256]);  clone_74 = None\n",
       "                    add_2520: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(permute_30, view_54);  permute_30 = view_54 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_21: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_2520, [256], p_model_encoder_model_stages_2_blocks_1_attn_block_norm2_weight, p_model_encoder_model_stages_2_blocks_1_attn_block_norm2_bias);  p_model_encoder_model_stages_2_blocks_1_attn_block_norm2_weight = p_model_encoder_model_stages_2_blocks_1_attn_block_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_42: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.linear.default(layer_norm_21, p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc1_weight, p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc1_bias);  layer_norm_21 = p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc1_weight = p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_23: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.gelu.default(linear_42, approximate = 'tanh');  linear_42 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_75: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.clone.default(gelu_23);  gelu_23 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_43: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.linear.default(clone_75, p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc2_weight, p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc2_bias);  clone_75 = p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc2_weight = p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_76: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.clone.default(linear_43);  linear_43 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_2556: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_2520, clone_76);  add_2520 = clone_76 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_22: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_2556, [256], p_model_encoder_model_stages_2_blocks_1_attn_grid_norm1_weight, p_model_encoder_model_stages_2_blocks_1_attn_grid_norm1_bias);  p_model_encoder_model_stages_2_blocks_1_attn_grid_norm1_weight = p_model_encoder_model_stages_2_blocks_1_attn_grid_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_55: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.view.default(layer_norm_22, [sym_size_int_37, 16, 2, 16, 2, 256]);  layer_norm_22 = None\n",
       "                    permute_33: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.permute.default(view_55, [0, 2, 4, 1, 3, 5]);  view_55 = None\n",
       "                    clone_77: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.clone.default(permute_33, memory_format = torch.contiguous_format);  permute_33 = None\n",
       "                    view_56: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.view.default(clone_77, [-1, 16, 16, 256]);  clone_77 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_44: \"f32[4*s77, 16, 16, 768]\" = torch.ops.aten.linear.default(view_56, p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_qkv_weight, p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_qkv_bias);  view_56 = p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_qkv_weight = p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_57: \"f32[4*s77, 256, 3, 8, 32]\" = torch.ops.aten.view.default(linear_44, [mul_1147, -1, 3, 8, 32]);  linear_44 = None\n",
       "                    transpose_22: \"f32[4*s77, 8, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_57, 1, 3);  view_57 = None\n",
       "                    unbind_11 = torch.ops.aten.unbind.int(transpose_22, 2);  transpose_22 = None\n",
       "                    getitem_90: \"f32[4*s77, 8, 256, 32]\" = unbind_11[0]\n",
       "                    getitem_91: \"f32[4*s77, 8, 256, 32]\" = unbind_11[1]\n",
       "                    getitem_92: \"f32[4*s77, 8, 256, 32]\" = unbind_11[2];  unbind_11 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_22: \"f32[8, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_2_blocks_1_attn_grid_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_2_blocks_1_attn_grid_attn_rel_pos_height_lookup = None\n",
       "                    einsum_23: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_22, b_model_encoder_model_stages_2_blocks_1_attn_grid_attn_rel_pos_width_lookup]);  einsum_22 = b_model_encoder_model_stages_2_blocks_1_attn_grid_attn_rel_pos_width_lookup = None\n",
       "                    clone_78: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_23, memory_format = torch.contiguous_format);  einsum_23 = None\n",
       "                    _unsafe_view_22: \"f32[8, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_78, [8, 256, 256]);  clone_78 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_11: \"f32[4*s77, 8, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_90, getitem_91, getitem_92, _unsafe_view_22);  getitem_90 = getitem_91 = getitem_92 = _unsafe_view_22 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_23: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_11, 1, 2);  scaled_dot_product_attention_11 = None\n",
       "                    clone_79: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.clone.default(transpose_23, memory_format = torch.contiguous_format);  transpose_23 = None\n",
       "                    _unsafe_view_23: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten._unsafe_view.default(clone_79, [mul_1147, 16, 16, 256]);  clone_79 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_45: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.linear.default(_unsafe_view_23, p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_proj_weight, p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_proj_bias);  _unsafe_view_23 = p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_proj_weight = p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_80: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.clone.default(linear_45);  linear_45 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_58: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.view.default(clone_80, [-1, 2, 2, 16, 16, 256]);  clone_80 = None\n",
       "                    permute_34: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.permute.default(view_58, [0, 3, 1, 4, 2, 5]);  view_58 = None\n",
       "                    clone_81: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.clone.default(permute_34, memory_format = torch.contiguous_format);  permute_34 = None\n",
       "                    view_59: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.view.default(clone_81, [-1, 32, 32, 256]);  clone_81 = None\n",
       "                    add_2681: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_2556, view_59);  add_2556 = view_59 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_23: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_2681, [256], p_model_encoder_model_stages_2_blocks_1_attn_grid_norm2_weight, p_model_encoder_model_stages_2_blocks_1_attn_grid_norm2_bias);  p_model_encoder_model_stages_2_blocks_1_attn_grid_norm2_weight = p_model_encoder_model_stages_2_blocks_1_attn_grid_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_46: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.linear.default(layer_norm_23, p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc1_weight, p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc1_bias);  layer_norm_23 = p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc1_weight = p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_24: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.gelu.default(linear_46, approximate = 'tanh');  linear_46 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_82: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.clone.default(gelu_24);  gelu_24 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_47: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.linear.default(clone_82, p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc2_weight, p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc2_bias);  clone_82 = p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc2_weight = p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_83: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.clone.default(linear_47);  linear_47 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_2717: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_2681, clone_83);  add_2681 = clone_83 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1122 in forward, code: x = x.permute(0, 3, 1, 2)  # back to NCHW\n",
       "                    permute_35: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.permute.default(add_2717, [0, 3, 1, 2]);  add_2717 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_19 = torch.ops.aten._native_batch_norm_legit_no_training.default(permute_35, p_model_encoder_model_stages_2_blocks_2_conv_pre_norm_weight, p_model_encoder_model_stages_2_blocks_2_conv_pre_norm_bias, b_model_encoder_model_stages_2_blocks_2_conv_pre_norm_running_mean, b_model_encoder_model_stages_2_blocks_2_conv_pre_norm_running_var, 0.1, 0.001);  p_model_encoder_model_stages_2_blocks_2_conv_pre_norm_weight = p_model_encoder_model_stages_2_blocks_2_conv_pre_norm_bias = b_model_encoder_model_stages_2_blocks_2_conv_pre_norm_running_mean = b_model_encoder_model_stages_2_blocks_2_conv_pre_norm_running_var = None\n",
       "                    getitem_93: \"f32[s77, 256, 32, 32]\" = _native_batch_norm_legit_no_training_19[0];  _native_batch_norm_legit_no_training_19 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_34: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.conv2d.default(getitem_93, p_model_encoder_model_stages_2_blocks_2_conv_conv1_1x1_weight);  getitem_93 = p_model_encoder_model_stages_2_blocks_2_conv_conv1_1x1_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_20 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_34, p_model_encoder_model_stages_2_blocks_2_conv_norm1_weight, p_model_encoder_model_stages_2_blocks_2_conv_norm1_bias, b_model_encoder_model_stages_2_blocks_2_conv_norm1_running_mean, b_model_encoder_model_stages_2_blocks_2_conv_norm1_running_var, 0.1, 0.001);  conv2d_34 = p_model_encoder_model_stages_2_blocks_2_conv_norm1_weight = p_model_encoder_model_stages_2_blocks_2_conv_norm1_bias = b_model_encoder_model_stages_2_blocks_2_conv_norm1_running_mean = b_model_encoder_model_stages_2_blocks_2_conv_norm1_running_var = None\n",
       "                    getitem_96: \"f32[s77, 1024, 32, 32]\" = _native_batch_norm_legit_no_training_20[0];  _native_batch_norm_legit_no_training_20 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_25: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.gelu.default(getitem_96, approximate = 'tanh');  getitem_96 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_35: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.conv2d.default(gelu_25, p_model_encoder_model_stages_2_blocks_2_conv_conv2_kxk_weight, None, [1, 1], [1, 1], [1, 1], 1024);  gelu_25 = p_model_encoder_model_stages_2_blocks_2_conv_conv2_kxk_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_21 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_35, p_model_encoder_model_stages_2_blocks_2_conv_norm2_weight, p_model_encoder_model_stages_2_blocks_2_conv_norm2_bias, b_model_encoder_model_stages_2_blocks_2_conv_norm2_running_mean, b_model_encoder_model_stages_2_blocks_2_conv_norm2_running_var, 0.1, 0.001);  conv2d_35 = p_model_encoder_model_stages_2_blocks_2_conv_norm2_weight = p_model_encoder_model_stages_2_blocks_2_conv_norm2_bias = b_model_encoder_model_stages_2_blocks_2_conv_norm2_running_mean = b_model_encoder_model_stages_2_blocks_2_conv_norm2_running_var = None\n",
       "                    getitem_99: \"f32[s77, 1024, 32, 32]\" = _native_batch_norm_legit_no_training_21[0];  _native_batch_norm_legit_no_training_21 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_26: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.gelu.default(getitem_99, approximate = 'tanh');  getitem_99 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:56 in forward, code: x_se = x.mean((2, 3), keepdim=True)\n",
       "                    mean_6: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.mean.dim(gelu_26, [2, 3], True)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_36: \"f32[s77, 64, 1, 1]\" = torch.ops.aten.conv2d.default(mean_6, p_model_encoder_model_stages_2_blocks_2_conv_se_fc1_weight, p_model_encoder_model_stages_2_blocks_2_conv_se_fc1_bias);  mean_6 = p_model_encoder_model_stages_2_blocks_2_conv_se_fc1_weight = p_model_encoder_model_stages_2_blocks_2_conv_se_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:473 in forward, code: return F.silu(input, inplace=self.inplace)\n",
       "                    silu_6: \"f32[s77, 64, 1, 1]\" = torch.ops.aten.silu.default(conv2d_36);  conv2d_36 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_37: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.conv2d.default(silu_6, p_model_encoder_model_stages_2_blocks_2_conv_se_fc2_weight, p_model_encoder_model_stages_2_blocks_2_conv_se_fc2_bias);  silu_6 = p_model_encoder_model_stages_2_blocks_2_conv_se_fc2_weight = p_model_encoder_model_stages_2_blocks_2_conv_se_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:57 in forward, code: return x.sigmoid_() if self.inplace else x.sigmoid()\n",
       "                    sigmoid_6: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.sigmoid.default(conv2d_37);  conv2d_37 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:63 in forward, code: return x * self.gate(x_se)\n",
       "                    mul_1609: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.mul.Tensor(gelu_26, sigmoid_6);  gelu_26 = sigmoid_6 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_38: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.conv2d.default(mul_1609, p_model_encoder_model_stages_2_blocks_2_conv_conv3_1x1_weight, p_model_encoder_model_stages_2_blocks_2_conv_conv3_1x1_bias);  mul_1609 = p_model_encoder_model_stages_2_blocks_2_conv_conv3_1x1_weight = p_model_encoder_model_stages_2_blocks_2_conv_conv3_1x1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:635 in forward, code: x = self.drop_path(x) + shortcut\n",
       "                    add_2804: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_38, permute_35);  conv2d_38 = permute_35 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1117 in forward, code: x = x.permute(0, 2, 3, 1)  # to NHWC (channels-last)\n",
       "                    permute_36: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.permute.default(add_2804, [0, 2, 3, 1]);  add_2804 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_24: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(permute_36, [256], p_model_encoder_model_stages_2_blocks_2_attn_block_norm1_weight, p_model_encoder_model_stages_2_blocks_2_attn_block_norm1_bias);  p_model_encoder_model_stages_2_blocks_2_attn_block_norm1_weight = p_model_encoder_model_stages_2_blocks_2_attn_block_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_60: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.view.default(layer_norm_24, [sym_size_int_37, 2, 16, 2, 16, 256]);  layer_norm_24 = None\n",
       "                    permute_37: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.permute.default(view_60, [0, 1, 3, 2, 4, 5]);  view_60 = None\n",
       "                    clone_84: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.clone.default(permute_37, memory_format = torch.contiguous_format);  permute_37 = None\n",
       "                    view_61: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.view.default(clone_84, [-1, 16, 16, 256]);  clone_84 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_48: \"f32[4*s77, 16, 16, 768]\" = torch.ops.aten.linear.default(view_61, p_model_encoder_model_stages_2_blocks_2_attn_block_attn_qkv_weight, p_model_encoder_model_stages_2_blocks_2_attn_block_attn_qkv_bias);  view_61 = p_model_encoder_model_stages_2_blocks_2_attn_block_attn_qkv_weight = p_model_encoder_model_stages_2_blocks_2_attn_block_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_62: \"f32[4*s77, 256, 3, 8, 32]\" = torch.ops.aten.view.default(linear_48, [mul_1147, -1, 3, 8, 32]);  linear_48 = None\n",
       "                    transpose_24: \"f32[4*s77, 8, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_62, 1, 3);  view_62 = None\n",
       "                    unbind_12 = torch.ops.aten.unbind.int(transpose_24, 2);  transpose_24 = None\n",
       "                    getitem_102: \"f32[4*s77, 8, 256, 32]\" = unbind_12[0]\n",
       "                    getitem_103: \"f32[4*s77, 8, 256, 32]\" = unbind_12[1]\n",
       "                    getitem_104: \"f32[4*s77, 8, 256, 32]\" = unbind_12[2];  unbind_12 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_24: \"f32[8, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_2_blocks_2_attn_block_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_2_blocks_2_attn_block_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_2_blocks_2_attn_block_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_2_blocks_2_attn_block_attn_rel_pos_height_lookup = None\n",
       "                    einsum_25: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_24, b_model_encoder_model_stages_2_blocks_2_attn_block_attn_rel_pos_width_lookup]);  einsum_24 = b_model_encoder_model_stages_2_blocks_2_attn_block_attn_rel_pos_width_lookup = None\n",
       "                    clone_85: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_25, memory_format = torch.contiguous_format);  einsum_25 = None\n",
       "                    _unsafe_view_24: \"f32[8, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_85, [8, 256, 256]);  clone_85 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_12: \"f32[4*s77, 8, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_102, getitem_103, getitem_104, _unsafe_view_24);  getitem_102 = getitem_103 = getitem_104 = _unsafe_view_24 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_25: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_12, 1, 2);  scaled_dot_product_attention_12 = None\n",
       "                    clone_86: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.clone.default(transpose_25, memory_format = torch.contiguous_format);  transpose_25 = None\n",
       "                    _unsafe_view_25: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten._unsafe_view.default(clone_86, [mul_1147, 16, 16, 256]);  clone_86 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_49: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.linear.default(_unsafe_view_25, p_model_encoder_model_stages_2_blocks_2_attn_block_attn_proj_weight, p_model_encoder_model_stages_2_blocks_2_attn_block_attn_proj_bias);  _unsafe_view_25 = p_model_encoder_model_stages_2_blocks_2_attn_block_attn_proj_weight = p_model_encoder_model_stages_2_blocks_2_attn_block_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_87: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.clone.default(linear_49);  linear_49 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_63: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.view.default(clone_87, [-1, 2, 2, 16, 16, 256]);  clone_87 = None\n",
       "                    permute_38: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.permute.default(view_63, [0, 1, 3, 2, 4, 5]);  view_63 = None\n",
       "                    clone_88: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.clone.default(permute_38, memory_format = torch.contiguous_format);  permute_38 = None\n",
       "                    view_64: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.view.default(clone_88, [-1, 32, 32, 256]);  clone_88 = None\n",
       "                    add_2934: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(permute_36, view_64);  permute_36 = view_64 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_25: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_2934, [256], p_model_encoder_model_stages_2_blocks_2_attn_block_norm2_weight, p_model_encoder_model_stages_2_blocks_2_attn_block_norm2_bias);  p_model_encoder_model_stages_2_blocks_2_attn_block_norm2_weight = p_model_encoder_model_stages_2_blocks_2_attn_block_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_50: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.linear.default(layer_norm_25, p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc1_weight, p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc1_bias);  layer_norm_25 = p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc1_weight = p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_27: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.gelu.default(linear_50, approximate = 'tanh');  linear_50 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_89: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.clone.default(gelu_27);  gelu_27 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_51: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.linear.default(clone_89, p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc2_weight, p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc2_bias);  clone_89 = p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc2_weight = p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_90: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.clone.default(linear_51);  linear_51 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_2970: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_2934, clone_90);  add_2934 = clone_90 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_26: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_2970, [256], p_model_encoder_model_stages_2_blocks_2_attn_grid_norm1_weight, p_model_encoder_model_stages_2_blocks_2_attn_grid_norm1_bias);  p_model_encoder_model_stages_2_blocks_2_attn_grid_norm1_weight = p_model_encoder_model_stages_2_blocks_2_attn_grid_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_65: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.view.default(layer_norm_26, [sym_size_int_37, 16, 2, 16, 2, 256]);  layer_norm_26 = None\n",
       "                    permute_39: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.permute.default(view_65, [0, 2, 4, 1, 3, 5]);  view_65 = None\n",
       "                    clone_91: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.clone.default(permute_39, memory_format = torch.contiguous_format);  permute_39 = None\n",
       "                    view_66: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.view.default(clone_91, [-1, 16, 16, 256]);  clone_91 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_52: \"f32[4*s77, 16, 16, 768]\" = torch.ops.aten.linear.default(view_66, p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_qkv_weight, p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_qkv_bias);  view_66 = p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_qkv_weight = p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_67: \"f32[4*s77, 256, 3, 8, 32]\" = torch.ops.aten.view.default(linear_52, [mul_1147, -1, 3, 8, 32]);  linear_52 = None\n",
       "                    transpose_26: \"f32[4*s77, 8, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_67, 1, 3);  view_67 = None\n",
       "                    unbind_13 = torch.ops.aten.unbind.int(transpose_26, 2);  transpose_26 = None\n",
       "                    getitem_105: \"f32[4*s77, 8, 256, 32]\" = unbind_13[0]\n",
       "                    getitem_106: \"f32[4*s77, 8, 256, 32]\" = unbind_13[1]\n",
       "                    getitem_107: \"f32[4*s77, 8, 256, 32]\" = unbind_13[2];  unbind_13 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_26: \"f32[8, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_2_blocks_2_attn_grid_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_2_blocks_2_attn_grid_attn_rel_pos_height_lookup = None\n",
       "                    einsum_27: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_26, b_model_encoder_model_stages_2_blocks_2_attn_grid_attn_rel_pos_width_lookup]);  einsum_26 = b_model_encoder_model_stages_2_blocks_2_attn_grid_attn_rel_pos_width_lookup = None\n",
       "                    clone_92: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_27, memory_format = torch.contiguous_format);  einsum_27 = None\n",
       "                    _unsafe_view_26: \"f32[8, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_92, [8, 256, 256]);  clone_92 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_13: \"f32[4*s77, 8, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_105, getitem_106, getitem_107, _unsafe_view_26);  getitem_105 = getitem_106 = getitem_107 = _unsafe_view_26 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_27: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_13, 1, 2);  scaled_dot_product_attention_13 = None\n",
       "                    clone_93: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.clone.default(transpose_27, memory_format = torch.contiguous_format);  transpose_27 = None\n",
       "                    _unsafe_view_27: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten._unsafe_view.default(clone_93, [mul_1147, 16, 16, 256]);  clone_93 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_53: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.linear.default(_unsafe_view_27, p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_proj_weight, p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_proj_bias);  _unsafe_view_27 = p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_proj_weight = p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_94: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.clone.default(linear_53);  linear_53 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_68: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.view.default(clone_94, [-1, 2, 2, 16, 16, 256]);  clone_94 = None\n",
       "                    permute_40: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.permute.default(view_68, [0, 3, 1, 4, 2, 5]);  view_68 = None\n",
       "                    clone_95: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.clone.default(permute_40, memory_format = torch.contiguous_format);  permute_40 = None\n",
       "                    view_69: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.view.default(clone_95, [-1, 32, 32, 256]);  clone_95 = None\n",
       "                    add_3095: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_2970, view_69);  add_2970 = view_69 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_27: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_3095, [256], p_model_encoder_model_stages_2_blocks_2_attn_grid_norm2_weight, p_model_encoder_model_stages_2_blocks_2_attn_grid_norm2_bias);  p_model_encoder_model_stages_2_blocks_2_attn_grid_norm2_weight = p_model_encoder_model_stages_2_blocks_2_attn_grid_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_54: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.linear.default(layer_norm_27, p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc1_weight, p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc1_bias);  layer_norm_27 = p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc1_weight = p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_28: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.gelu.default(linear_54, approximate = 'tanh');  linear_54 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_96: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.clone.default(gelu_28);  gelu_28 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_55: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.linear.default(clone_96, p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc2_weight, p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc2_bias);  clone_96 = p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc2_weight = p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_97: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.clone.default(linear_55);  linear_55 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_3131: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_3095, clone_97);  add_3095 = clone_97 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1122 in forward, code: x = x.permute(0, 3, 1, 2)  # back to NCHW\n",
       "                    permute_41: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.permute.default(add_3131, [0, 3, 1, 2]);  add_3131 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_22 = torch.ops.aten._native_batch_norm_legit_no_training.default(permute_41, p_model_encoder_model_stages_2_blocks_3_conv_pre_norm_weight, p_model_encoder_model_stages_2_blocks_3_conv_pre_norm_bias, b_model_encoder_model_stages_2_blocks_3_conv_pre_norm_running_mean, b_model_encoder_model_stages_2_blocks_3_conv_pre_norm_running_var, 0.1, 0.001);  p_model_encoder_model_stages_2_blocks_3_conv_pre_norm_weight = p_model_encoder_model_stages_2_blocks_3_conv_pre_norm_bias = b_model_encoder_model_stages_2_blocks_3_conv_pre_norm_running_mean = b_model_encoder_model_stages_2_blocks_3_conv_pre_norm_running_var = None\n",
       "                    getitem_108: \"f32[s77, 256, 32, 32]\" = _native_batch_norm_legit_no_training_22[0];  _native_batch_norm_legit_no_training_22 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_39: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.conv2d.default(getitem_108, p_model_encoder_model_stages_2_blocks_3_conv_conv1_1x1_weight);  getitem_108 = p_model_encoder_model_stages_2_blocks_3_conv_conv1_1x1_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_23 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_39, p_model_encoder_model_stages_2_blocks_3_conv_norm1_weight, p_model_encoder_model_stages_2_blocks_3_conv_norm1_bias, b_model_encoder_model_stages_2_blocks_3_conv_norm1_running_mean, b_model_encoder_model_stages_2_blocks_3_conv_norm1_running_var, 0.1, 0.001);  conv2d_39 = p_model_encoder_model_stages_2_blocks_3_conv_norm1_weight = p_model_encoder_model_stages_2_blocks_3_conv_norm1_bias = b_model_encoder_model_stages_2_blocks_3_conv_norm1_running_mean = b_model_encoder_model_stages_2_blocks_3_conv_norm1_running_var = None\n",
       "                    getitem_111: \"f32[s77, 1024, 32, 32]\" = _native_batch_norm_legit_no_training_23[0];  _native_batch_norm_legit_no_training_23 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_29: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.gelu.default(getitem_111, approximate = 'tanh');  getitem_111 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_40: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.conv2d.default(gelu_29, p_model_encoder_model_stages_2_blocks_3_conv_conv2_kxk_weight, None, [1, 1], [1, 1], [1, 1], 1024);  gelu_29 = p_model_encoder_model_stages_2_blocks_3_conv_conv2_kxk_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_24 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_40, p_model_encoder_model_stages_2_blocks_3_conv_norm2_weight, p_model_encoder_model_stages_2_blocks_3_conv_norm2_bias, b_model_encoder_model_stages_2_blocks_3_conv_norm2_running_mean, b_model_encoder_model_stages_2_blocks_3_conv_norm2_running_var, 0.1, 0.001);  conv2d_40 = p_model_encoder_model_stages_2_blocks_3_conv_norm2_weight = p_model_encoder_model_stages_2_blocks_3_conv_norm2_bias = b_model_encoder_model_stages_2_blocks_3_conv_norm2_running_mean = b_model_encoder_model_stages_2_blocks_3_conv_norm2_running_var = None\n",
       "                    getitem_114: \"f32[s77, 1024, 32, 32]\" = _native_batch_norm_legit_no_training_24[0];  _native_batch_norm_legit_no_training_24 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_30: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.gelu.default(getitem_114, approximate = 'tanh');  getitem_114 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:56 in forward, code: x_se = x.mean((2, 3), keepdim=True)\n",
       "                    mean_7: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.mean.dim(gelu_30, [2, 3], True)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_41: \"f32[s77, 64, 1, 1]\" = torch.ops.aten.conv2d.default(mean_7, p_model_encoder_model_stages_2_blocks_3_conv_se_fc1_weight, p_model_encoder_model_stages_2_blocks_3_conv_se_fc1_bias);  mean_7 = p_model_encoder_model_stages_2_blocks_3_conv_se_fc1_weight = p_model_encoder_model_stages_2_blocks_3_conv_se_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:473 in forward, code: return F.silu(input, inplace=self.inplace)\n",
       "                    silu_7: \"f32[s77, 64, 1, 1]\" = torch.ops.aten.silu.default(conv2d_41);  conv2d_41 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_42: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.conv2d.default(silu_7, p_model_encoder_model_stages_2_blocks_3_conv_se_fc2_weight, p_model_encoder_model_stages_2_blocks_3_conv_se_fc2_bias);  silu_7 = p_model_encoder_model_stages_2_blocks_3_conv_se_fc2_weight = p_model_encoder_model_stages_2_blocks_3_conv_se_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:57 in forward, code: return x.sigmoid_() if self.inplace else x.sigmoid()\n",
       "                    sigmoid_7: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.sigmoid.default(conv2d_42);  conv2d_42 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:63 in forward, code: return x * self.gate(x_se)\n",
       "                    mul_1853: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.mul.Tensor(gelu_30, sigmoid_7);  gelu_30 = sigmoid_7 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_43: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.conv2d.default(mul_1853, p_model_encoder_model_stages_2_blocks_3_conv_conv3_1x1_weight, p_model_encoder_model_stages_2_blocks_3_conv_conv3_1x1_bias);  mul_1853 = p_model_encoder_model_stages_2_blocks_3_conv_conv3_1x1_weight = p_model_encoder_model_stages_2_blocks_3_conv_conv3_1x1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:635 in forward, code: x = self.drop_path(x) + shortcut\n",
       "                    add_3218: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_43, permute_41);  conv2d_43 = permute_41 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1117 in forward, code: x = x.permute(0, 2, 3, 1)  # to NHWC (channels-last)\n",
       "                    permute_42: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.permute.default(add_3218, [0, 2, 3, 1]);  add_3218 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_28: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(permute_42, [256], p_model_encoder_model_stages_2_blocks_3_attn_block_norm1_weight, p_model_encoder_model_stages_2_blocks_3_attn_block_norm1_bias);  p_model_encoder_model_stages_2_blocks_3_attn_block_norm1_weight = p_model_encoder_model_stages_2_blocks_3_attn_block_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_70: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.view.default(layer_norm_28, [sym_size_int_37, 2, 16, 2, 16, 256]);  layer_norm_28 = None\n",
       "                    permute_43: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.permute.default(view_70, [0, 1, 3, 2, 4, 5]);  view_70 = None\n",
       "                    clone_98: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.clone.default(permute_43, memory_format = torch.contiguous_format);  permute_43 = None\n",
       "                    view_71: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.view.default(clone_98, [-1, 16, 16, 256]);  clone_98 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_56: \"f32[4*s77, 16, 16, 768]\" = torch.ops.aten.linear.default(view_71, p_model_encoder_model_stages_2_blocks_3_attn_block_attn_qkv_weight, p_model_encoder_model_stages_2_blocks_3_attn_block_attn_qkv_bias);  view_71 = p_model_encoder_model_stages_2_blocks_3_attn_block_attn_qkv_weight = p_model_encoder_model_stages_2_blocks_3_attn_block_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_72: \"f32[4*s77, 256, 3, 8, 32]\" = torch.ops.aten.view.default(linear_56, [mul_1147, -1, 3, 8, 32]);  linear_56 = None\n",
       "                    transpose_28: \"f32[4*s77, 8, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_72, 1, 3);  view_72 = None\n",
       "                    unbind_14 = torch.ops.aten.unbind.int(transpose_28, 2);  transpose_28 = None\n",
       "                    getitem_117: \"f32[4*s77, 8, 256, 32]\" = unbind_14[0]\n",
       "                    getitem_118: \"f32[4*s77, 8, 256, 32]\" = unbind_14[1]\n",
       "                    getitem_119: \"f32[4*s77, 8, 256, 32]\" = unbind_14[2];  unbind_14 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_28: \"f32[8, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_2_blocks_3_attn_block_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_2_blocks_3_attn_block_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_2_blocks_3_attn_block_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_2_blocks_3_attn_block_attn_rel_pos_height_lookup = None\n",
       "                    einsum_29: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_28, b_model_encoder_model_stages_2_blocks_3_attn_block_attn_rel_pos_width_lookup]);  einsum_28 = b_model_encoder_model_stages_2_blocks_3_attn_block_attn_rel_pos_width_lookup = None\n",
       "                    clone_99: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_29, memory_format = torch.contiguous_format);  einsum_29 = None\n",
       "                    _unsafe_view_28: \"f32[8, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_99, [8, 256, 256]);  clone_99 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_14: \"f32[4*s77, 8, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_117, getitem_118, getitem_119, _unsafe_view_28);  getitem_117 = getitem_118 = getitem_119 = _unsafe_view_28 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_29: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_14, 1, 2);  scaled_dot_product_attention_14 = None\n",
       "                    clone_100: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.clone.default(transpose_29, memory_format = torch.contiguous_format);  transpose_29 = None\n",
       "                    _unsafe_view_29: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten._unsafe_view.default(clone_100, [mul_1147, 16, 16, 256]);  clone_100 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_57: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.linear.default(_unsafe_view_29, p_model_encoder_model_stages_2_blocks_3_attn_block_attn_proj_weight, p_model_encoder_model_stages_2_blocks_3_attn_block_attn_proj_bias);  _unsafe_view_29 = p_model_encoder_model_stages_2_blocks_3_attn_block_attn_proj_weight = p_model_encoder_model_stages_2_blocks_3_attn_block_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_101: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.clone.default(linear_57);  linear_57 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_73: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.view.default(clone_101, [-1, 2, 2, 16, 16, 256]);  clone_101 = None\n",
       "                    permute_44: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.permute.default(view_73, [0, 1, 3, 2, 4, 5]);  view_73 = None\n",
       "                    clone_102: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.clone.default(permute_44, memory_format = torch.contiguous_format);  permute_44 = None\n",
       "                    view_74: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.view.default(clone_102, [-1, 32, 32, 256]);  clone_102 = None\n",
       "                    add_3348: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(permute_42, view_74);  permute_42 = view_74 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_29: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_3348, [256], p_model_encoder_model_stages_2_blocks_3_attn_block_norm2_weight, p_model_encoder_model_stages_2_blocks_3_attn_block_norm2_bias);  p_model_encoder_model_stages_2_blocks_3_attn_block_norm2_weight = p_model_encoder_model_stages_2_blocks_3_attn_block_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_58: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.linear.default(layer_norm_29, p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc1_weight, p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc1_bias);  layer_norm_29 = p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc1_weight = p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_31: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.gelu.default(linear_58, approximate = 'tanh');  linear_58 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_103: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.clone.default(gelu_31);  gelu_31 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_59: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.linear.default(clone_103, p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc2_weight, p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc2_bias);  clone_103 = p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc2_weight = p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_104: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.clone.default(linear_59);  linear_59 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_3384: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_3348, clone_104);  add_3348 = clone_104 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_30: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_3384, [256], p_model_encoder_model_stages_2_blocks_3_attn_grid_norm1_weight, p_model_encoder_model_stages_2_blocks_3_attn_grid_norm1_bias);  p_model_encoder_model_stages_2_blocks_3_attn_grid_norm1_weight = p_model_encoder_model_stages_2_blocks_3_attn_grid_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_75: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.view.default(layer_norm_30, [sym_size_int_37, 16, 2, 16, 2, 256]);  layer_norm_30 = None\n",
       "                    permute_45: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.permute.default(view_75, [0, 2, 4, 1, 3, 5]);  view_75 = None\n",
       "                    clone_105: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.clone.default(permute_45, memory_format = torch.contiguous_format);  permute_45 = None\n",
       "                    view_76: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.view.default(clone_105, [-1, 16, 16, 256]);  clone_105 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_60: \"f32[4*s77, 16, 16, 768]\" = torch.ops.aten.linear.default(view_76, p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_qkv_weight, p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_qkv_bias);  view_76 = p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_qkv_weight = p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_77: \"f32[4*s77, 256, 3, 8, 32]\" = torch.ops.aten.view.default(linear_60, [mul_1147, -1, 3, 8, 32]);  linear_60 = None\n",
       "                    transpose_30: \"f32[4*s77, 8, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_77, 1, 3);  view_77 = None\n",
       "                    unbind_15 = torch.ops.aten.unbind.int(transpose_30, 2);  transpose_30 = None\n",
       "                    getitem_120: \"f32[4*s77, 8, 256, 32]\" = unbind_15[0]\n",
       "                    getitem_121: \"f32[4*s77, 8, 256, 32]\" = unbind_15[1]\n",
       "                    getitem_122: \"f32[4*s77, 8, 256, 32]\" = unbind_15[2];  unbind_15 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_30: \"f32[8, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_2_blocks_3_attn_grid_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_2_blocks_3_attn_grid_attn_rel_pos_height_lookup = None\n",
       "                    einsum_31: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_30, b_model_encoder_model_stages_2_blocks_3_attn_grid_attn_rel_pos_width_lookup]);  einsum_30 = b_model_encoder_model_stages_2_blocks_3_attn_grid_attn_rel_pos_width_lookup = None\n",
       "                    clone_106: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_31, memory_format = torch.contiguous_format);  einsum_31 = None\n",
       "                    _unsafe_view_30: \"f32[8, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_106, [8, 256, 256]);  clone_106 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_15: \"f32[4*s77, 8, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_120, getitem_121, getitem_122, _unsafe_view_30);  getitem_120 = getitem_121 = getitem_122 = _unsafe_view_30 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_31: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_15, 1, 2);  scaled_dot_product_attention_15 = None\n",
       "                    clone_107: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.clone.default(transpose_31, memory_format = torch.contiguous_format);  transpose_31 = None\n",
       "                    _unsafe_view_31: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten._unsafe_view.default(clone_107, [mul_1147, 16, 16, 256]);  clone_107 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_61: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.linear.default(_unsafe_view_31, p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_proj_weight, p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_proj_bias);  _unsafe_view_31 = p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_proj_weight = p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_108: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.clone.default(linear_61);  linear_61 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_78: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.view.default(clone_108, [-1, 2, 2, 16, 16, 256]);  clone_108 = None\n",
       "                    permute_46: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.permute.default(view_78, [0, 3, 1, 4, 2, 5]);  view_78 = None\n",
       "                    clone_109: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.clone.default(permute_46, memory_format = torch.contiguous_format);  permute_46 = None\n",
       "                    view_79: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.view.default(clone_109, [-1, 32, 32, 256]);  clone_109 = None\n",
       "                    add_3509: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_3384, view_79);  add_3384 = view_79 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_31: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_3509, [256], p_model_encoder_model_stages_2_blocks_3_attn_grid_norm2_weight, p_model_encoder_model_stages_2_blocks_3_attn_grid_norm2_bias);  p_model_encoder_model_stages_2_blocks_3_attn_grid_norm2_weight = p_model_encoder_model_stages_2_blocks_3_attn_grid_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_62: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.linear.default(layer_norm_31, p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc1_weight, p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc1_bias);  layer_norm_31 = p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc1_weight = p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_32: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.gelu.default(linear_62, approximate = 'tanh');  linear_62 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_110: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.clone.default(gelu_32);  gelu_32 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_63: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.linear.default(clone_110, p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc2_weight, p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc2_bias);  clone_110 = p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc2_weight = p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_111: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.clone.default(linear_63);  linear_63 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_3545: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_3509, clone_111);  add_3509 = clone_111 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1122 in forward, code: x = x.permute(0, 3, 1, 2)  # back to NCHW\n",
       "                    permute_47: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.permute.default(add_3545, [0, 3, 1, 2]);  add_3545 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_25 = torch.ops.aten._native_batch_norm_legit_no_training.default(permute_47, p_model_encoder_model_stages_2_blocks_4_conv_pre_norm_weight, p_model_encoder_model_stages_2_blocks_4_conv_pre_norm_bias, b_model_encoder_model_stages_2_blocks_4_conv_pre_norm_running_mean, b_model_encoder_model_stages_2_blocks_4_conv_pre_norm_running_var, 0.1, 0.001);  p_model_encoder_model_stages_2_blocks_4_conv_pre_norm_weight = p_model_encoder_model_stages_2_blocks_4_conv_pre_norm_bias = b_model_encoder_model_stages_2_blocks_4_conv_pre_norm_running_mean = b_model_encoder_model_stages_2_blocks_4_conv_pre_norm_running_var = None\n",
       "                    getitem_123: \"f32[s77, 256, 32, 32]\" = _native_batch_norm_legit_no_training_25[0];  _native_batch_norm_legit_no_training_25 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_44: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.conv2d.default(getitem_123, p_model_encoder_model_stages_2_blocks_4_conv_conv1_1x1_weight);  getitem_123 = p_model_encoder_model_stages_2_blocks_4_conv_conv1_1x1_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_26 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_44, p_model_encoder_model_stages_2_blocks_4_conv_norm1_weight, p_model_encoder_model_stages_2_blocks_4_conv_norm1_bias, b_model_encoder_model_stages_2_blocks_4_conv_norm1_running_mean, b_model_encoder_model_stages_2_blocks_4_conv_norm1_running_var, 0.1, 0.001);  conv2d_44 = p_model_encoder_model_stages_2_blocks_4_conv_norm1_weight = p_model_encoder_model_stages_2_blocks_4_conv_norm1_bias = b_model_encoder_model_stages_2_blocks_4_conv_norm1_running_mean = b_model_encoder_model_stages_2_blocks_4_conv_norm1_running_var = None\n",
       "                    getitem_126: \"f32[s77, 1024, 32, 32]\" = _native_batch_norm_legit_no_training_26[0];  _native_batch_norm_legit_no_training_26 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_33: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.gelu.default(getitem_126, approximate = 'tanh');  getitem_126 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_45: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.conv2d.default(gelu_33, p_model_encoder_model_stages_2_blocks_4_conv_conv2_kxk_weight, None, [1, 1], [1, 1], [1, 1], 1024);  gelu_33 = p_model_encoder_model_stages_2_blocks_4_conv_conv2_kxk_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_27 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_45, p_model_encoder_model_stages_2_blocks_4_conv_norm2_weight, p_model_encoder_model_stages_2_blocks_4_conv_norm2_bias, b_model_encoder_model_stages_2_blocks_4_conv_norm2_running_mean, b_model_encoder_model_stages_2_blocks_4_conv_norm2_running_var, 0.1, 0.001);  conv2d_45 = p_model_encoder_model_stages_2_blocks_4_conv_norm2_weight = p_model_encoder_model_stages_2_blocks_4_conv_norm2_bias = b_model_encoder_model_stages_2_blocks_4_conv_norm2_running_mean = b_model_encoder_model_stages_2_blocks_4_conv_norm2_running_var = None\n",
       "                    getitem_129: \"f32[s77, 1024, 32, 32]\" = _native_batch_norm_legit_no_training_27[0];  _native_batch_norm_legit_no_training_27 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_34: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.gelu.default(getitem_129, approximate = 'tanh');  getitem_129 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:56 in forward, code: x_se = x.mean((2, 3), keepdim=True)\n",
       "                    mean_8: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.mean.dim(gelu_34, [2, 3], True)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_46: \"f32[s77, 64, 1, 1]\" = torch.ops.aten.conv2d.default(mean_8, p_model_encoder_model_stages_2_blocks_4_conv_se_fc1_weight, p_model_encoder_model_stages_2_blocks_4_conv_se_fc1_bias);  mean_8 = p_model_encoder_model_stages_2_blocks_4_conv_se_fc1_weight = p_model_encoder_model_stages_2_blocks_4_conv_se_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:473 in forward, code: return F.silu(input, inplace=self.inplace)\n",
       "                    silu_8: \"f32[s77, 64, 1, 1]\" = torch.ops.aten.silu.default(conv2d_46);  conv2d_46 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_47: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.conv2d.default(silu_8, p_model_encoder_model_stages_2_blocks_4_conv_se_fc2_weight, p_model_encoder_model_stages_2_blocks_4_conv_se_fc2_bias);  silu_8 = p_model_encoder_model_stages_2_blocks_4_conv_se_fc2_weight = p_model_encoder_model_stages_2_blocks_4_conv_se_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:57 in forward, code: return x.sigmoid_() if self.inplace else x.sigmoid()\n",
       "                    sigmoid_8: \"f32[s77, 1024, 1, 1]\" = torch.ops.aten.sigmoid.default(conv2d_47);  conv2d_47 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:63 in forward, code: return x * self.gate(x_se)\n",
       "                    mul_2097: \"f32[s77, 1024, 32, 32]\" = torch.ops.aten.mul.Tensor(gelu_34, sigmoid_8);  gelu_34 = sigmoid_8 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_48: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.conv2d.default(mul_2097, p_model_encoder_model_stages_2_blocks_4_conv_conv3_1x1_weight, p_model_encoder_model_stages_2_blocks_4_conv_conv3_1x1_bias);  mul_2097 = p_model_encoder_model_stages_2_blocks_4_conv_conv3_1x1_weight = p_model_encoder_model_stages_2_blocks_4_conv_conv3_1x1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:635 in forward, code: x = self.drop_path(x) + shortcut\n",
       "                    add_3632: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.add.Tensor(conv2d_48, permute_47);  conv2d_48 = permute_47 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1117 in forward, code: x = x.permute(0, 2, 3, 1)  # to NHWC (channels-last)\n",
       "                    permute_48: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.permute.default(add_3632, [0, 2, 3, 1]);  add_3632 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_32: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(permute_48, [256], p_model_encoder_model_stages_2_blocks_4_attn_block_norm1_weight, p_model_encoder_model_stages_2_blocks_4_attn_block_norm1_bias);  p_model_encoder_model_stages_2_blocks_4_attn_block_norm1_weight = p_model_encoder_model_stages_2_blocks_4_attn_block_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_80: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.view.default(layer_norm_32, [sym_size_int_37, 2, 16, 2, 16, 256]);  layer_norm_32 = None\n",
       "                    permute_49: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.permute.default(view_80, [0, 1, 3, 2, 4, 5]);  view_80 = None\n",
       "                    clone_112: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.clone.default(permute_49, memory_format = torch.contiguous_format);  permute_49 = None\n",
       "                    view_81: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.view.default(clone_112, [-1, 16, 16, 256]);  clone_112 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_64: \"f32[4*s77, 16, 16, 768]\" = torch.ops.aten.linear.default(view_81, p_model_encoder_model_stages_2_blocks_4_attn_block_attn_qkv_weight, p_model_encoder_model_stages_2_blocks_4_attn_block_attn_qkv_bias);  view_81 = p_model_encoder_model_stages_2_blocks_4_attn_block_attn_qkv_weight = p_model_encoder_model_stages_2_blocks_4_attn_block_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_82: \"f32[4*s77, 256, 3, 8, 32]\" = torch.ops.aten.view.default(linear_64, [mul_1147, -1, 3, 8, 32]);  linear_64 = None\n",
       "                    transpose_32: \"f32[4*s77, 8, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_82, 1, 3);  view_82 = None\n",
       "                    unbind_16 = torch.ops.aten.unbind.int(transpose_32, 2);  transpose_32 = None\n",
       "                    getitem_132: \"f32[4*s77, 8, 256, 32]\" = unbind_16[0]\n",
       "                    getitem_133: \"f32[4*s77, 8, 256, 32]\" = unbind_16[1]\n",
       "                    getitem_134: \"f32[4*s77, 8, 256, 32]\" = unbind_16[2];  unbind_16 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_32: \"f32[8, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_2_blocks_4_attn_block_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_2_blocks_4_attn_block_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_2_blocks_4_attn_block_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_2_blocks_4_attn_block_attn_rel_pos_height_lookup = None\n",
       "                    einsum_33: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_32, b_model_encoder_model_stages_2_blocks_4_attn_block_attn_rel_pos_width_lookup]);  einsum_32 = b_model_encoder_model_stages_2_blocks_4_attn_block_attn_rel_pos_width_lookup = None\n",
       "                    clone_113: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_33, memory_format = torch.contiguous_format);  einsum_33 = None\n",
       "                    _unsafe_view_32: \"f32[8, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_113, [8, 256, 256]);  clone_113 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_16: \"f32[4*s77, 8, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_132, getitem_133, getitem_134, _unsafe_view_32);  getitem_132 = getitem_133 = getitem_134 = _unsafe_view_32 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_33: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_16, 1, 2);  scaled_dot_product_attention_16 = None\n",
       "                    clone_114: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.clone.default(transpose_33, memory_format = torch.contiguous_format);  transpose_33 = None\n",
       "                    _unsafe_view_33: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten._unsafe_view.default(clone_114, [mul_1147, 16, 16, 256]);  clone_114 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_65: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.linear.default(_unsafe_view_33, p_model_encoder_model_stages_2_blocks_4_attn_block_attn_proj_weight, p_model_encoder_model_stages_2_blocks_4_attn_block_attn_proj_bias);  _unsafe_view_33 = p_model_encoder_model_stages_2_blocks_4_attn_block_attn_proj_weight = p_model_encoder_model_stages_2_blocks_4_attn_block_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_115: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.clone.default(linear_65);  linear_65 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_83: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.view.default(clone_115, [-1, 2, 2, 16, 16, 256]);  clone_115 = None\n",
       "                    permute_50: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.permute.default(view_83, [0, 1, 3, 2, 4, 5]);  view_83 = None\n",
       "                    clone_116: \"f32[s77, 2, 16, 2, 16, 256]\" = torch.ops.aten.clone.default(permute_50, memory_format = torch.contiguous_format);  permute_50 = None\n",
       "                    view_84: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.view.default(clone_116, [-1, 32, 32, 256]);  clone_116 = None\n",
       "                    add_3762: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(permute_48, view_84);  permute_48 = view_84 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_33: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_3762, [256], p_model_encoder_model_stages_2_blocks_4_attn_block_norm2_weight, p_model_encoder_model_stages_2_blocks_4_attn_block_norm2_bias);  p_model_encoder_model_stages_2_blocks_4_attn_block_norm2_weight = p_model_encoder_model_stages_2_blocks_4_attn_block_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_66: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.linear.default(layer_norm_33, p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc1_weight, p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc1_bias);  layer_norm_33 = p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc1_weight = p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_35: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.gelu.default(linear_66, approximate = 'tanh');  linear_66 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_117: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.clone.default(gelu_35);  gelu_35 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_67: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.linear.default(clone_117, p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc2_weight, p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc2_bias);  clone_117 = p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc2_weight = p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_118: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.clone.default(linear_67);  linear_67 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_3798: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_3762, clone_118);  add_3762 = clone_118 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_34: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_3798, [256], p_model_encoder_model_stages_2_blocks_4_attn_grid_norm1_weight, p_model_encoder_model_stages_2_blocks_4_attn_grid_norm1_bias);  p_model_encoder_model_stages_2_blocks_4_attn_grid_norm1_weight = p_model_encoder_model_stages_2_blocks_4_attn_grid_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_85: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.view.default(layer_norm_34, [sym_size_int_37, 16, 2, 16, 2, 256]);  layer_norm_34 = None\n",
       "                    permute_51: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.permute.default(view_85, [0, 2, 4, 1, 3, 5]);  view_85 = None\n",
       "                    clone_119: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.clone.default(permute_51, memory_format = torch.contiguous_format);  permute_51 = None\n",
       "                    view_86: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.view.default(clone_119, [-1, 16, 16, 256]);  clone_119 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_68: \"f32[4*s77, 16, 16, 768]\" = torch.ops.aten.linear.default(view_86, p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_qkv_weight, p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_qkv_bias);  view_86 = p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_qkv_weight = p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_87: \"f32[4*s77, 256, 3, 8, 32]\" = torch.ops.aten.view.default(linear_68, [mul_1147, -1, 3, 8, 32]);  linear_68 = None\n",
       "                    transpose_34: \"f32[4*s77, 8, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_87, 1, 3);  view_87 = None\n",
       "                    unbind_17 = torch.ops.aten.unbind.int(transpose_34, 2);  transpose_34 = None\n",
       "                    getitem_135: \"f32[4*s77, 8, 256, 32]\" = unbind_17[0]\n",
       "                    getitem_136: \"f32[4*s77, 8, 256, 32]\" = unbind_17[1]\n",
       "                    getitem_137: \"f32[4*s77, 8, 256, 32]\" = unbind_17[2];  unbind_17 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_34: \"f32[8, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_2_blocks_4_attn_grid_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_2_blocks_4_attn_grid_attn_rel_pos_height_lookup = None\n",
       "                    einsum_35: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_34, b_model_encoder_model_stages_2_blocks_4_attn_grid_attn_rel_pos_width_lookup]);  einsum_34 = b_model_encoder_model_stages_2_blocks_4_attn_grid_attn_rel_pos_width_lookup = None\n",
       "                    clone_120: \"f32[8, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_35, memory_format = torch.contiguous_format);  einsum_35 = None\n",
       "                    _unsafe_view_34: \"f32[8, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_120, [8, 256, 256]);  clone_120 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_17: \"f32[4*s77, 8, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_135, getitem_136, getitem_137, _unsafe_view_34);  getitem_135 = getitem_136 = getitem_137 = _unsafe_view_34 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_35: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_17, 1, 2);  scaled_dot_product_attention_17 = None\n",
       "                    clone_121: \"f32[4*s77, 256, 8, 32]\" = torch.ops.aten.clone.default(transpose_35, memory_format = torch.contiguous_format);  transpose_35 = None\n",
       "                    _unsafe_view_35: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten._unsafe_view.default(clone_121, [mul_1147, 16, 16, 256]);  clone_121 = mul_1147 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_69: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.linear.default(_unsafe_view_35, p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_proj_weight, p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_proj_bias);  _unsafe_view_35 = p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_proj_weight = p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_122: \"f32[4*s77, 16, 16, 256]\" = torch.ops.aten.clone.default(linear_69);  linear_69 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_88: \"f32[s77, 2, 2, 16, 16, 256]\" = torch.ops.aten.view.default(clone_122, [-1, 2, 2, 16, 16, 256]);  clone_122 = None\n",
       "                    permute_52: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.permute.default(view_88, [0, 3, 1, 4, 2, 5]);  view_88 = None\n",
       "                    clone_123: \"f32[s77, 16, 2, 16, 2, 256]\" = torch.ops.aten.clone.default(permute_52, memory_format = torch.contiguous_format);  permute_52 = None\n",
       "                    view_89: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.view.default(clone_123, [-1, 32, 32, 256]);  clone_123 = None\n",
       "                    add_3923: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_3798, view_89);  add_3798 = view_89 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_35: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.layer_norm.default(add_3923, [256], p_model_encoder_model_stages_2_blocks_4_attn_grid_norm2_weight, p_model_encoder_model_stages_2_blocks_4_attn_grid_norm2_bias);  p_model_encoder_model_stages_2_blocks_4_attn_grid_norm2_weight = p_model_encoder_model_stages_2_blocks_4_attn_grid_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_70: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.linear.default(layer_norm_35, p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc1_weight, p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc1_bias);  layer_norm_35 = p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc1_weight = p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_36: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.gelu.default(linear_70, approximate = 'tanh');  linear_70 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_124: \"f32[s77, 32, 32, 1024]\" = torch.ops.aten.clone.default(gelu_36);  gelu_36 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_71: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.linear.default(clone_124, p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc2_weight, p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc2_bias);  clone_124 = p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc2_weight = p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_125: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.clone.default(linear_71);  linear_71 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_3959: \"f32[s77, 32, 32, 256]\" = torch.ops.aten.add.Tensor(add_3923, clone_125);  add_3923 = clone_125 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1122 in forward, code: x = x.permute(0, 3, 1, 2)  # back to NCHW\n",
       "                    permute_53: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.permute.default(add_3959, [0, 3, 1, 2]);  add_3959 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/pool2d_same.py:45 in forward, code: x = pad_same(x, self.kernel_size, self.stride)\n",
       "                    pad_7: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.pad.default(permute_53, [0, 0, 0, 0], 'constant', 0.0)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/pool2d_same.py:46 in forward, code: return F.avg_pool2d(\n",
       "                    avg_pool2d_3: \"f32[s77, 256, 16, 16]\" = torch.ops.aten.avg_pool2d.default(pad_7, [2, 2], [2, 2]);  pad_7 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_49: \"f32[s77, 512, 16, 16]\" = torch.ops.aten.conv2d.default(avg_pool2d_3, p_model_encoder_model_stages_3_blocks_0_conv_shortcut_expand_weight, p_model_encoder_model_stages_3_blocks_0_conv_shortcut_expand_bias);  avg_pool2d_3 = p_model_encoder_model_stages_3_blocks_0_conv_shortcut_expand_weight = p_model_encoder_model_stages_3_blocks_0_conv_shortcut_expand_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_28 = torch.ops.aten._native_batch_norm_legit_no_training.default(permute_53, p_model_encoder_model_stages_3_blocks_0_conv_pre_norm_weight, p_model_encoder_model_stages_3_blocks_0_conv_pre_norm_bias, b_model_encoder_model_stages_3_blocks_0_conv_pre_norm_running_mean, b_model_encoder_model_stages_3_blocks_0_conv_pre_norm_running_var, 0.1, 0.001);  p_model_encoder_model_stages_3_blocks_0_conv_pre_norm_weight = p_model_encoder_model_stages_3_blocks_0_conv_pre_norm_bias = b_model_encoder_model_stages_3_blocks_0_conv_pre_norm_running_mean = b_model_encoder_model_stages_3_blocks_0_conv_pre_norm_running_var = None\n",
       "                    getitem_138: \"f32[s77, 256, 32, 32]\" = _native_batch_norm_legit_no_training_28[0];  _native_batch_norm_legit_no_training_28 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_50: \"f32[s77, 2048, 32, 32]\" = torch.ops.aten.conv2d.default(getitem_138, p_model_encoder_model_stages_3_blocks_0_conv_conv1_1x1_weight);  getitem_138 = p_model_encoder_model_stages_3_blocks_0_conv_conv1_1x1_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_29 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_50, p_model_encoder_model_stages_3_blocks_0_conv_norm1_weight, p_model_encoder_model_stages_3_blocks_0_conv_norm1_bias, b_model_encoder_model_stages_3_blocks_0_conv_norm1_running_mean, b_model_encoder_model_stages_3_blocks_0_conv_norm1_running_var, 0.1, 0.001);  conv2d_50 = p_model_encoder_model_stages_3_blocks_0_conv_norm1_weight = p_model_encoder_model_stages_3_blocks_0_conv_norm1_bias = b_model_encoder_model_stages_3_blocks_0_conv_norm1_running_mean = b_model_encoder_model_stages_3_blocks_0_conv_norm1_running_var = None\n",
       "                    getitem_141: \"f32[s77, 2048, 32, 32]\" = _native_batch_norm_legit_no_training_29[0];  _native_batch_norm_legit_no_training_29 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_37: \"f32[s77, 2048, 32, 32]\" = torch.ops.aten.gelu.default(getitem_141, approximate = 'tanh');  getitem_141 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/conv2d_same.py:63 in forward, code: return conv2d_same(\n",
       "                    pad_8: \"f32[s77, 2048, 33, 33]\" = torch.ops.aten.pad.default(gelu_37, [0, 1, 0, 1], 'constant', 0.0);  gelu_37 = None\n",
       "                    conv2d_51: \"f32[s77, 2048, 16, 16]\" = torch.ops.aten.conv2d.default(pad_8, p_model_encoder_model_stages_3_blocks_0_conv_conv2_kxk_weight, None, [2, 2], [0, 0], [1, 1], 2048);  pad_8 = p_model_encoder_model_stages_3_blocks_0_conv_conv2_kxk_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_30 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_51, p_model_encoder_model_stages_3_blocks_0_conv_norm2_weight, p_model_encoder_model_stages_3_blocks_0_conv_norm2_bias, b_model_encoder_model_stages_3_blocks_0_conv_norm2_running_mean, b_model_encoder_model_stages_3_blocks_0_conv_norm2_running_var, 0.1, 0.001);  conv2d_51 = p_model_encoder_model_stages_3_blocks_0_conv_norm2_weight = p_model_encoder_model_stages_3_blocks_0_conv_norm2_bias = b_model_encoder_model_stages_3_blocks_0_conv_norm2_running_mean = b_model_encoder_model_stages_3_blocks_0_conv_norm2_running_var = None\n",
       "                    getitem_144: \"f32[s77, 2048, 16, 16]\" = _native_batch_norm_legit_no_training_30[0];  _native_batch_norm_legit_no_training_30 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_38: \"f32[s77, 2048, 16, 16]\" = torch.ops.aten.gelu.default(getitem_144, approximate = 'tanh');  getitem_144 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:56 in forward, code: x_se = x.mean((2, 3), keepdim=True)\n",
       "                    mean_9: \"f32[s77, 2048, 1, 1]\" = torch.ops.aten.mean.dim(gelu_38, [2, 3], True)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_52: \"f32[s77, 128, 1, 1]\" = torch.ops.aten.conv2d.default(mean_9, p_model_encoder_model_stages_3_blocks_0_conv_se_fc1_weight, p_model_encoder_model_stages_3_blocks_0_conv_se_fc1_bias);  mean_9 = p_model_encoder_model_stages_3_blocks_0_conv_se_fc1_weight = p_model_encoder_model_stages_3_blocks_0_conv_se_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:473 in forward, code: return F.silu(input, inplace=self.inplace)\n",
       "                    silu_9: \"f32[s77, 128, 1, 1]\" = torch.ops.aten.silu.default(conv2d_52);  conv2d_52 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_53: \"f32[s77, 2048, 1, 1]\" = torch.ops.aten.conv2d.default(silu_9, p_model_encoder_model_stages_3_blocks_0_conv_se_fc2_weight, p_model_encoder_model_stages_3_blocks_0_conv_se_fc2_bias);  silu_9 = p_model_encoder_model_stages_3_blocks_0_conv_se_fc2_weight = p_model_encoder_model_stages_3_blocks_0_conv_se_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:57 in forward, code: return x.sigmoid_() if self.inplace else x.sigmoid()\n",
       "                    sigmoid_9: \"f32[s77, 2048, 1, 1]\" = torch.ops.aten.sigmoid.default(conv2d_53);  conv2d_53 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:63 in forward, code: return x * self.gate(x_se)\n",
       "                    mul_2349: \"f32[s77, 2048, 16, 16]\" = torch.ops.aten.mul.Tensor(gelu_38, sigmoid_9);  gelu_38 = sigmoid_9 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_54: \"f32[s77, 512, 16, 16]\" = torch.ops.aten.conv2d.default(mul_2349, p_model_encoder_model_stages_3_blocks_0_conv_conv3_1x1_weight, p_model_encoder_model_stages_3_blocks_0_conv_conv3_1x1_bias);  mul_2349 = p_model_encoder_model_stages_3_blocks_0_conv_conv3_1x1_weight = p_model_encoder_model_stages_3_blocks_0_conv_conv3_1x1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:635 in forward, code: x = self.drop_path(x) + shortcut\n",
       "                    add_4066: \"f32[s77, 512, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_54, conv2d_49);  conv2d_54 = conv2d_49 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1117 in forward, code: x = x.permute(0, 2, 3, 1)  # to NHWC (channels-last)\n",
       "                    permute_54: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.permute.default(add_4066, [0, 2, 3, 1]);  add_4066 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_36: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.layer_norm.default(permute_54, [512], p_model_encoder_model_stages_3_blocks_0_attn_block_norm1_weight, p_model_encoder_model_stages_3_blocks_0_attn_block_norm1_bias);  p_model_encoder_model_stages_3_blocks_0_attn_block_norm1_weight = p_model_encoder_model_stages_3_blocks_0_attn_block_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_90: \"f32[s77, 1, 16, 1, 16, 512]\" = torch.ops.aten.view.default(layer_norm_36, [sym_size_int_37, 1, 16, 1, 16, 512]);  layer_norm_36 = None\n",
       "                    permute_55: \"f32[s77, 1, 1, 16, 16, 512]\" = torch.ops.aten.permute.default(view_90, [0, 1, 3, 2, 4, 5]);  view_90 = None\n",
       "                    view_91: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.view.default(permute_55, [-1, 16, 16, 512]);  permute_55 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_72: \"f32[s77, 16, 16, 1536]\" = torch.ops.aten.linear.default(view_91, p_model_encoder_model_stages_3_blocks_0_attn_block_attn_qkv_weight, p_model_encoder_model_stages_3_blocks_0_attn_block_attn_qkv_bias);  view_91 = p_model_encoder_model_stages_3_blocks_0_attn_block_attn_qkv_weight = p_model_encoder_model_stages_3_blocks_0_attn_block_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_92: \"f32[s77, 256, 3, 16, 32]\" = torch.ops.aten.view.default(linear_72, [sym_size_int_37, -1, 3, 16, 32]);  linear_72 = None\n",
       "                    transpose_36: \"f32[s77, 16, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_92, 1, 3);  view_92 = None\n",
       "                    unbind_18 = torch.ops.aten.unbind.int(transpose_36, 2);  transpose_36 = None\n",
       "                    getitem_147: \"f32[s77, 16, 256, 32]\" = unbind_18[0]\n",
       "                    getitem_148: \"f32[s77, 16, 256, 32]\" = unbind_18[1]\n",
       "                    getitem_149: \"f32[s77, 16, 256, 32]\" = unbind_18[2];  unbind_18 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_36: \"f32[16, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_3_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_3_blocks_0_attn_block_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_3_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_3_blocks_0_attn_block_attn_rel_pos_height_lookup = None\n",
       "                    einsum_37: \"f32[16, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_36, b_model_encoder_model_stages_3_blocks_0_attn_block_attn_rel_pos_width_lookup]);  einsum_36 = b_model_encoder_model_stages_3_blocks_0_attn_block_attn_rel_pos_width_lookup = None\n",
       "                    clone_126: \"f32[16, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_37, memory_format = torch.contiguous_format);  einsum_37 = None\n",
       "                    _unsafe_view_36: \"f32[16, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_126, [16, 256, 256]);  clone_126 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_18: \"f32[s77, 16, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_147, getitem_148, getitem_149, _unsafe_view_36);  getitem_147 = getitem_148 = getitem_149 = _unsafe_view_36 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_37: \"f32[s77, 256, 16, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_18, 1, 2);  scaled_dot_product_attention_18 = None\n",
       "                    clone_127: \"f32[s77, 256, 16, 32]\" = torch.ops.aten.clone.default(transpose_37, memory_format = torch.contiguous_format);  transpose_37 = None\n",
       "                    _unsafe_view_37: \"f32[s77, 16, 16, 512]\" = torch.ops.aten._unsafe_view.default(clone_127, [sym_size_int_37, 16, 16, 512]);  clone_127 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_73: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.linear.default(_unsafe_view_37, p_model_encoder_model_stages_3_blocks_0_attn_block_attn_proj_weight, p_model_encoder_model_stages_3_blocks_0_attn_block_attn_proj_bias);  _unsafe_view_37 = p_model_encoder_model_stages_3_blocks_0_attn_block_attn_proj_weight = p_model_encoder_model_stages_3_blocks_0_attn_block_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_128: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.clone.default(linear_73);  linear_73 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_93: \"f32[s77, 1, 1, 16, 16, 512]\" = torch.ops.aten.view.default(clone_128, [-1, 1, 1, 16, 16, 512]);  clone_128 = None\n",
       "                    permute_56: \"f32[s77, 1, 16, 1, 16, 512]\" = torch.ops.aten.permute.default(view_93, [0, 1, 3, 2, 4, 5]);  view_93 = None\n",
       "                    view_94: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.view.default(permute_56, [-1, 16, 16, 512]);  permute_56 = None\n",
       "                    add_4182: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.add.Tensor(permute_54, view_94);  permute_54 = view_94 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_37: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.layer_norm.default(add_4182, [512], p_model_encoder_model_stages_3_blocks_0_attn_block_norm2_weight, p_model_encoder_model_stages_3_blocks_0_attn_block_norm2_bias);  p_model_encoder_model_stages_3_blocks_0_attn_block_norm2_weight = p_model_encoder_model_stages_3_blocks_0_attn_block_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_74: \"f32[s77, 16, 16, 2048]\" = torch.ops.aten.linear.default(layer_norm_37, p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc1_weight, p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc1_bias);  layer_norm_37 = p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc1_weight = p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_39: \"f32[s77, 16, 16, 2048]\" = torch.ops.aten.gelu.default(linear_74, approximate = 'tanh');  linear_74 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_129: \"f32[s77, 16, 16, 2048]\" = torch.ops.aten.clone.default(gelu_39);  gelu_39 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_75: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.linear.default(clone_129, p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc2_weight, p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc2_bias);  clone_129 = p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc2_weight = p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_130: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.clone.default(linear_75);  linear_75 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_4218: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.add.Tensor(add_4182, clone_130);  add_4182 = clone_130 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_38: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.layer_norm.default(add_4218, [512], p_model_encoder_model_stages_3_blocks_0_attn_grid_norm1_weight, p_model_encoder_model_stages_3_blocks_0_attn_grid_norm1_bias);  p_model_encoder_model_stages_3_blocks_0_attn_grid_norm1_weight = p_model_encoder_model_stages_3_blocks_0_attn_grid_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_95: \"f32[s77, 16, 1, 16, 1, 512]\" = torch.ops.aten.view.default(layer_norm_38, [sym_size_int_37, 16, 1, 16, 1, 512]);  layer_norm_38 = None\n",
       "                    permute_57: \"f32[s77, 1, 1, 16, 16, 512]\" = torch.ops.aten.permute.default(view_95, [0, 2, 4, 1, 3, 5]);  view_95 = None\n",
       "                    view_96: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.view.default(permute_57, [-1, 16, 16, 512]);  permute_57 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_76: \"f32[s77, 16, 16, 1536]\" = torch.ops.aten.linear.default(view_96, p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_qkv_weight, p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_qkv_bias);  view_96 = p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_qkv_weight = p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_97: \"f32[s77, 256, 3, 16, 32]\" = torch.ops.aten.view.default(linear_76, [sym_size_int_37, -1, 3, 16, 32]);  linear_76 = None\n",
       "                    transpose_38: \"f32[s77, 16, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_97, 1, 3);  view_97 = None\n",
       "                    unbind_19 = torch.ops.aten.unbind.int(transpose_38, 2);  transpose_38 = None\n",
       "                    getitem_150: \"f32[s77, 16, 256, 32]\" = unbind_19[0]\n",
       "                    getitem_151: \"f32[s77, 16, 256, 32]\" = unbind_19[1]\n",
       "                    getitem_152: \"f32[s77, 16, 256, 32]\" = unbind_19[2];  unbind_19 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_38: \"f32[16, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_3_blocks_0_attn_grid_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_3_blocks_0_attn_grid_attn_rel_pos_height_lookup = None\n",
       "                    einsum_39: \"f32[16, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_38, b_model_encoder_model_stages_3_blocks_0_attn_grid_attn_rel_pos_width_lookup]);  einsum_38 = b_model_encoder_model_stages_3_blocks_0_attn_grid_attn_rel_pos_width_lookup = None\n",
       "                    clone_131: \"f32[16, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_39, memory_format = torch.contiguous_format);  einsum_39 = None\n",
       "                    _unsafe_view_38: \"f32[16, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_131, [16, 256, 256]);  clone_131 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_19: \"f32[s77, 16, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_150, getitem_151, getitem_152, _unsafe_view_38);  getitem_150 = getitem_151 = getitem_152 = _unsafe_view_38 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_39: \"f32[s77, 256, 16, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_19, 1, 2);  scaled_dot_product_attention_19 = None\n",
       "                    clone_132: \"f32[s77, 256, 16, 32]\" = torch.ops.aten.clone.default(transpose_39, memory_format = torch.contiguous_format);  transpose_39 = None\n",
       "                    _unsafe_view_39: \"f32[s77, 16, 16, 512]\" = torch.ops.aten._unsafe_view.default(clone_132, [sym_size_int_37, 16, 16, 512]);  clone_132 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_77: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.linear.default(_unsafe_view_39, p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_proj_weight, p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_proj_bias);  _unsafe_view_39 = p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_proj_weight = p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_133: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.clone.default(linear_77);  linear_77 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_98: \"f32[s77, 1, 1, 16, 16, 512]\" = torch.ops.aten.view.default(clone_133, [-1, 1, 1, 16, 16, 512]);  clone_133 = None\n",
       "                    permute_58: \"f32[s77, 16, 1, 16, 1, 512]\" = torch.ops.aten.permute.default(view_98, [0, 3, 1, 4, 2, 5]);  view_98 = None\n",
       "                    view_99: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.view.default(permute_58, [-1, 16, 16, 512]);  permute_58 = None\n",
       "                    add_4329: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.add.Tensor(add_4218, view_99);  add_4218 = view_99 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_39: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.layer_norm.default(add_4329, [512], p_model_encoder_model_stages_3_blocks_0_attn_grid_norm2_weight, p_model_encoder_model_stages_3_blocks_0_attn_grid_norm2_bias);  p_model_encoder_model_stages_3_blocks_0_attn_grid_norm2_weight = p_model_encoder_model_stages_3_blocks_0_attn_grid_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_78: \"f32[s77, 16, 16, 2048]\" = torch.ops.aten.linear.default(layer_norm_39, p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc1_weight, p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc1_bias);  layer_norm_39 = p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc1_weight = p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_40: \"f32[s77, 16, 16, 2048]\" = torch.ops.aten.gelu.default(linear_78, approximate = 'tanh');  linear_78 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_134: \"f32[s77, 16, 16, 2048]\" = torch.ops.aten.clone.default(gelu_40);  gelu_40 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_79: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.linear.default(clone_134, p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc2_weight, p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc2_bias);  clone_134 = p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc2_weight = p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_135: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.clone.default(linear_79);  linear_79 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_4365: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.add.Tensor(add_4329, clone_135);  add_4329 = clone_135 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1122 in forward, code: x = x.permute(0, 3, 1, 2)  # back to NCHW\n",
       "                    permute_59: \"f32[s77, 512, 16, 16]\" = torch.ops.aten.permute.default(add_4365, [0, 3, 1, 2]);  add_4365 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_31 = torch.ops.aten._native_batch_norm_legit_no_training.default(permute_59, p_model_encoder_model_stages_3_blocks_1_conv_pre_norm_weight, p_model_encoder_model_stages_3_blocks_1_conv_pre_norm_bias, b_model_encoder_model_stages_3_blocks_1_conv_pre_norm_running_mean, b_model_encoder_model_stages_3_blocks_1_conv_pre_norm_running_var, 0.1, 0.001);  p_model_encoder_model_stages_3_blocks_1_conv_pre_norm_weight = p_model_encoder_model_stages_3_blocks_1_conv_pre_norm_bias = b_model_encoder_model_stages_3_blocks_1_conv_pre_norm_running_mean = b_model_encoder_model_stages_3_blocks_1_conv_pre_norm_running_var = None\n",
       "                    getitem_153: \"f32[s77, 512, 16, 16]\" = _native_batch_norm_legit_no_training_31[0];  _native_batch_norm_legit_no_training_31 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_55: \"f32[s77, 2048, 16, 16]\" = torch.ops.aten.conv2d.default(getitem_153, p_model_encoder_model_stages_3_blocks_1_conv_conv1_1x1_weight);  getitem_153 = p_model_encoder_model_stages_3_blocks_1_conv_conv1_1x1_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_32 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_55, p_model_encoder_model_stages_3_blocks_1_conv_norm1_weight, p_model_encoder_model_stages_3_blocks_1_conv_norm1_bias, b_model_encoder_model_stages_3_blocks_1_conv_norm1_running_mean, b_model_encoder_model_stages_3_blocks_1_conv_norm1_running_var, 0.1, 0.001);  conv2d_55 = p_model_encoder_model_stages_3_blocks_1_conv_norm1_weight = p_model_encoder_model_stages_3_blocks_1_conv_norm1_bias = b_model_encoder_model_stages_3_blocks_1_conv_norm1_running_mean = b_model_encoder_model_stages_3_blocks_1_conv_norm1_running_var = None\n",
       "                    getitem_156: \"f32[s77, 2048, 16, 16]\" = _native_batch_norm_legit_no_training_32[0];  _native_batch_norm_legit_no_training_32 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_41: \"f32[s77, 2048, 16, 16]\" = torch.ops.aten.gelu.default(getitem_156, approximate = 'tanh');  getitem_156 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_56: \"f32[s77, 2048, 16, 16]\" = torch.ops.aten.conv2d.default(gelu_41, p_model_encoder_model_stages_3_blocks_1_conv_conv2_kxk_weight, None, [1, 1], [1, 1], [1, 1], 2048);  gelu_41 = p_model_encoder_model_stages_3_blocks_1_conv_conv2_kxk_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:136 in forward, code: x = F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_33 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_56, p_model_encoder_model_stages_3_blocks_1_conv_norm2_weight, p_model_encoder_model_stages_3_blocks_1_conv_norm2_bias, b_model_encoder_model_stages_3_blocks_1_conv_norm2_running_mean, b_model_encoder_model_stages_3_blocks_1_conv_norm2_running_var, 0.1, 0.001);  conv2d_56 = p_model_encoder_model_stages_3_blocks_1_conv_norm2_weight = p_model_encoder_model_stages_3_blocks_1_conv_norm2_bias = b_model_encoder_model_stages_3_blocks_1_conv_norm2_running_mean = b_model_encoder_model_stages_3_blocks_1_conv_norm2_running_var = None\n",
       "                    getitem_159: \"f32[s77, 2048, 16, 16]\" = _native_batch_norm_legit_no_training_33[0];  _native_batch_norm_legit_no_training_33 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_42: \"f32[s77, 2048, 16, 16]\" = torch.ops.aten.gelu.default(getitem_159, approximate = 'tanh');  getitem_159 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:56 in forward, code: x_se = x.mean((2, 3), keepdim=True)\n",
       "                    mean_10: \"f32[s77, 2048, 1, 1]\" = torch.ops.aten.mean.dim(gelu_42, [2, 3], True)\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_57: \"f32[s77, 128, 1, 1]\" = torch.ops.aten.conv2d.default(mean_10, p_model_encoder_model_stages_3_blocks_1_conv_se_fc1_weight, p_model_encoder_model_stages_3_blocks_1_conv_se_fc1_bias);  mean_10 = p_model_encoder_model_stages_3_blocks_1_conv_se_fc1_weight = p_model_encoder_model_stages_3_blocks_1_conv_se_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:473 in forward, code: return F.silu(input, inplace=self.inplace)\n",
       "                    silu_10: \"f32[s77, 128, 1, 1]\" = torch.ops.aten.silu.default(conv2d_57);  conv2d_57 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_58: \"f32[s77, 2048, 1, 1]\" = torch.ops.aten.conv2d.default(silu_10, p_model_encoder_model_stages_3_blocks_1_conv_se_fc2_weight, p_model_encoder_model_stages_3_blocks_1_conv_se_fc2_bias);  silu_10 = p_model_encoder_model_stages_3_blocks_1_conv_se_fc2_weight = p_model_encoder_model_stages_3_blocks_1_conv_se_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:57 in forward, code: return x.sigmoid_() if self.inplace else x.sigmoid()\n",
       "                    sigmoid_10: \"f32[s77, 2048, 1, 1]\" = torch.ops.aten.sigmoid.default(conv2d_58);  conv2d_58 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/squeeze_excite.py:63 in forward, code: return x * self.gate(x_se)\n",
       "                    mul_2561: \"f32[s77, 2048, 16, 16]\" = torch.ops.aten.mul.Tensor(gelu_42, sigmoid_10);  gelu_42 = sigmoid_10 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_59: \"f32[s77, 512, 16, 16]\" = torch.ops.aten.conv2d.default(mul_2561, p_model_encoder_model_stages_3_blocks_1_conv_conv3_1x1_weight, p_model_encoder_model_stages_3_blocks_1_conv_conv3_1x1_bias);  mul_2561 = p_model_encoder_model_stages_3_blocks_1_conv_conv3_1x1_weight = p_model_encoder_model_stages_3_blocks_1_conv_conv3_1x1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:635 in forward, code: x = self.drop_path(x) + shortcut\n",
       "                    add_4452: \"f32[s77, 512, 16, 16]\" = torch.ops.aten.add.Tensor(conv2d_59, permute_59);  conv2d_59 = permute_59 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1117 in forward, code: x = x.permute(0, 2, 3, 1)  # to NHWC (channels-last)\n",
       "                    permute_60: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.permute.default(add_4452, [0, 2, 3, 1]);  add_4452 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_40: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.layer_norm.default(permute_60, [512], p_model_encoder_model_stages_3_blocks_1_attn_block_norm1_weight, p_model_encoder_model_stages_3_blocks_1_attn_block_norm1_bias);  p_model_encoder_model_stages_3_blocks_1_attn_block_norm1_weight = p_model_encoder_model_stages_3_blocks_1_attn_block_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_100: \"f32[s77, 1, 16, 1, 16, 512]\" = torch.ops.aten.view.default(layer_norm_40, [sym_size_int_37, 1, 16, 1, 16, 512]);  layer_norm_40 = None\n",
       "                    permute_61: \"f32[s77, 1, 1, 16, 16, 512]\" = torch.ops.aten.permute.default(view_100, [0, 1, 3, 2, 4, 5]);  view_100 = None\n",
       "                    view_101: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.view.default(permute_61, [-1, 16, 16, 512]);  permute_61 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_80: \"f32[s77, 16, 16, 1536]\" = torch.ops.aten.linear.default(view_101, p_model_encoder_model_stages_3_blocks_1_attn_block_attn_qkv_weight, p_model_encoder_model_stages_3_blocks_1_attn_block_attn_qkv_bias);  view_101 = p_model_encoder_model_stages_3_blocks_1_attn_block_attn_qkv_weight = p_model_encoder_model_stages_3_blocks_1_attn_block_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_102: \"f32[s77, 256, 3, 16, 32]\" = torch.ops.aten.view.default(linear_80, [sym_size_int_37, -1, 3, 16, 32]);  linear_80 = None\n",
       "                    transpose_40: \"f32[s77, 16, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_102, 1, 3);  view_102 = None\n",
       "                    unbind_20 = torch.ops.aten.unbind.int(transpose_40, 2);  transpose_40 = None\n",
       "                    getitem_162: \"f32[s77, 16, 256, 32]\" = unbind_20[0]\n",
       "                    getitem_163: \"f32[s77, 16, 256, 32]\" = unbind_20[1]\n",
       "                    getitem_164: \"f32[s77, 16, 256, 32]\" = unbind_20[2];  unbind_20 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_40: \"f32[16, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_3_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_3_blocks_1_attn_block_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_3_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_3_blocks_1_attn_block_attn_rel_pos_height_lookup = None\n",
       "                    einsum_41: \"f32[16, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_40, b_model_encoder_model_stages_3_blocks_1_attn_block_attn_rel_pos_width_lookup]);  einsum_40 = b_model_encoder_model_stages_3_blocks_1_attn_block_attn_rel_pos_width_lookup = None\n",
       "                    clone_136: \"f32[16, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_41, memory_format = torch.contiguous_format);  einsum_41 = None\n",
       "                    _unsafe_view_40: \"f32[16, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_136, [16, 256, 256]);  clone_136 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_20: \"f32[s77, 16, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_162, getitem_163, getitem_164, _unsafe_view_40);  getitem_162 = getitem_163 = getitem_164 = _unsafe_view_40 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_41: \"f32[s77, 256, 16, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_20, 1, 2);  scaled_dot_product_attention_20 = None\n",
       "                    clone_137: \"f32[s77, 256, 16, 32]\" = torch.ops.aten.clone.default(transpose_41, memory_format = torch.contiguous_format);  transpose_41 = None\n",
       "                    _unsafe_view_41: \"f32[s77, 16, 16, 512]\" = torch.ops.aten._unsafe_view.default(clone_137, [sym_size_int_37, 16, 16, 512]);  clone_137 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_81: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.linear.default(_unsafe_view_41, p_model_encoder_model_stages_3_blocks_1_attn_block_attn_proj_weight, p_model_encoder_model_stages_3_blocks_1_attn_block_attn_proj_bias);  _unsafe_view_41 = p_model_encoder_model_stages_3_blocks_1_attn_block_attn_proj_weight = p_model_encoder_model_stages_3_blocks_1_attn_block_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_138: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.clone.default(linear_81);  linear_81 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_103: \"f32[s77, 1, 1, 16, 16, 512]\" = torch.ops.aten.view.default(clone_138, [-1, 1, 1, 16, 16, 512]);  clone_138 = None\n",
       "                    permute_62: \"f32[s77, 1, 16, 1, 16, 512]\" = torch.ops.aten.permute.default(view_103, [0, 1, 3, 2, 4, 5]);  view_103 = None\n",
       "                    view_104: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.view.default(permute_62, [-1, 16, 16, 512]);  permute_62 = None\n",
       "                    add_4568: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.add.Tensor(permute_60, view_104);  permute_60 = view_104 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_41: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.layer_norm.default(add_4568, [512], p_model_encoder_model_stages_3_blocks_1_attn_block_norm2_weight, p_model_encoder_model_stages_3_blocks_1_attn_block_norm2_bias);  p_model_encoder_model_stages_3_blocks_1_attn_block_norm2_weight = p_model_encoder_model_stages_3_blocks_1_attn_block_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_82: \"f32[s77, 16, 16, 2048]\" = torch.ops.aten.linear.default(layer_norm_41, p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc1_weight, p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc1_bias);  layer_norm_41 = p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc1_weight = p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_43: \"f32[s77, 16, 16, 2048]\" = torch.ops.aten.gelu.default(linear_82, approximate = 'tanh');  linear_82 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_139: \"f32[s77, 16, 16, 2048]\" = torch.ops.aten.clone.default(gelu_43);  gelu_43 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_83: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.linear.default(clone_139, p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc2_weight, p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc2_bias);  clone_139 = p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc2_weight = p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_140: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.clone.default(linear_83);  linear_83 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_4604: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.add.Tensor(add_4568, clone_140);  add_4568 = clone_140 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_42: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.layer_norm.default(add_4604, [512], p_model_encoder_model_stages_3_blocks_1_attn_grid_norm1_weight, p_model_encoder_model_stages_3_blocks_1_attn_grid_norm1_bias);  p_model_encoder_model_stages_3_blocks_1_attn_grid_norm1_weight = p_model_encoder_model_stages_3_blocks_1_attn_grid_norm1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_105: \"f32[s77, 16, 1, 16, 1, 512]\" = torch.ops.aten.view.default(layer_norm_42, [sym_size_int_37, 16, 1, 16, 1, 512]);  layer_norm_42 = None\n",
       "                    permute_63: \"f32[s77, 1, 1, 16, 16, 512]\" = torch.ops.aten.permute.default(view_105, [0, 2, 4, 1, 3, 5]);  view_105 = None\n",
       "                    view_106: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.view.default(permute_63, [-1, 16, 16, 512]);  permute_63 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_84: \"f32[s77, 16, 16, 1536]\" = torch.ops.aten.linear.default(view_106, p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_qkv_weight, p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_qkv_bias);  view_106 = p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_qkv_weight = p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:307 in forward, code: q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
       "                    view_107: \"f32[s77, 256, 3, 16, 32]\" = torch.ops.aten.view.default(linear_84, [sym_size_int_37, -1, 3, 16, 32]);  linear_84 = None\n",
       "                    transpose_42: \"f32[s77, 16, 3, 256, 32]\" = torch.ops.aten.transpose.int(view_107, 1, 3);  view_107 = None\n",
       "                    unbind_21 = torch.ops.aten.unbind.int(transpose_42, 2);  transpose_42 = None\n",
       "                    getitem_165: \"f32[s77, 16, 256, 32]\" = unbind_21[0]\n",
       "                    getitem_166: \"f32[s77, 16, 256, 32]\" = unbind_21[1]\n",
       "                    getitem_167: \"f32[s77, 16, 256, 32]\" = unbind_21[2];  unbind_21 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:312 in forward, code: attn_bias = self.rel_pos.get_bias()\n",
       "                    einsum_42: \"f32[16, 16, 16, 31]\" = torch.ops.aten.einsum.default('nhw,ixh->nixw', [p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table, b_model_encoder_model_stages_3_blocks_1_attn_grid_attn_rel_pos_height_lookup]);  p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table = b_model_encoder_model_stages_3_blocks_1_attn_grid_attn_rel_pos_height_lookup = None\n",
       "                    einsum_43: \"f32[16, 16, 16, 16, 16]\" = torch.ops.aten.einsum.default('nixw,jyw->nijxy', [einsum_42, b_model_encoder_model_stages_3_blocks_1_attn_grid_attn_rel_pos_width_lookup]);  einsum_42 = b_model_encoder_model_stages_3_blocks_1_attn_grid_attn_rel_pos_width_lookup = None\n",
       "                    clone_141: \"f32[16, 16, 16, 16, 16]\" = torch.ops.aten.clone.default(einsum_43, memory_format = torch.contiguous_format);  einsum_43 = None\n",
       "                    _unsafe_view_42: \"f32[16, 256, 256]\" = torch.ops.aten._unsafe_view.default(clone_141, [16, 256, 256]);  clone_141 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:316 in forward, code: x = torch.nn.functional.scaled_dot_product_attention(\n",
       "                    scaled_dot_product_attention_21: \"f32[s77, 16, 256, 32]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_165, getitem_166, getitem_167, _unsafe_view_42);  getitem_165 = getitem_166 = getitem_167 = _unsafe_view_42 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:332 in forward, code: x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
       "                    transpose_43: \"f32[s77, 256, 16, 32]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_21, 1, 2);  scaled_dot_product_attention_21 = None\n",
       "                    clone_142: \"f32[s77, 256, 16, 32]\" = torch.ops.aten.clone.default(transpose_43, memory_format = torch.contiguous_format);  transpose_43 = None\n",
       "                    _unsafe_view_43: \"f32[s77, 16, 16, 512]\" = torch.ops.aten._unsafe_view.default(clone_142, [sym_size_int_37, 16, 16, 512]);  clone_142 = sym_size_int_37 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_85: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.linear.default(_unsafe_view_43, p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_proj_weight, p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_proj_bias);  _unsafe_view_43 = p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_proj_weight = p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_143: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.clone.default(linear_85);  linear_85 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:860 in forward, code: x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
       "                    view_108: \"f32[s77, 1, 1, 16, 16, 512]\" = torch.ops.aten.view.default(clone_143, [-1, 1, 1, 16, 16, 512]);  clone_143 = None\n",
       "                    permute_64: \"f32[s77, 16, 1, 16, 1, 512]\" = torch.ops.aten.permute.default(view_108, [0, 3, 1, 4, 2, 5]);  view_108 = None\n",
       "                    view_109: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.view.default(permute_64, [-1, 16, 16, 512]);  permute_64 = None\n",
       "                    add_4715: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.add.Tensor(add_4604, view_109);  add_4604 = view_109 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm_43: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.layer_norm.default(add_4715, [512], p_model_encoder_model_stages_3_blocks_1_attn_grid_norm2_weight, p_model_encoder_model_stages_3_blocks_1_attn_grid_norm2_bias);  p_model_encoder_model_stages_3_blocks_1_attn_grid_norm2_weight = p_model_encoder_model_stages_3_blocks_1_attn_grid_norm2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_86: \"f32[s77, 16, 16, 2048]\" = torch.ops.aten.linear.default(layer_norm_43, p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc1_weight, p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc1_bias);  layer_norm_43 = p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc1_weight = p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/layers/activations.py:159 in forward, code: return F.gelu(input, approximate='tanh')\n",
       "                    gelu_44: \"f32[s77, 16, 16, 2048]\" = torch.ops.aten.gelu.default(linear_86, approximate = 'tanh');  linear_86 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_144: \"f32[s77, 16, 16, 2048]\" = torch.ops.aten.clone.default(gelu_44);  gelu_44 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_87: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.linear.default(clone_144, p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc2_weight, p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc2_bias);  clone_144 = p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc2_weight = p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_145: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.clone.default(linear_87);  linear_87 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:861 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
       "                    add_4751: \"f32[s77, 16, 16, 512]\" = torch.ops.aten.add.Tensor(add_4715, clone_145);  add_4715 = clone_145 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/timm/models/maxxvit.py:1122 in forward, code: x = x.permute(0, 3, 1, 2)  # back to NCHW\n",
       "                    permute_65: \"f32[s77, 512, 16, 16]\" = torch.ops.aten.permute.default(add_4751, [0, 3, 1, 2]);  add_4751 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:50 in forward, code: feature_map = F.interpolate(\n",
       "                    upsample_nearest2d: \"f32[s77, 512, 32, 32]\" = torch.ops.aten.upsample_nearest2d.vec(permute_65, [32, 32], None);  permute_65 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:56 in forward, code: feature_map = torch.cat([feature_map, skip_connection], dim=1)\n",
       "                    cat_1: \"f32[s77, 768, 32, 32]\" = torch.ops.aten.cat.default([upsample_nearest2d, permute_53], 1);  upsample_nearest2d = permute_53 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_60: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.conv2d.default(cat_1, p_model_decoder_blocks_0_conv1_0_weight, None, [1, 1], [1, 1]);  cat_1 = p_model_decoder_blocks_0_conv1_0_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_34 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_60, p_model_decoder_blocks_0_conv1_1_weight, p_model_decoder_blocks_0_conv1_1_bias, b_model_decoder_blocks_0_conv1_1_running_mean, b_model_decoder_blocks_0_conv1_1_running_var, 0.1, 1e-05);  conv2d_60 = p_model_decoder_blocks_0_conv1_1_weight = p_model_decoder_blocks_0_conv1_1_bias = b_model_decoder_blocks_0_conv1_1_running_mean = b_model_decoder_blocks_0_conv1_1_running_var = None\n",
       "                    getitem_168: \"f32[s77, 256, 32, 32]\" = _native_batch_norm_legit_no_training_34[0];  _native_batch_norm_legit_no_training_34 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.relu.default(getitem_168);  getitem_168 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_61: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.conv2d.default(relu, p_model_decoder_blocks_0_conv2_0_weight, None, [1, 1], [1, 1]);  relu = p_model_decoder_blocks_0_conv2_0_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_35 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_61, p_model_decoder_blocks_0_conv2_1_weight, p_model_decoder_blocks_0_conv2_1_bias, b_model_decoder_blocks_0_conv2_1_running_mean, b_model_decoder_blocks_0_conv2_1_running_var, 0.1, 1e-05);  conv2d_61 = p_model_decoder_blocks_0_conv2_1_weight = p_model_decoder_blocks_0_conv2_1_bias = b_model_decoder_blocks_0_conv2_1_running_mean = b_model_decoder_blocks_0_conv2_1_running_var = None\n",
       "                    getitem_171: \"f32[s77, 256, 32, 32]\" = _native_batch_norm_legit_no_training_35[0];  _native_batch_norm_legit_no_training_35 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_1: \"f32[s77, 256, 32, 32]\" = torch.ops.aten.relu.default(getitem_171);  getitem_171 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:50 in forward, code: feature_map = F.interpolate(\n",
       "                    upsample_nearest2d_1: \"f32[s77, 256, 64, 64]\" = torch.ops.aten.upsample_nearest2d.vec(relu_1, [64, 64], None);  relu_1 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:56 in forward, code: feature_map = torch.cat([feature_map, skip_connection], dim=1)\n",
       "                    cat_2: \"f32[s77, 384, 64, 64]\" = torch.ops.aten.cat.default([upsample_nearest2d_1, permute_23], 1);  upsample_nearest2d_1 = permute_23 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_62: \"f32[s77, 128, 64, 64]\" = torch.ops.aten.conv2d.default(cat_2, p_model_decoder_blocks_1_conv1_0_weight, None, [1, 1], [1, 1]);  cat_2 = p_model_decoder_blocks_1_conv1_0_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_36 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_62, p_model_decoder_blocks_1_conv1_1_weight, p_model_decoder_blocks_1_conv1_1_bias, b_model_decoder_blocks_1_conv1_1_running_mean, b_model_decoder_blocks_1_conv1_1_running_var, 0.1, 1e-05);  conv2d_62 = p_model_decoder_blocks_1_conv1_1_weight = p_model_decoder_blocks_1_conv1_1_bias = b_model_decoder_blocks_1_conv1_1_running_mean = b_model_decoder_blocks_1_conv1_1_running_var = None\n",
       "                    getitem_174: \"f32[s77, 128, 64, 64]\" = _native_batch_norm_legit_no_training_36[0];  _native_batch_norm_legit_no_training_36 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_2: \"f32[s77, 128, 64, 64]\" = torch.ops.aten.relu.default(getitem_174);  getitem_174 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_63: \"f32[s77, 128, 64, 64]\" = torch.ops.aten.conv2d.default(relu_2, p_model_decoder_blocks_1_conv2_0_weight, None, [1, 1], [1, 1]);  relu_2 = p_model_decoder_blocks_1_conv2_0_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_37 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_63, p_model_decoder_blocks_1_conv2_1_weight, p_model_decoder_blocks_1_conv2_1_bias, b_model_decoder_blocks_1_conv2_1_running_mean, b_model_decoder_blocks_1_conv2_1_running_var, 0.1, 1e-05);  conv2d_63 = p_model_decoder_blocks_1_conv2_1_weight = p_model_decoder_blocks_1_conv2_1_bias = b_model_decoder_blocks_1_conv2_1_running_mean = b_model_decoder_blocks_1_conv2_1_running_var = None\n",
       "                    getitem_177: \"f32[s77, 128, 64, 64]\" = _native_batch_norm_legit_no_training_37[0];  _native_batch_norm_legit_no_training_37 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_3: \"f32[s77, 128, 64, 64]\" = torch.ops.aten.relu.default(getitem_177);  getitem_177 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:50 in forward, code: feature_map = F.interpolate(\n",
       "                    upsample_nearest2d_2: \"f32[s77, 128, 128, 128]\" = torch.ops.aten.upsample_nearest2d.vec(relu_3, [128, 128], None);  relu_3 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:56 in forward, code: feature_map = torch.cat([feature_map, skip_connection], dim=1)\n",
       "                    cat_3: \"f32[s77, 192, 128, 128]\" = torch.ops.aten.cat.default([upsample_nearest2d_2, permute_11], 1);  upsample_nearest2d_2 = permute_11 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_64: \"f32[s77, 64, 128, 128]\" = torch.ops.aten.conv2d.default(cat_3, p_model_decoder_blocks_2_conv1_0_weight, None, [1, 1], [1, 1]);  cat_3 = p_model_decoder_blocks_2_conv1_0_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_38 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_64, p_model_decoder_blocks_2_conv1_1_weight, p_model_decoder_blocks_2_conv1_1_bias, b_model_decoder_blocks_2_conv1_1_running_mean, b_model_decoder_blocks_2_conv1_1_running_var, 0.1, 1e-05);  conv2d_64 = p_model_decoder_blocks_2_conv1_1_weight = p_model_decoder_blocks_2_conv1_1_bias = b_model_decoder_blocks_2_conv1_1_running_mean = b_model_decoder_blocks_2_conv1_1_running_var = None\n",
       "                    getitem_180: \"f32[s77, 64, 128, 128]\" = _native_batch_norm_legit_no_training_38[0];  _native_batch_norm_legit_no_training_38 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_4: \"f32[s77, 64, 128, 128]\" = torch.ops.aten.relu.default(getitem_180);  getitem_180 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_65: \"f32[s77, 64, 128, 128]\" = torch.ops.aten.conv2d.default(relu_4, p_model_decoder_blocks_2_conv2_0_weight, None, [1, 1], [1, 1]);  relu_4 = p_model_decoder_blocks_2_conv2_0_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_39 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_65, p_model_decoder_blocks_2_conv2_1_weight, p_model_decoder_blocks_2_conv2_1_bias, b_model_decoder_blocks_2_conv2_1_running_mean, b_model_decoder_blocks_2_conv2_1_running_var, 0.1, 1e-05);  conv2d_65 = p_model_decoder_blocks_2_conv2_1_weight = p_model_decoder_blocks_2_conv2_1_bias = b_model_decoder_blocks_2_conv2_1_running_mean = b_model_decoder_blocks_2_conv2_1_running_var = None\n",
       "                    getitem_183: \"f32[s77, 64, 128, 128]\" = _native_batch_norm_legit_no_training_39[0];  _native_batch_norm_legit_no_training_39 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_5: \"f32[s77, 64, 128, 128]\" = torch.ops.aten.relu.default(getitem_183);  getitem_183 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:50 in forward, code: feature_map = F.interpolate(\n",
       "                    upsample_nearest2d_3: \"f32[s77, 64, 256, 256]\" = torch.ops.aten.upsample_nearest2d.vec(relu_5, [256, 256], None);  relu_5 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:56 in forward, code: feature_map = torch.cat([feature_map, skip_connection], dim=1)\n",
       "                    cat_4: \"f32[s77, 128, 256, 256]\" = torch.ops.aten.cat.default([upsample_nearest2d_3, conv2d_1], 1);  upsample_nearest2d_3 = conv2d_1 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_66: \"f32[s77, 32, 256, 256]\" = torch.ops.aten.conv2d.default(cat_4, p_model_decoder_blocks_3_conv1_0_weight, None, [1, 1], [1, 1]);  cat_4 = p_model_decoder_blocks_3_conv1_0_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_40 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_66, p_model_decoder_blocks_3_conv1_1_weight, p_model_decoder_blocks_3_conv1_1_bias, b_model_decoder_blocks_3_conv1_1_running_mean, b_model_decoder_blocks_3_conv1_1_running_var, 0.1, 1e-05);  conv2d_66 = p_model_decoder_blocks_3_conv1_1_weight = p_model_decoder_blocks_3_conv1_1_bias = b_model_decoder_blocks_3_conv1_1_running_mean = b_model_decoder_blocks_3_conv1_1_running_var = None\n",
       "                    getitem_186: \"f32[s77, 32, 256, 256]\" = _native_batch_norm_legit_no_training_40[0];  _native_batch_norm_legit_no_training_40 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_6: \"f32[s77, 32, 256, 256]\" = torch.ops.aten.relu.default(getitem_186);  getitem_186 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_67: \"f32[s77, 32, 256, 256]\" = torch.ops.aten.conv2d.default(relu_6, p_model_decoder_blocks_3_conv2_0_weight, None, [1, 1], [1, 1]);  relu_6 = p_model_decoder_blocks_3_conv2_0_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_41 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_67, p_model_decoder_blocks_3_conv2_1_weight, p_model_decoder_blocks_3_conv2_1_bias, b_model_decoder_blocks_3_conv2_1_running_mean, b_model_decoder_blocks_3_conv2_1_running_var, 0.1, 1e-05);  conv2d_67 = p_model_decoder_blocks_3_conv2_1_weight = p_model_decoder_blocks_3_conv2_1_bias = b_model_decoder_blocks_3_conv2_1_running_mean = b_model_decoder_blocks_3_conv2_1_running_var = None\n",
       "                    getitem_189: \"f32[s77, 32, 256, 256]\" = _native_batch_norm_legit_no_training_41[0];  _native_batch_norm_legit_no_training_41 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_7: \"f32[s77, 32, 256, 256]\" = torch.ops.aten.relu.default(getitem_189);  getitem_189 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:50 in forward, code: feature_map = F.interpolate(\n",
       "                    upsample_nearest2d_4: \"f32[s77, 32, 512, 512]\" = torch.ops.aten.upsample_nearest2d.vec(relu_7, [512, 512], None);  relu_7 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_68: \"f32[s77, 16, 512, 512]\" = torch.ops.aten.conv2d.default(upsample_nearest2d_4, p_model_decoder_blocks_4_conv1_0_weight, None, [1, 1], [1, 1]);  upsample_nearest2d_4 = p_model_decoder_blocks_4_conv1_0_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_42 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_68, p_model_decoder_blocks_4_conv1_1_weight, p_model_decoder_blocks_4_conv1_1_bias, b_model_decoder_blocks_4_conv1_1_running_mean, b_model_decoder_blocks_4_conv1_1_running_var, 0.1, 1e-05);  conv2d_68 = p_model_decoder_blocks_4_conv1_1_weight = p_model_decoder_blocks_4_conv1_1_bias = b_model_decoder_blocks_4_conv1_1_running_mean = b_model_decoder_blocks_4_conv1_1_running_var = None\n",
       "                    getitem_192: \"f32[s77, 16, 512, 512]\" = _native_batch_norm_legit_no_training_42[0];  _native_batch_norm_legit_no_training_42 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_8: \"f32[s77, 16, 512, 512]\" = torch.ops.aten.relu.default(getitem_192);  getitem_192 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_69: \"f32[s77, 16, 512, 512]\" = torch.ops.aten.conv2d.default(relu_8, p_model_decoder_blocks_4_conv2_0_weight, None, [1, 1], [1, 1]);  relu_8 = p_model_decoder_blocks_4_conv2_0_weight = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_43 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_69, p_model_decoder_blocks_4_conv2_1_weight, p_model_decoder_blocks_4_conv2_1_bias, b_model_decoder_blocks_4_conv2_1_running_mean, b_model_decoder_blocks_4_conv2_1_running_var, 0.1, 1e-05);  conv2d_69 = p_model_decoder_blocks_4_conv2_1_weight = p_model_decoder_blocks_4_conv2_1_bias = b_model_decoder_blocks_4_conv2_1_running_mean = b_model_decoder_blocks_4_conv2_1_running_var = None\n",
       "                    getitem_195: \"f32[s77, 16, 512, 512]\" = _native_batch_norm_legit_no_training_43[0];  _native_batch_norm_legit_no_training_43 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_9: \"f32[s77, 16, 512, 512]\" = torch.ops.aten.relu.default(getitem_195);  getitem_195 = None\n",
       "            \n",
       "                     # File: /Users/taylor.denouden/Documents/PycharmProjects/kelp-o-matic/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_70: \"f32[s77, 1, 512, 512]\" = torch.ops.aten.conv2d.default(relu_9, p_model_segmentation_head_0_weight, p_model_segmentation_head_0_bias, [1, 1], [1, 1]);  relu_9 = p_model_segmentation_head_0_weight = p_model_segmentation_head_0_bias = None\n",
       "                    return (conv2d_70,)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_model_encoder_model_stem_conv1_weight: PARAMETER target='model.encoder.model.stem.conv1.weight'\n",
       "            p_model_encoder_model_stem_conv1_bias: PARAMETER target='model.encoder.model.stem.conv1.bias'\n",
       "            p_model_encoder_model_stem_norm1_weight: PARAMETER target='model.encoder.model.stem.norm1.weight'\n",
       "            p_model_encoder_model_stem_norm1_bias: PARAMETER target='model.encoder.model.stem.norm1.bias'\n",
       "            p_model_encoder_model_stem_conv2_weight: PARAMETER target='model.encoder.model.stem.conv2.weight'\n",
       "            p_model_encoder_model_stem_conv2_bias: PARAMETER target='model.encoder.model.stem.conv2.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_pre_norm_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.pre_norm.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_pre_norm_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.pre_norm.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_conv1_1x1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.conv1_1x1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_norm1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.norm1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_norm1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.norm1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_conv2_kxk_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.conv2_kxk.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_norm2_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.norm2.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_norm2_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.norm2.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_se_fc1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.se.fc1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_se_fc1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.se.fc1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_se_fc2_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.se.fc2.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_se_fc2_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.se.fc2.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_conv3_1x1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.conv3_1x1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_conv_conv3_1x1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.conv.conv3_1x1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_block_norm1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_block.norm1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_block_norm1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_block.norm1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_block_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_block.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_block_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_block.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_block.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_block_attn_proj_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_block.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_block_attn_proj_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_block.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_block_norm2_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_block.norm2.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_block_norm2_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_block.norm2.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_block.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_block.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_block.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_block_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_block.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_grid_norm1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_grid.norm1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_grid_norm1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_grid.norm1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_grid.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_grid.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_grid.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_proj_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_grid.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_grid_attn_proj_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_grid.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_grid_norm2_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_grid.norm2.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_grid_norm2_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_grid.norm2.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_grid.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_grid.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_grid.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_0_attn_grid_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_0.blocks.0.attn_grid.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_pre_norm_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.pre_norm.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_pre_norm_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.pre_norm.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_conv1_1x1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.conv1_1x1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_norm1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.norm1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_norm1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.norm1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_conv2_kxk_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.conv2_kxk.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_norm2_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.norm2.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_norm2_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.norm2.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_se_fc1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.se.fc1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_se_fc1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.se.fc1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_se_fc2_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.se.fc2.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_se_fc2_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.se.fc2.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_conv3_1x1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.conv3_1x1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_conv_conv3_1x1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.conv.conv3_1x1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_block_norm1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_block.norm1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_block_norm1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_block.norm1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_block_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_block.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_block_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_block.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_block.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_block_attn_proj_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_block.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_block_attn_proj_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_block.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_block_norm2_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_block.norm2.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_block_norm2_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_block.norm2.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_block.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_block.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_block.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_block_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_block.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_grid_norm1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_grid.norm1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_grid_norm1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_grid.norm1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_grid.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_grid.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_grid.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_proj_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_grid.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_grid_attn_proj_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_grid.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_grid_norm2_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_grid.norm2.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_grid_norm2_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_grid.norm2.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_grid.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_grid.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_grid.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_0_blocks_1_attn_grid_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_0.blocks.1.attn_grid.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_shortcut_expand_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.shortcut.expand.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_shortcut_expand_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.shortcut.expand.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_pre_norm_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.pre_norm.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_pre_norm_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.pre_norm.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_conv1_1x1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.conv1_1x1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_norm1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.norm1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_norm1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.norm1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_conv2_kxk_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.conv2_kxk.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_norm2_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.norm2.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_norm2_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.norm2.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_se_fc1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.se.fc1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_se_fc1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.se.fc1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_se_fc2_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.se.fc2.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_se_fc2_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.se.fc2.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_conv3_1x1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.conv3_1x1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_conv_conv3_1x1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.conv.conv3_1x1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_block_norm1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_block.norm1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_block_norm1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_block.norm1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_block_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_block.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_block_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_block.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_block.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_block_attn_proj_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_block.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_block_attn_proj_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_block.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_block_norm2_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_block.norm2.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_block_norm2_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_block.norm2.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_block.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_block.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_block.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_block_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_block.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_grid_norm1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_grid.norm1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_grid_norm1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_grid.norm1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_grid.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_grid.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_grid.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_proj_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_grid.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_grid_attn_proj_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_grid.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_grid_norm2_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_grid.norm2.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_grid_norm2_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_grid.norm2.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_grid.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_grid.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_grid.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_0_attn_grid_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_1.blocks.0.attn_grid.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_pre_norm_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.pre_norm.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_pre_norm_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.pre_norm.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_conv1_1x1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.conv1_1x1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_norm1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.norm1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_norm1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.norm1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_conv2_kxk_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.conv2_kxk.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_norm2_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.norm2.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_norm2_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.norm2.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_se_fc1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.se.fc1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_se_fc1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.se.fc1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_se_fc2_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.se.fc2.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_se_fc2_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.se.fc2.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_conv3_1x1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.conv3_1x1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_conv_conv3_1x1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.conv.conv3_1x1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_block_norm1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_block.norm1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_block_norm1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_block.norm1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_block_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_block.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_block_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_block.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_block.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_block_attn_proj_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_block.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_block_attn_proj_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_block.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_block_norm2_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_block.norm2.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_block_norm2_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_block.norm2.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_block.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_block.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_block.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_block_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_block.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_grid_norm1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_grid.norm1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_grid_norm1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_grid.norm1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_grid.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_grid.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_grid.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_proj_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_grid.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_grid_attn_proj_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_grid.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_grid_norm2_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_grid.norm2.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_grid_norm2_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_grid.norm2.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_grid.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_grid.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_grid.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_1_blocks_1_attn_grid_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_1.blocks.1.attn_grid.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_shortcut_expand_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.shortcut.expand.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_shortcut_expand_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.shortcut.expand.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_pre_norm_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.pre_norm.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_pre_norm_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.pre_norm.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_conv1_1x1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.conv1_1x1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_conv2_kxk_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.conv2_kxk.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_se_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.se.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_se_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.se.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_se_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.se.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_se_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.se.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_conv3_1x1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.conv3_1x1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_conv_conv3_1x1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.conv.conv3_1x1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_block_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_block.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_block_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_block.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_block_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_block.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_block_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_block.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_block.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_block_attn_proj_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_block.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_block_attn_proj_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_block.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_block_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_block.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_block_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_block.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_block.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_block.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_block.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_block_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_block.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_grid_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_grid.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_grid_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_grid.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_grid.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_grid.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_grid.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_proj_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_grid.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_grid_attn_proj_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_grid.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_grid_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_grid.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_grid_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_grid.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_grid.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_grid.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_grid.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_0_attn_grid_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.0.attn_grid.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_pre_norm_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.pre_norm.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_pre_norm_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.pre_norm.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_conv1_1x1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.conv1_1x1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_conv2_kxk_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.conv2_kxk.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_se_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.se.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_se_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.se.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_se_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.se.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_se_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.se.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_conv3_1x1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.conv3_1x1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_conv_conv3_1x1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.conv.conv3_1x1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_block_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_block.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_block_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_block.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_block_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_block.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_block_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_block.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_block.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_block_attn_proj_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_block.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_block_attn_proj_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_block.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_block_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_block.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_block_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_block.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_block.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_block.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_block.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_block_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_block.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_grid_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_grid.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_grid_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_grid.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_grid.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_grid.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_grid.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_proj_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_grid.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_grid_attn_proj_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_grid.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_grid_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_grid.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_grid_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_grid.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_grid.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_grid.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_grid.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_1_attn_grid_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.1.attn_grid.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_pre_norm_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.pre_norm.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_pre_norm_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.pre_norm.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_conv1_1x1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.conv1_1x1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_conv2_kxk_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.conv2_kxk.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_se_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.se.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_se_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.se.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_se_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.se.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_se_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.se.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_conv3_1x1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.conv3_1x1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_conv_conv3_1x1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.conv.conv3_1x1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_block_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_block.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_block_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_block.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_block_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_block.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_block_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_block.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_block_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_block.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_block_attn_proj_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_block.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_block_attn_proj_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_block.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_block_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_block.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_block_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_block.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_block.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_block.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_block.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_block_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_block.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_grid_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_grid.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_grid_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_grid.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_grid.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_grid.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_grid.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_proj_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_grid.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_grid_attn_proj_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_grid.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_grid_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_grid.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_grid_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_grid.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_grid.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_grid.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_grid.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_2_attn_grid_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.2.attn_grid.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_pre_norm_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.pre_norm.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_pre_norm_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.pre_norm.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_conv1_1x1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.conv1_1x1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_conv2_kxk_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.conv2_kxk.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_se_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.se.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_se_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.se.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_se_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.se.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_se_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.se.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_conv3_1x1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.conv3_1x1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_conv_conv3_1x1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.conv.conv3_1x1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_block_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_block.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_block_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_block.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_block_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_block.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_block_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_block.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_block_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_block.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_block_attn_proj_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_block.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_block_attn_proj_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_block.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_block_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_block.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_block_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_block.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_block.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_block.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_block.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_block_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_block.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_grid_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_grid.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_grid_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_grid.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_grid.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_grid.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_grid.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_proj_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_grid.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_grid_attn_proj_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_grid.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_grid_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_grid.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_grid_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_grid.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_grid.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_grid.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_grid.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_3_attn_grid_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.3.attn_grid.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_pre_norm_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.pre_norm.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_pre_norm_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.pre_norm.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_conv1_1x1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.conv1_1x1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_conv2_kxk_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.conv2_kxk.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_se_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.se.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_se_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.se.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_se_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.se.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_se_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.se.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_conv3_1x1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.conv3_1x1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_conv_conv3_1x1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.conv.conv3_1x1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_block_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_block.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_block_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_block.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_block_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_block.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_block_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_block.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_block_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_block.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_block_attn_proj_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_block.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_block_attn_proj_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_block.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_block_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_block.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_block_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_block.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_block.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_block.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_block.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_block_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_block.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_grid_norm1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_grid.norm1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_grid_norm1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_grid.norm1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_grid.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_grid.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_grid.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_proj_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_grid.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_grid_attn_proj_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_grid.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_grid_norm2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_grid.norm2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_grid_norm2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_grid.norm2.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_grid.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_grid.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_grid.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_2_blocks_4_attn_grid_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_2.blocks.4.attn_grid.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_shortcut_expand_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.shortcut.expand.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_shortcut_expand_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.shortcut.expand.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_pre_norm_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.pre_norm.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_pre_norm_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.pre_norm.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_conv1_1x1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.conv1_1x1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_norm1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.norm1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_norm1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.norm1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_conv2_kxk_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.conv2_kxk.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_norm2_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.norm2.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_norm2_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.norm2.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_se_fc1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.se.fc1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_se_fc1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.se.fc1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_se_fc2_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.se.fc2.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_se_fc2_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.se.fc2.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_conv3_1x1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.conv3_1x1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_conv_conv3_1x1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.conv.conv3_1x1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_block_norm1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_block.norm1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_block_norm1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_block.norm1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_block_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_block.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_block_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_block.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_block_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_block.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_block_attn_proj_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_block.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_block_attn_proj_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_block.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_block_norm2_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_block.norm2.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_block_norm2_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_block.norm2.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_block.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_block.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_block.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_block_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_block.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_grid_norm1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_grid.norm1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_grid_norm1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_grid.norm1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_grid.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_grid.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_grid.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_proj_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_grid.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_grid_attn_proj_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_grid.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_grid_norm2_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_grid.norm2.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_grid_norm2_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_grid.norm2.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_grid.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_grid.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_grid.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_0_attn_grid_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_3.blocks.0.attn_grid.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_pre_norm_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.pre_norm.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_pre_norm_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.pre_norm.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_conv1_1x1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.conv1_1x1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_norm1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.norm1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_norm1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.norm1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_conv2_kxk_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.conv2_kxk.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_norm2_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.norm2.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_norm2_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.norm2.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_se_fc1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.se.fc1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_se_fc1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.se.fc1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_se_fc2_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.se.fc2.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_se_fc2_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.se.fc2.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_conv3_1x1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.conv3_1x1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_conv_conv3_1x1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.conv.conv3_1x1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_block_norm1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_block.norm1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_block_norm1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_block.norm1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_block_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_block.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_block_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_block.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_block_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_block.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_block_attn_proj_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_block.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_block_attn_proj_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_block.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_block_norm2_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_block.norm2.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_block_norm2_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_block.norm2.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_block.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_block.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_block.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_block_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_block.mlp.fc2.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_grid_norm1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_grid.norm1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_grid_norm1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_grid.norm1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_qkv_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_grid.attn.qkv.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_qkv_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_grid.attn.qkv.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_rel_pos_relative_position_bias_table: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_grid.attn.rel_pos.relative_position_bias_table'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_proj_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_grid.attn.proj.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_grid_attn_proj_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_grid.attn.proj.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_grid_norm2_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_grid.norm2.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_grid_norm2_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_grid.norm2.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc1_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_grid.mlp.fc1.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc1_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_grid.mlp.fc1.bias'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc2_weight: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_grid.mlp.fc2.weight'\n",
       "            p_model_encoder_model_stages_3_blocks_1_attn_grid_mlp_fc2_bias: PARAMETER target='model.encoder.model.stages_3.blocks.1.attn_grid.mlp.fc2.bias'\n",
       "            p_model_decoder_blocks_0_conv1_0_weight: PARAMETER target='model.decoder.blocks.0.conv1.0.weight'\n",
       "            p_model_decoder_blocks_0_conv1_1_weight: PARAMETER target='model.decoder.blocks.0.conv1.1.weight'\n",
       "            p_model_decoder_blocks_0_conv1_1_bias: PARAMETER target='model.decoder.blocks.0.conv1.1.bias'\n",
       "            p_model_decoder_blocks_0_conv2_0_weight: PARAMETER target='model.decoder.blocks.0.conv2.0.weight'\n",
       "            p_model_decoder_blocks_0_conv2_1_weight: PARAMETER target='model.decoder.blocks.0.conv2.1.weight'\n",
       "            p_model_decoder_blocks_0_conv2_1_bias: PARAMETER target='model.decoder.blocks.0.conv2.1.bias'\n",
       "            p_model_decoder_blocks_1_conv1_0_weight: PARAMETER target='model.decoder.blocks.1.conv1.0.weight'\n",
       "            p_model_decoder_blocks_1_conv1_1_weight: PARAMETER target='model.decoder.blocks.1.conv1.1.weight'\n",
       "            p_model_decoder_blocks_1_conv1_1_bias: PARAMETER target='model.decoder.blocks.1.conv1.1.bias'\n",
       "            p_model_decoder_blocks_1_conv2_0_weight: PARAMETER target='model.decoder.blocks.1.conv2.0.weight'\n",
       "            p_model_decoder_blocks_1_conv2_1_weight: PARAMETER target='model.decoder.blocks.1.conv2.1.weight'\n",
       "            p_model_decoder_blocks_1_conv2_1_bias: PARAMETER target='model.decoder.blocks.1.conv2.1.bias'\n",
       "            p_model_decoder_blocks_2_conv1_0_weight: PARAMETER target='model.decoder.blocks.2.conv1.0.weight'\n",
       "            p_model_decoder_blocks_2_conv1_1_weight: PARAMETER target='model.decoder.blocks.2.conv1.1.weight'\n",
       "            p_model_decoder_blocks_2_conv1_1_bias: PARAMETER target='model.decoder.blocks.2.conv1.1.bias'\n",
       "            p_model_decoder_blocks_2_conv2_0_weight: PARAMETER target='model.decoder.blocks.2.conv2.0.weight'\n",
       "            p_model_decoder_blocks_2_conv2_1_weight: PARAMETER target='model.decoder.blocks.2.conv2.1.weight'\n",
       "            p_model_decoder_blocks_2_conv2_1_bias: PARAMETER target='model.decoder.blocks.2.conv2.1.bias'\n",
       "            p_model_decoder_blocks_3_conv1_0_weight: PARAMETER target='model.decoder.blocks.3.conv1.0.weight'\n",
       "            p_model_decoder_blocks_3_conv1_1_weight: PARAMETER target='model.decoder.blocks.3.conv1.1.weight'\n",
       "            p_model_decoder_blocks_3_conv1_1_bias: PARAMETER target='model.decoder.blocks.3.conv1.1.bias'\n",
       "            p_model_decoder_blocks_3_conv2_0_weight: PARAMETER target='model.decoder.blocks.3.conv2.0.weight'\n",
       "            p_model_decoder_blocks_3_conv2_1_weight: PARAMETER target='model.decoder.blocks.3.conv2.1.weight'\n",
       "            p_model_decoder_blocks_3_conv2_1_bias: PARAMETER target='model.decoder.blocks.3.conv2.1.bias'\n",
       "            p_model_decoder_blocks_4_conv1_0_weight: PARAMETER target='model.decoder.blocks.4.conv1.0.weight'\n",
       "            p_model_decoder_blocks_4_conv1_1_weight: PARAMETER target='model.decoder.blocks.4.conv1.1.weight'\n",
       "            p_model_decoder_blocks_4_conv1_1_bias: PARAMETER target='model.decoder.blocks.4.conv1.1.bias'\n",
       "            p_model_decoder_blocks_4_conv2_0_weight: PARAMETER target='model.decoder.blocks.4.conv2.0.weight'\n",
       "            p_model_decoder_blocks_4_conv2_1_weight: PARAMETER target='model.decoder.blocks.4.conv2.1.weight'\n",
       "            p_model_decoder_blocks_4_conv2_1_bias: PARAMETER target='model.decoder.blocks.4.conv2.1.bias'\n",
       "            p_model_segmentation_head_0_weight: PARAMETER target='model.segmentation_head.0.weight'\n",
       "            p_model_segmentation_head_0_bias: PARAMETER target='model.segmentation_head.0.bias'\n",
       "            b_per_channel_mean: BUFFER target='per_channel_mean' persistent=True\n",
       "            b_per_channel_std: BUFFER target='per_channel_std' persistent=True\n",
       "            b_model_encoder_model_stem_norm1_running_mean: BUFFER target='model.encoder.model.stem.norm1.running_mean' persistent=True\n",
       "            b_model_encoder_model_stem_norm1_running_var: BUFFER target='model.encoder.model.stem.norm1.running_var' persistent=True\n",
       "            b_model_encoder_model_stem_norm1_num_batches_tracked: BUFFER target='model.encoder.model.stem.norm1.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_0_conv_pre_norm_running_mean: BUFFER target='model.encoder.model.stages_0.blocks.0.conv.pre_norm.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_0_conv_pre_norm_running_var: BUFFER target='model.encoder.model.stages_0.blocks.0.conv.pre_norm.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_0_conv_pre_norm_num_batches_tracked: BUFFER target='model.encoder.model.stages_0.blocks.0.conv.pre_norm.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_0_conv_norm1_running_mean: BUFFER target='model.encoder.model.stages_0.blocks.0.conv.norm1.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_0_conv_norm1_running_var: BUFFER target='model.encoder.model.stages_0.blocks.0.conv.norm1.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_0_conv_norm1_num_batches_tracked: BUFFER target='model.encoder.model.stages_0.blocks.0.conv.norm1.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_0_conv_norm2_running_mean: BUFFER target='model.encoder.model.stages_0.blocks.0.conv.norm2.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_0_conv_norm2_running_var: BUFFER target='model.encoder.model.stages_0.blocks.0.conv.norm2.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_0_conv_norm2_num_batches_tracked: BUFFER target='model.encoder.model.stages_0.blocks.0.conv.norm2.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_0_attn_block_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_0.blocks.0.attn_block.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_0_blocks_0_attn_block_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_0.blocks.0.attn_block.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_0_blocks_0_attn_grid_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_0.blocks.0.attn_grid.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_0_blocks_0_attn_grid_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_0.blocks.0.attn_grid.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_0_blocks_1_conv_pre_norm_running_mean: BUFFER target='model.encoder.model.stages_0.blocks.1.conv.pre_norm.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_1_conv_pre_norm_running_var: BUFFER target='model.encoder.model.stages_0.blocks.1.conv.pre_norm.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_1_conv_pre_norm_num_batches_tracked: BUFFER target='model.encoder.model.stages_0.blocks.1.conv.pre_norm.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_1_conv_norm1_running_mean: BUFFER target='model.encoder.model.stages_0.blocks.1.conv.norm1.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_1_conv_norm1_running_var: BUFFER target='model.encoder.model.stages_0.blocks.1.conv.norm1.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_1_conv_norm1_num_batches_tracked: BUFFER target='model.encoder.model.stages_0.blocks.1.conv.norm1.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_1_conv_norm2_running_mean: BUFFER target='model.encoder.model.stages_0.blocks.1.conv.norm2.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_1_conv_norm2_running_var: BUFFER target='model.encoder.model.stages_0.blocks.1.conv.norm2.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_1_conv_norm2_num_batches_tracked: BUFFER target='model.encoder.model.stages_0.blocks.1.conv.norm2.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_0_blocks_1_attn_block_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_0.blocks.1.attn_block.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_0_blocks_1_attn_block_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_0.blocks.1.attn_block.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_0_blocks_1_attn_grid_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_0.blocks.1.attn_grid.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_0_blocks_1_attn_grid_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_0.blocks.1.attn_grid.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_1_blocks_0_conv_pre_norm_running_mean: BUFFER target='model.encoder.model.stages_1.blocks.0.conv.pre_norm.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_0_conv_pre_norm_running_var: BUFFER target='model.encoder.model.stages_1.blocks.0.conv.pre_norm.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_0_conv_pre_norm_num_batches_tracked: BUFFER target='model.encoder.model.stages_1.blocks.0.conv.pre_norm.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_0_conv_norm1_running_mean: BUFFER target='model.encoder.model.stages_1.blocks.0.conv.norm1.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_0_conv_norm1_running_var: BUFFER target='model.encoder.model.stages_1.blocks.0.conv.norm1.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_0_conv_norm1_num_batches_tracked: BUFFER target='model.encoder.model.stages_1.blocks.0.conv.norm1.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_0_conv_norm2_running_mean: BUFFER target='model.encoder.model.stages_1.blocks.0.conv.norm2.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_0_conv_norm2_running_var: BUFFER target='model.encoder.model.stages_1.blocks.0.conv.norm2.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_0_conv_norm2_num_batches_tracked: BUFFER target='model.encoder.model.stages_1.blocks.0.conv.norm2.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_0_attn_block_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_1.blocks.0.attn_block.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_1_blocks_0_attn_block_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_1.blocks.0.attn_block.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_1_blocks_0_attn_grid_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_1.blocks.0.attn_grid.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_1_blocks_0_attn_grid_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_1.blocks.0.attn_grid.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_1_blocks_1_conv_pre_norm_running_mean: BUFFER target='model.encoder.model.stages_1.blocks.1.conv.pre_norm.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_1_conv_pre_norm_running_var: BUFFER target='model.encoder.model.stages_1.blocks.1.conv.pre_norm.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_1_conv_pre_norm_num_batches_tracked: BUFFER target='model.encoder.model.stages_1.blocks.1.conv.pre_norm.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_1_conv_norm1_running_mean: BUFFER target='model.encoder.model.stages_1.blocks.1.conv.norm1.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_1_conv_norm1_running_var: BUFFER target='model.encoder.model.stages_1.blocks.1.conv.norm1.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_1_conv_norm1_num_batches_tracked: BUFFER target='model.encoder.model.stages_1.blocks.1.conv.norm1.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_1_conv_norm2_running_mean: BUFFER target='model.encoder.model.stages_1.blocks.1.conv.norm2.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_1_conv_norm2_running_var: BUFFER target='model.encoder.model.stages_1.blocks.1.conv.norm2.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_1_conv_norm2_num_batches_tracked: BUFFER target='model.encoder.model.stages_1.blocks.1.conv.norm2.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_1_blocks_1_attn_block_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_1.blocks.1.attn_block.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_1_blocks_1_attn_block_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_1.blocks.1.attn_block.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_1_blocks_1_attn_grid_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_1.blocks.1.attn_grid.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_1_blocks_1_attn_grid_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_1.blocks.1.attn_grid.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_0_conv_pre_norm_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.0.conv.pre_norm.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_0_conv_pre_norm_running_var: BUFFER target='model.encoder.model.stages_2.blocks.0.conv.pre_norm.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_0_conv_pre_norm_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.0.conv.pre_norm.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_0_conv_norm1_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.0.conv.norm1.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_0_conv_norm1_running_var: BUFFER target='model.encoder.model.stages_2.blocks.0.conv.norm1.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_0_conv_norm1_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.0.conv.norm1.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_0_conv_norm2_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.0.conv.norm2.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_0_conv_norm2_running_var: BUFFER target='model.encoder.model.stages_2.blocks.0.conv.norm2.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_0_conv_norm2_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.0.conv.norm2.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_0_attn_block_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_2.blocks.0.attn_block.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_0_attn_block_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_2.blocks.0.attn_block.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_0_attn_grid_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_2.blocks.0.attn_grid.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_0_attn_grid_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_2.blocks.0.attn_grid.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_1_conv_pre_norm_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.1.conv.pre_norm.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_1_conv_pre_norm_running_var: BUFFER target='model.encoder.model.stages_2.blocks.1.conv.pre_norm.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_1_conv_pre_norm_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.1.conv.pre_norm.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_1_conv_norm1_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.1.conv.norm1.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_1_conv_norm1_running_var: BUFFER target='model.encoder.model.stages_2.blocks.1.conv.norm1.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_1_conv_norm1_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.1.conv.norm1.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_1_conv_norm2_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.1.conv.norm2.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_1_conv_norm2_running_var: BUFFER target='model.encoder.model.stages_2.blocks.1.conv.norm2.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_1_conv_norm2_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.1.conv.norm2.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_1_attn_block_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_2.blocks.1.attn_block.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_1_attn_block_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_2.blocks.1.attn_block.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_1_attn_grid_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_2.blocks.1.attn_grid.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_1_attn_grid_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_2.blocks.1.attn_grid.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_2_conv_pre_norm_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.2.conv.pre_norm.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_2_conv_pre_norm_running_var: BUFFER target='model.encoder.model.stages_2.blocks.2.conv.pre_norm.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_2_conv_pre_norm_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.2.conv.pre_norm.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_2_conv_norm1_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.2.conv.norm1.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_2_conv_norm1_running_var: BUFFER target='model.encoder.model.stages_2.blocks.2.conv.norm1.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_2_conv_norm1_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.2.conv.norm1.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_2_conv_norm2_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.2.conv.norm2.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_2_conv_norm2_running_var: BUFFER target='model.encoder.model.stages_2.blocks.2.conv.norm2.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_2_conv_norm2_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.2.conv.norm2.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_2_attn_block_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_2.blocks.2.attn_block.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_2_attn_block_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_2.blocks.2.attn_block.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_2_attn_grid_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_2.blocks.2.attn_grid.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_2_attn_grid_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_2.blocks.2.attn_grid.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_3_conv_pre_norm_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.3.conv.pre_norm.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_3_conv_pre_norm_running_var: BUFFER target='model.encoder.model.stages_2.blocks.3.conv.pre_norm.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_3_conv_pre_norm_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.3.conv.pre_norm.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_3_conv_norm1_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.3.conv.norm1.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_3_conv_norm1_running_var: BUFFER target='model.encoder.model.stages_2.blocks.3.conv.norm1.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_3_conv_norm1_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.3.conv.norm1.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_3_conv_norm2_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.3.conv.norm2.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_3_conv_norm2_running_var: BUFFER target='model.encoder.model.stages_2.blocks.3.conv.norm2.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_3_conv_norm2_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.3.conv.norm2.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_3_attn_block_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_2.blocks.3.attn_block.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_3_attn_block_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_2.blocks.3.attn_block.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_3_attn_grid_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_2.blocks.3.attn_grid.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_3_attn_grid_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_2.blocks.3.attn_grid.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_4_conv_pre_norm_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.4.conv.pre_norm.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_4_conv_pre_norm_running_var: BUFFER target='model.encoder.model.stages_2.blocks.4.conv.pre_norm.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_4_conv_pre_norm_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.4.conv.pre_norm.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_4_conv_norm1_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.4.conv.norm1.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_4_conv_norm1_running_var: BUFFER target='model.encoder.model.stages_2.blocks.4.conv.norm1.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_4_conv_norm1_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.4.conv.norm1.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_4_conv_norm2_running_mean: BUFFER target='model.encoder.model.stages_2.blocks.4.conv.norm2.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_4_conv_norm2_running_var: BUFFER target='model.encoder.model.stages_2.blocks.4.conv.norm2.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_4_conv_norm2_num_batches_tracked: BUFFER target='model.encoder.model.stages_2.blocks.4.conv.norm2.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_2_blocks_4_attn_block_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_2.blocks.4.attn_block.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_4_attn_block_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_2.blocks.4.attn_block.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_4_attn_grid_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_2.blocks.4.attn_grid.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_2_blocks_4_attn_grid_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_2.blocks.4.attn_grid.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_3_blocks_0_conv_pre_norm_running_mean: BUFFER target='model.encoder.model.stages_3.blocks.0.conv.pre_norm.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_0_conv_pre_norm_running_var: BUFFER target='model.encoder.model.stages_3.blocks.0.conv.pre_norm.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_0_conv_pre_norm_num_batches_tracked: BUFFER target='model.encoder.model.stages_3.blocks.0.conv.pre_norm.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_0_conv_norm1_running_mean: BUFFER target='model.encoder.model.stages_3.blocks.0.conv.norm1.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_0_conv_norm1_running_var: BUFFER target='model.encoder.model.stages_3.blocks.0.conv.norm1.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_0_conv_norm1_num_batches_tracked: BUFFER target='model.encoder.model.stages_3.blocks.0.conv.norm1.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_0_conv_norm2_running_mean: BUFFER target='model.encoder.model.stages_3.blocks.0.conv.norm2.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_0_conv_norm2_running_var: BUFFER target='model.encoder.model.stages_3.blocks.0.conv.norm2.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_0_conv_norm2_num_batches_tracked: BUFFER target='model.encoder.model.stages_3.blocks.0.conv.norm2.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_0_attn_block_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_3.blocks.0.attn_block.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_3_blocks_0_attn_block_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_3.blocks.0.attn_block.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_3_blocks_0_attn_grid_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_3.blocks.0.attn_grid.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_3_blocks_0_attn_grid_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_3.blocks.0.attn_grid.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_3_blocks_1_conv_pre_norm_running_mean: BUFFER target='model.encoder.model.stages_3.blocks.1.conv.pre_norm.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_1_conv_pre_norm_running_var: BUFFER target='model.encoder.model.stages_3.blocks.1.conv.pre_norm.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_1_conv_pre_norm_num_batches_tracked: BUFFER target='model.encoder.model.stages_3.blocks.1.conv.pre_norm.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_1_conv_norm1_running_mean: BUFFER target='model.encoder.model.stages_3.blocks.1.conv.norm1.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_1_conv_norm1_running_var: BUFFER target='model.encoder.model.stages_3.blocks.1.conv.norm1.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_1_conv_norm1_num_batches_tracked: BUFFER target='model.encoder.model.stages_3.blocks.1.conv.norm1.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_1_conv_norm2_running_mean: BUFFER target='model.encoder.model.stages_3.blocks.1.conv.norm2.running_mean' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_1_conv_norm2_running_var: BUFFER target='model.encoder.model.stages_3.blocks.1.conv.norm2.running_var' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_1_conv_norm2_num_batches_tracked: BUFFER target='model.encoder.model.stages_3.blocks.1.conv.norm2.num_batches_tracked' persistent=True\n",
       "            b_model_encoder_model_stages_3_blocks_1_attn_block_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_3.blocks.1.attn_block.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_3_blocks_1_attn_block_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_3.blocks.1.attn_block.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_3_blocks_1_attn_grid_attn_rel_pos_height_lookup: BUFFER target='model.encoder.model.stages_3.blocks.1.attn_grid.attn.rel_pos.height_lookup' persistent=False\n",
       "            b_model_encoder_model_stages_3_blocks_1_attn_grid_attn_rel_pos_width_lookup: BUFFER target='model.encoder.model.stages_3.blocks.1.attn_grid.attn.rel_pos.width_lookup' persistent=False\n",
       "            b_model_decoder_blocks_0_conv1_1_running_mean: BUFFER target='model.decoder.blocks.0.conv1.1.running_mean' persistent=True\n",
       "            b_model_decoder_blocks_0_conv1_1_running_var: BUFFER target='model.decoder.blocks.0.conv1.1.running_var' persistent=True\n",
       "            b_model_decoder_blocks_0_conv1_1_num_batches_tracked: BUFFER target='model.decoder.blocks.0.conv1.1.num_batches_tracked' persistent=True\n",
       "            b_model_decoder_blocks_0_conv2_1_running_mean: BUFFER target='model.decoder.blocks.0.conv2.1.running_mean' persistent=True\n",
       "            b_model_decoder_blocks_0_conv2_1_running_var: BUFFER target='model.decoder.blocks.0.conv2.1.running_var' persistent=True\n",
       "            b_model_decoder_blocks_0_conv2_1_num_batches_tracked: BUFFER target='model.decoder.blocks.0.conv2.1.num_batches_tracked' persistent=True\n",
       "            b_model_decoder_blocks_1_conv1_1_running_mean: BUFFER target='model.decoder.blocks.1.conv1.1.running_mean' persistent=True\n",
       "            b_model_decoder_blocks_1_conv1_1_running_var: BUFFER target='model.decoder.blocks.1.conv1.1.running_var' persistent=True\n",
       "            b_model_decoder_blocks_1_conv1_1_num_batches_tracked: BUFFER target='model.decoder.blocks.1.conv1.1.num_batches_tracked' persistent=True\n",
       "            b_model_decoder_blocks_1_conv2_1_running_mean: BUFFER target='model.decoder.blocks.1.conv2.1.running_mean' persistent=True\n",
       "            b_model_decoder_blocks_1_conv2_1_running_var: BUFFER target='model.decoder.blocks.1.conv2.1.running_var' persistent=True\n",
       "            b_model_decoder_blocks_1_conv2_1_num_batches_tracked: BUFFER target='model.decoder.blocks.1.conv2.1.num_batches_tracked' persistent=True\n",
       "            b_model_decoder_blocks_2_conv1_1_running_mean: BUFFER target='model.decoder.blocks.2.conv1.1.running_mean' persistent=True\n",
       "            b_model_decoder_blocks_2_conv1_1_running_var: BUFFER target='model.decoder.blocks.2.conv1.1.running_var' persistent=True\n",
       "            b_model_decoder_blocks_2_conv1_1_num_batches_tracked: BUFFER target='model.decoder.blocks.2.conv1.1.num_batches_tracked' persistent=True\n",
       "            b_model_decoder_blocks_2_conv2_1_running_mean: BUFFER target='model.decoder.blocks.2.conv2.1.running_mean' persistent=True\n",
       "            b_model_decoder_blocks_2_conv2_1_running_var: BUFFER target='model.decoder.blocks.2.conv2.1.running_var' persistent=True\n",
       "            b_model_decoder_blocks_2_conv2_1_num_batches_tracked: BUFFER target='model.decoder.blocks.2.conv2.1.num_batches_tracked' persistent=True\n",
       "            b_model_decoder_blocks_3_conv1_1_running_mean: BUFFER target='model.decoder.blocks.3.conv1.1.running_mean' persistent=True\n",
       "            b_model_decoder_blocks_3_conv1_1_running_var: BUFFER target='model.decoder.blocks.3.conv1.1.running_var' persistent=True\n",
       "            b_model_decoder_blocks_3_conv1_1_num_batches_tracked: BUFFER target='model.decoder.blocks.3.conv1.1.num_batches_tracked' persistent=True\n",
       "            b_model_decoder_blocks_3_conv2_1_running_mean: BUFFER target='model.decoder.blocks.3.conv2.1.running_mean' persistent=True\n",
       "            b_model_decoder_blocks_3_conv2_1_running_var: BUFFER target='model.decoder.blocks.3.conv2.1.running_var' persistent=True\n",
       "            b_model_decoder_blocks_3_conv2_1_num_batches_tracked: BUFFER target='model.decoder.blocks.3.conv2.1.num_batches_tracked' persistent=True\n",
       "            b_model_decoder_blocks_4_conv1_1_running_mean: BUFFER target='model.decoder.blocks.4.conv1.1.running_mean' persistent=True\n",
       "            b_model_decoder_blocks_4_conv1_1_running_var: BUFFER target='model.decoder.blocks.4.conv1.1.running_var' persistent=True\n",
       "            b_model_decoder_blocks_4_conv1_1_num_batches_tracked: BUFFER target='model.decoder.blocks.4.conv1.1.num_batches_tracked' persistent=True\n",
       "            b_model_decoder_blocks_4_conv2_1_running_mean: BUFFER target='model.decoder.blocks.4.conv2.1.running_mean' persistent=True\n",
       "            b_model_decoder_blocks_4_conv2_1_running_var: BUFFER target='model.decoder.blocks.4.conv2.1.running_var' persistent=True\n",
       "            b_model_decoder_blocks_4_conv2_1_num_batches_tracked: BUFFER target='model.decoder.blocks.4.conv2.1.num_batches_tracked' persistent=True\n",
       "            x: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            conv2d_70: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {s77: VR[0, int_oo]}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    sample_input,\n",
    "    \"./Unet_tu-maxvit_tiny_tf_512_20250818_164043.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    export_params=True,\n",
    "    external_data=False,  # Store model weights in the model file\n",
    "    opset_version=18,  # ONNX opset version\n",
    "    do_constant_folding=True,  # Optimize constants\n",
    "    verbose=False,\n",
    "    # dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    dynamic_shapes={\"x\": (torch.export.Dim(\"batch\"), 5, 512, 512)},\n",
    "    dynamo=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429523c38c895b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
